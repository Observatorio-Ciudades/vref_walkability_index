{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c12bb55-64e2-4367-9490-d5ef96f093b8",
   "metadata": {},
   "source": [
    "# 01_PL_04_Combine_networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ca5c9b-a3ce-4059-9579-b8305cbc51db",
   "metadata": {},
   "source": [
    "The notebook __merges two different street networks into one.__ \n",
    "* Boeing is considered network 01, the base network. From the base network only what's necessary to modify is modified. \n",
    "* Tessellations is considered the network 02, the complementary network.\n",
    "* Parts of the complementary network that are NOT in a zone already covered by the base network are added to the output network.\n",
    "\n",
    "__IMPORTANT: Defining network 01 (Base) and network 02 (Complementary) has an effect on various steps of the process, from deciding what to join to how the final cleaning is performed.__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8aee1f5-bbea-49ce-8596-b4220df4e324",
   "metadata": {},
   "source": [
    "## __Import libraries__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a54aa77-acbb-4e76-b26c-154d762cdc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_folder_path = \"../\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7da95a1b-9cc4-4ef2-a436-4cd7c85fee5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import osmnx as ox\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# To create Point from coordinates\n",
    "from shapely import Point\n",
    "# For calculate distance between points\n",
    "import math\n",
    "# To know if it is a LineString or a MultiLineString, and create them\n",
    "from shapely.geometry import LineString, MultiLineString\n",
    "# To split a line using a point in that line\n",
    "from shapely.ops import split\n",
    "# To force MultiLineStrings to LineStrings\n",
    "from shapely import ops\n",
    "# To reverse a line's geometry (Check for duplicated edges)\n",
    "import shapely\n",
    "# Time processes\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(first_folder_path))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    import src"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fc0ec2-de9b-4b4b-a118-78cd75a96984",
   "metadata": {},
   "source": [
    "## __Notebook config__ [Modify input and output dirs as required]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b4e26e2-5f13-41d0-8dfe-e9a85316152b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/output/shape/network_boeing/guadalajara/guadalajara_nodes.shp\n",
      "../data/output/shape/network_boeing/guadalajara/guadalajara_edges.shp\n",
      "../data/output/shape/network_tessellations/guadalajara/guadalajara_tessellations_nodes_before_consolidation.shp\n",
      "../data/output/shape/network_tessellations/guadalajara/guadalajara_tessellations_edges_before_consolidation.shp\n"
     ]
    }
   ],
   "source": [
    "city = 'guadalajara'\n",
    "\n",
    "# ----- ----- ----- Input (Accepts .zip files) ----- ----- -----\n",
    "# Boeing input network - Specify nodes and edges files. [Both citie's files are .shp]\n",
    "boeing_nodes_dir = first_folder_path + f\"data/output/shape/network_boeing/{city}/{city}_nodes.shp\"\n",
    "boeing_edges_dir = first_folder_path + f\"data/output/shape/network_boeing/{city}/{city}_edges.shp\"\n",
    "# Tessellations input network - Specify nodes and edges files. [NOTE: Guadalajara's files were saved as .shp, and Medellin's as .geojson]\n",
    "tess_nodes_dir = first_folder_path+ f\"data/output/shape/network_tessellations/{city}/{city}_tessellations_nodes_before_consolidation.shp\"\n",
    "tess_edges_dir = first_folder_path+ f\"data/output/shape/network_tessellations/{city}/{city}_tessellations_edges_before_consolidation.shp\"\n",
    "\n",
    "# ----- ----- ----- Projection to be used when needed ----- ----- -----\n",
    "if city == 'guadalajara':\n",
    "    projected_crs = \"EPSG:32613\"\n",
    "elif city == 'medellin':\n",
    "    projected_crs = \"EPSG:32618\"\n",
    "    \n",
    "# ----- ----- ----- Output ----- ----- -----\n",
    "# Output dir - Specify where outputs will be saved [Create folder path manually if creation fails]\n",
    "output_dir = first_folder_path + f\"data/output/shape/network_project/{city}/\"\n",
    "# Create folder if it doesn't exist\n",
    "if os.path.exists(output_dir) == False:\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "# Outputs to be saved - Specify which steps will be saved (Final result is localsave_02_07)\n",
    "# PART 01 STEP 01 - Saves pre-formated Boeing nodes and edges [Base network], and tessellation nodes and edges [Complementary network]\n",
    "localsave_01_01 = False\n",
    "# PART 01 STEP 02 - Saves the uncovered parts of the complementary network (nodes and edges) and the contact nodes that will be used to connect both networks.\n",
    "localsave_01_02 = False\n",
    "# PART 02 STEP 01 - Saves both networks (Base, complementary) nodes and edges after being intersected by themselves. Saves the intersection nodes.\n",
    "localsave_02_01 = False\n",
    "# PART 02 STEP 02 - Saves the Dataframe that has the connection's data between both networks (connected_nodes_df) and the intersections to be performed on the Base ntw (intersection_nodes_2)\n",
    "localsave_02_02 = False\n",
    "# PART 02 STEP 03 - Saves both networks joined (concatenated_nodes, concatenated_edges)\n",
    "localsave_02_03 = False\n",
    "# PART 02 STEP 04 - Saves both networks after being joined (joined_nodes_fix, joined_edges_fix)\n",
    "localsave_02_04 = False\n",
    "# PART 02 STEP 05 - Saves both networks after being cleaned (joined_nodes_clean, joined_edges_clean)\n",
    "localsave_02_05 = False\n",
    "# PART 02 STEP 06 - Saves both networks after being consolidated (Close nodes become one node, edges travel up to that consolidated node)\n",
    "localsave_02_06 = False\n",
    "# PART 02 STEP 07 - Saves both networks after being rebuilt (Consolidation creates osmid in the form of strings [osmid_1, osmid_2], this process\n",
    "# restores network to coordinate-based IDs (osmid, u, v))\n",
    "localsave_02_07 = False\n",
    "\n",
    "# ----- ----- ----- Previous step data loading (Mainly for building and debugging the code) ----- ----- -----\n",
    "# If there's a need to load previously saved outputs in order to start running from a specific step, turn to True\n",
    "\n",
    "# Data for part01_step02_identifyuncovered (boeing_nodes, boeing_edges, tess_nodes, tess_edges)\n",
    "load_datafor_part01_step02 = False\n",
    "# Data for part02_step01_ntwsintersection (boeing_nodes, boeing_edges, complementary_uncovered_nodes_clean, complementary_uncovered_edges_clean, contact_nodes)\n",
    "load_datafor_part02_step01 = False\n",
    "# Data for part02_step02_connectidentif (contact_nodes, base_nodes_i, base_edges_i, comp_nodes_i, comp_edges_i)\n",
    "load_datafor_part02_step02 = False\n",
    "# Data for part02_step03_ntwsconcat (base_nodes_i, base_edges_i, comp_nodes_i, comp_edges_i, intersection_nodes_2)\n",
    "load_datafor_part02_step03 = False\n",
    "# Data for part02_step04_ntwsconcat (connected_nodes, concatenated_nodes, concatenated_edges) [Draw edges and identify consequential intersections]\n",
    "load_datafor_part02_step04 = False\n",
    "# Data for substep inside part02_step04_ntwsconcat. Substep that fixes consequential intersections. (concatenated nodes, joined_edges_concat, intersection_nodes_3)\n",
    "load_datafor_substep_part02_step04 = False\n",
    "# Data for part02_step05_ntwsclean (joined_nodes_fix, joined_edges_fix)\n",
    "load_datafor_part02_step05 = False\n",
    "# Data for part02_step06_ntwsconsolidation (project_nodes, project_edges)\n",
    "load_datafor_part02_step06 = False\n",
    "# Data for part02_step07_ntwsrebuild (cons_nodes, cons_edges)\n",
    "load_datafor_part02_step07 = False\n",
    "\n",
    "# Show dirs\n",
    "print(boeing_nodes_dir)\n",
    "print(boeing_edges_dir)\n",
    "print(tess_nodes_dir)\n",
    "print(tess_edges_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d2dd80-558d-4959-ad47-38e1aaa287b6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### __Unzip files if necessary -__ Checks for .shp, .gpkg or .geojson files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14602cc4-64b0-4275-a675-ca5c28605ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directories = [boeing_nodes_dir,boeing_edges_dir,tess_nodes_dir,tess_edges_dir]\n",
    "output_directories = []\n",
    "\n",
    "for input_dir in input_directories:\n",
    "    if input_dir.endswith('.zip'):\n",
    "        # Extract .zip\n",
    "        with zipfile.ZipFile(input_dir, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(output_dir)\n",
    "        # Search for files with geospatial format inside extracted file\n",
    "        shp_files = glob.glob(os.path.join(extract_path, \"*.shp\"))\n",
    "        gpkg_files = glob.glob(os.path.join(extract_path, \"*.gpkg\"))\n",
    "        geojson_files = glob.glob(os.path.join(extract_path, \"*.geojson\"))\n",
    "        # Choose file (Priority: SHP>GPKG>GEOJSON)\n",
    "        if shp_files:\n",
    "            input_dir = shp_files[0]  # Usar el primer .shp encontrado\n",
    "        elif gpkg_files:\n",
    "            input_dir = gpkg_files[0]\n",
    "        elif geojson_files:\n",
    "            input_dir = geojson_files[0]\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"No geospatial files found on {extract_path}.\")\n",
    "            \n",
    "    # Add directory as output dir\n",
    "    output_directories.append(input_dir)  \n",
    "\n",
    "# Re-assign variables the updated values\n",
    "boeing_nodes_dir, boeing_edges_dir, tess_nodes_dir, tess_edges_dir = output_directories\n",
    "\n",
    "# Show dirs\n",
    "print(boeing_nodes_dir)\n",
    "print(boeing_edges_dir)\n",
    "print(tess_nodes_dir)\n",
    "print(tess_edges_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0a4748-d4a3-4477-8f9f-e5ae5b56c669",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## __Functions__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea2ef01-9cc5-4061-9929-19f3273daa46",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### __Functions -__ Common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9571bc-7d9b-4431-935b-eb01403eead3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_osmid(nodes_network_1, nodes_network_2, previously_produced):\n",
    "    \n",
    "    stop = False\n",
    "    produced_osmid = previously_produced\n",
    "    \n",
    "    while stop == False:\n",
    "\n",
    "        # Evaluate if fabricated_osmid exists in any network\n",
    "        if (produced_osmid in list(nodes_network_1.osmid.unique())) or (produced_osmid in list(nodes_network_2.osmid.unique())):\n",
    "            # Try the next one\n",
    "            produced_osmid +=1\n",
    "            \n",
    "        else:\n",
    "            # Reached an unique fabricated_osmid\n",
    "            stop = True\n",
    "            return produced_osmid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4354a40f-7454-4e1d-a0fb-30aa928638fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance between two points\n",
    "def distance_between_points(point1, point2):\n",
    "    return round(math.sqrt((point1[0] - point2[0])**2 + (point1[1] - point2[1])**2), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2584e5ab-2f8a-42be-919e-a5a3f67382e9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### __Functions -__ Identify_uncovered [Used in Part01_Step02]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a063c1-83e4-44c6-bb1a-3a26f79c792c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_uncovered(base_nodes, base_edges, complementary_nodes, complementary_edges, contact_analysis_dist, projected_crs=\"EPSG:6372\"):\n",
    "    \"\"\" This function identifies zones within a complementary network (nodes and edges) where currently there's no coverture in a base network.\n",
    "\tArgs:\n",
    "\t\tbase_nodes (geopandas.GeoDataFrame): GeoDataFrame containing nodes of the base network. \n",
    "        base_edges (geopandas.GeoDataFrame): GeoDataFrame containing edges of the base network. \n",
    "\t\tcomplementary_nodes  (geopandas.GeoDataFrame): GeoDataFrame containing nodes of the complementary network.\n",
    "\t\tcomplementary_edges  (geopandas.GeoDataFrame): GeoDataFrame containing edges of the complementary network.\n",
    "\t\tcontact_analysis_dist (float): Distance (meters) used when deciding which nodes from the complementary network should be added to the base network.\n",
    "                                A buffer of {contact_analysis_dist} is created around all center points of each complementary_edge.\n",
    "                                If the buffer touches any base_edges, the complementary_edge is considered as already covered by the base network. \n",
    "                                If the buffer does not touches any base_edge, the complementary_edge is considered uncovered.\n",
    "\t\tprojected_crs (str, optional): string containing projected crs to be used depending on area of interest. Defaults to \"EPSG:6372\".\n",
    "\n",
    "\tReturns:\n",
    "        complementary_uncovered_nodes (geopandas.GeoDataFrame): GeoDataFrame with nodes from the complementary network that are located \n",
    "                                                                in a zone not covered by the base network.\n",
    "        complementary_uncovered_nodes (geopandas.GeoDataFrame): GeoDataFrame with edges from the complementary network that are located\n",
    "                                                                in a zone not covered by the base network.\n",
    "\t\tcontact_nodes (geopandas.GeoDataFrame): GeoDataFrame with nodes from the complementary network that could be used to \n",
    "                                                connect an uncovered zone to a covered zone.\n",
    "\t\"\"\"\n",
    "\n",
    "    # Turn on or off function logs (General Logs)\n",
    "    function_logs = True\n",
    "    # Turn on or off debbugging logs (Lots of logs used for debugging)\n",
    "    debugging_logs = False\n",
    "    \n",
    "    # 1.0 --------------- Extract mid_point of each complementary edge\n",
    "    if function_logs:\n",
    "        print(\"1.0 - Extracting mid_point point of each complementary edge.\")\n",
    "    # ------------------- INPUT USED - READ COMPLEMENTARY EDGES\n",
    "    complementary_edges = complementary_edges.copy()\n",
    "    complementary_edges = complementary_edges.to_crs(projected_crs)\n",
    "    # ------------------- INPUT USED - READ COMPLEMENTARY EDGES\n",
    "\n",
    "    # Create unique ID for each edge using u+v+key\n",
    "    complementary_edges = src.create_unique_edge_id(complementary_edges)\n",
    "    # Find mid_point of each edge\n",
    "    complementary_edges['mid_point'] = complementary_edges.interpolate(complementary_edges.length / 2)\n",
    "    # Assign mid_point to its own gdf and drop column 'mid_point' from complementary_edges\n",
    "    mid_points = complementary_edges[['edge_id','mid_point']].copy()\n",
    "    mid_points.rename(columns={'mid_point':'geometry'},inplace=True)\n",
    "    complementary_edges.drop(columns=['mid_point'],inplace=True)\n",
    "    \n",
    "    # 2.0 --------------- Create contact-analysis buffer around mid_points using contact_analysis_dist \n",
    "    # ------------------- (keep edge-of-origin data)\n",
    "    if function_logs:\n",
    "        print(\"2.0 - Creating contact-analysis buffer around each mid_point.\")\n",
    "    \n",
    "    # Reset mid_points's index (Keeps data ordered starting from 0)\n",
    "    mid_points.reset_index(inplace=True,drop=True) #--> Resets index without saving col 'index'\n",
    "    # Save each mid_point's reseted index in a column named 'index'\n",
    "    points_to_buffer = mid_points.copy()\n",
    "    points_to_buffer.reset_index(inplace=True) #--> Also creates a col 'index', same order since it is reseted\n",
    "    # Create a gdf containing the contact-analysis buffer around mid_points\n",
    "    mid_points_buffer = points_to_buffer.buffer(contact_analysis_dist)\n",
    "    mid_points_buffer = gpd.GeoDataFrame(geometry=mid_points_buffer)\n",
    "    mid_points_buffer.reset_index(inplace=True) #--> Also creates a col 'index', same order since it is reseted\n",
    "    # Transfer data from mid_points to it's buffer using the previously reseted index as merge col\n",
    "    points_to_buffer.drop(columns=['geometry'],inplace=True)\n",
    "    mid_points_buffer = pd.merge(mid_points_buffer,points_to_buffer,on='index') #--> Merges using common reseted col 'index'\n",
    "    mid_points_buffer.drop(columns=['index'],inplace=True)\n",
    "\n",
    "    # Save disk space\n",
    "    del points_to_buffer\n",
    "    \n",
    "    # 3.0 --------------- Find mid_points whose buffer does not intersect with any part of the base network (Considering base_edges).\n",
    "    # ------------------- [This step creates function output COMPLEMENTARY_UNCOVERED_EDGES]\n",
    "    if function_logs:\n",
    "        print(\"3.0 - Extracting complementary_uncovered_edges.\")\n",
    "    \n",
    "    # ------------------- INPUT USED - READ BASE EDGES\n",
    "    base_edges = base_edges.copy()\n",
    "    base_edges = base_edges.to_crs(projected_crs)\n",
    "    # ------------------- INPUT USED - READ BASE EDGES\n",
    "    \n",
    "    # Buffers that touch any base edge\n",
    "    buffer_touch = mid_points_buffer.sjoin(base_edges)\n",
    "    # All unique complementary edge_ids whose mid_point's buffer touched any base_edge\n",
    "    edge_id_touch_lst = list(buffer_touch.edge_id.unique())\n",
    "    # Complementary edges that are NOT(~) near any base edge\n",
    "    complementary_uncovered_edges = complementary_edges.loc[~complementary_edges.edge_id.isin(edge_id_touch_lst)].copy()\n",
    "    complementary_uncovered_edges.reset_index(inplace=True,drop=True) #--> Resets index without saving col 'index'\n",
    "\n",
    "    # 4.0 --------------- Select the complementary_nodes that connect to the complementary_uncovered_edges\n",
    "    # ------------------- [This step creates function output COMPLEMENTARY_UNCOVERED_NODES]\n",
    "    if function_logs:\n",
    "        print(\"4.0 - Extracting complementary_uncovered_nodes.\")\n",
    "    \n",
    "    # ------------------- INPUT USED - READ COMPLEMENTARY NODES\n",
    "    complementary_nodes = complementary_nodes.copy()\n",
    "    complementary_nodes = complementary_nodes.to_crs(projected_crs)\n",
    "    # ------------------- INPUT USED - READ COMPLEMENTARY NODES \n",
    "    \n",
    "    # List of unique 'u's and 'v's that are connected to the complementary_uncovered_edges \n",
    "    complementary_uncovered_osmid_lst = set(list(complementary_uncovered_edges.u.unique()) + list(complementary_uncovered_edges.v.unique()))\n",
    "    # Select any node where its 'osmid' IS in complementary_uncovered_osmid_lst\n",
    "    complementary_uncovered_nodes = complementary_nodes.loc[complementary_nodes.osmid.isin(complementary_uncovered_osmid_lst)].copy()\n",
    "    # [Note: This nodes won't necessarily be in the uncovered zone since they could belong to \n",
    "    # an edge whose mid_point is far from the base network, but whose path extends into the base network.]\n",
    "\n",
    "    # 5.0 --------------- Find the nodes that would be used to connect the uncovered part of the complementary network to the base network.\n",
    "    # ------------------- [This step creates function output CONTACT_NODES]\n",
    "    if function_logs:\n",
    "        print(\"5.0 - Extracting contact_nodes.\")\n",
    "\n",
    "    # 5.1 --- Create a buffer around all complementary_uncovered_nodes \n",
    "    # Reset complementary_uncovered_nodes's index\n",
    "    complementary_uncovered_nodes.reset_index(inplace=True,drop=True) #--> Resets index without saving col 'index'\n",
    "    # Save each complementary_uncovered_nodes's reseted index in a column named 'index'\n",
    "    nodes_to_buffer = complementary_uncovered_nodes.copy()\n",
    "    nodes_to_buffer.reset_index(inplace=True) #--> Also creates a col 'index', same order since it is reseted\n",
    "    # Create a gdf containing the buffer around complementary_uncovered_nodes\n",
    "    complementary_uncovered_nodes_buffer = nodes_to_buffer.buffer(contact_analysis_dist)\n",
    "    complementary_uncovered_nodes_buffer = gpd.GeoDataFrame(geometry=complementary_uncovered_nodes_buffer)\n",
    "    complementary_uncovered_nodes_buffer.reset_index(inplace=True) #--> Also creates a col 'index', same order since it is reseted\n",
    "    # Transfer data from complementary_uncovered_nodes to it's buffer using the index as merge col\n",
    "    nodes_to_buffer.drop(columns=['geometry'],inplace=True)\n",
    "    complementary_uncovered_nodes_buffer = pd.merge(complementary_uncovered_nodes_buffer,nodes_to_buffer,on='index') #--> Merges using common reseted col 'index'\n",
    "    complementary_uncovered_nodes_buffer.drop(columns=['index'],inplace=True)\n",
    "\n",
    "    # Save disk space\n",
    "    del nodes_to_buffer\n",
    "    \n",
    "    # 5.2 --- Find complementary_uncovered_nodes whose buffer DOES intersect with any part of the base network (Considering base_edges).\n",
    "    # Buffers that touch any base edge\n",
    "    buffer_touch = complementary_uncovered_nodes_buffer.sjoin(base_edges)\n",
    "    # All unique osmids that touched any base_edge\n",
    "    contact_osmids = list(buffer_touch.osmid.unique())\n",
    "    # Complementary nodes that ARE near any base_edge\n",
    "    contact_nodes = complementary_uncovered_nodes.loc[complementary_uncovered_nodes.osmid.isin(contact_osmids)].copy()\n",
    "    contact_nodes.reset_index(inplace=True,drop=True) #--> Resets index without saving col 'index'\n",
    "    \n",
    "    # 6.0 --------------- Identify and shorten edges that extend into the base network and would be usefull to create connections.\n",
    "    # ------------------- (Up to this step, edges whose mid_points_buffer is in contact with the base network are not included in the\n",
    "    # ------------------- function's output since they are considered to be in an already-covered zone. However, some edges (if shortened)\n",
    "    # ------------------- could be usefull to connect the uncovered zone to the covered zone. This step takes the edge and shortens (Clips)\n",
    "    # ------------------- the edge until it's mid_point_buffer is no longer in contact with the base network)\n",
    "    # ------------------- [This step updates the uncovered network (nodes and edges) and the contact nodes]\n",
    "    if function_logs:\n",
    "        print(\"6.0 - Creating missing connections through complementary_edges that travel from the uncovered zone to the base network.\")\n",
    "    \n",
    "    # LOG CODE - Progress logs\n",
    "    # Will create progress logs when progress reaches these percentages:\n",
    "    progress_logs = [0,5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100] # for log statistics\n",
    "    osmid_count = 0\n",
    "    # LOG CODE - Progress logs\n",
    "    \n",
    "    # 6.0 --- PREPARATION FOR ANALYSIS - Supporting elements\n",
    "    # Keep track of the amount of edges that underwent a shortening process after finishing this section\n",
    "    fabricated_count = 0\n",
    "    # Keep track of which parts of the network where fabricated with the following code.\n",
    "    # All nodes and edges that keep its original geometry will be assigned 'clipping_i' = 0.\n",
    "    # All edges whose geometry was clipped will be assigned the amount of shortening (clipping) iterations used in them.\n",
    "    complementary_uncovered_nodes['clipping_i'] = 0\n",
    "    complementary_uncovered_edges['clipping_i'] = 0\n",
    "    complementary_uncovered_edges['original_edge_id'] = ''\n",
    "    contact_nodes['clipping_i'] = 0\n",
    "    # Create empty GeoDataFrame to data used in the process (Used for GIS visualization purposes)\n",
    "    original_diverging_nodes = gpd.GeoDataFrame()\n",
    "    original_diverging_edges = gpd.GeoDataFrame()\n",
    "    dropped_overlapping_edges = gpd.GeoDataFrame()\n",
    "\n",
    "    # 6.0 --- PREPARATION FOR ANALYSIS - Nodes and osmids\n",
    "    # Read base_nodes once (will be used to assess if the osmid being produced is unique to both input networks)\n",
    "    # ------------------- INPUT USED - READ BASE EDGES\n",
    "    base_nodes = base_nodes.copy()\n",
    "    base_nodes = base_nodes.to_crs(projected_crs)\n",
    "    # ------------------- INPUT USED - READ BASE EDGES\n",
    "    # Previously produced osmid. Since will be creating non-existing nodes, function produce_osmid() will use a starting number for\n",
    "    # trying to produce unique osmids. That function will check if that osmid already exists in either the base or complementary network.\n",
    "    # Start with number 0.\n",
    "    previously_produced = 0\n",
    "    # Find all complementary_uncovered_osmids (from previously created complementary_uncovered_osmid_lst)\n",
    "    # that are NOT a contact osmid (contact osmids are those that already serve as a connection to the base network)\n",
    "    non_contact_osmids = [osmid for osmid in complementary_uncovered_osmid_lst if osmid not in contact_osmids]\n",
    "    \n",
    "    a=\"\"\"\n",
    "    # 6.0 --- PREPARATION FOR ANALYSIS - Dictionaries\n",
    "    # Shortening dict\n",
    "    # Sometimes an edge could get shortened from both sides. \n",
    "    # (Clipped with starting point 'u' and then, on another case, clipped with starting point 'u')\n",
    "    # If an edge will be shortened from both sides exactly once (Shortened from 'u' to midpoint and from 'v' to midpoint),\n",
    "    # there would be two different new nodes in the same place. The dict helps make sure that only one contact_node is created.\n",
    "    # If not considered, this particular situation can create two different edges that coincide in two different contact_nodes exactly in the midpoint.\n",
    "    # (This is the main reason why it is necessary to keep track of which edges where shortened and up to which point)\n",
    "    shortening_dict = {}\n",
    "\n",
    "    # diverging_osmid dict\n",
    "    # Stores each shortened edge (original_edge_id) deriving from each nodes (osmid)\n",
    "    # {osmid:[shortened_original_edge_id_1, shortened_original_edge_id_2, ...]}\n",
    "    diverging_osmid_dict = {}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Review each node in the uncovered zone of the complementary network that is not already a contact_node\n",
    "    for osmid in non_contact_osmids:\n",
    "\n",
    "        # Development checks -----------------------------------\n",
    "        #dev_check = [39125252780557922]\n",
    "        #if osmid not in dev_check:\n",
    "        #    continue\n",
    "        #else:\n",
    "        #    print(\"--\"*50)\n",
    "        #    print(f\"DEVELOPMENT CHECK FOR OSMID {osmid}.\")\n",
    "        # Development checks -----------------------------------\n",
    "        \n",
    "        # LOG CODE - Progress logs\n",
    "        # Measures current progress, prints if passed a checkpoint of progress_logs list.\n",
    "        current_progress = (osmid_count / len(non_contact_osmids))*100\n",
    "        for checkpoint in progress_logs:\n",
    "            if (current_progress >= checkpoint) and function_logs:\n",
    "                print(f\"6.0 - Exploring osmids. {checkpoint}% done.\")\n",
    "                progress_logs.remove(checkpoint)\n",
    "                break\n",
    "        # LOG CODE - Progress logs\n",
    "\n",
    "        # Retrieve it's edges (Will be refered as diverging_edges). \n",
    "        # Must consider all edges (takes them from complementary_edges) and not only complementary_uncovered_edges since\n",
    "        # we are looking to identify if any edge that comes out of that osmid goes towards base_network.\n",
    "        diverging_edges = complementary_edges.loc[(complementary_edges.u==osmid) | (complementary_edges.v==osmid)].copy()\n",
    "        diverging_edges_ids = list(diverging_edges.edge_id.unique())\n",
    "        \n",
    "        # For each edge diverging from current node:\n",
    "        for diverging_edge_id in diverging_edges_ids:    \n",
    "                \n",
    "            # If the edge DOES touch the base network:\n",
    "            if diverging_edge_id in edge_id_touch_lst: # (Previously created edge_id_touch_lst)\n",
    "                # If an edge reaches this part of the code, it means that it is a complementary_edge that:\n",
    "                # a) Comes out from an node that's located in the uncovered zone (complementary_uncovered_node).\n",
    "                # b) The node it came out from is NOT a contact_node, it is far from the base network (According to contact_analysis_dist)\n",
    "                # c) The edge's current mid_point is located in proximity to the base network (According to contact_analysis_dist)\n",
    "                # --> Between the base_network and this edge itself, a connection point should be identified.\n",
    "                # --> Objective: Identify that connection (new contact_node) between the complementary and base network.\n",
    "\n",
    "                try:\n",
    "                    # 6.1 --- Retrieve current diverging_osmid and diverging_node\n",
    "                    # Save the osmid from the node which the current edge uses to come out from the uncovered zone into the base network.\n",
    "                    diverging_osmid = osmid\n",
    "                    if debugging_logs:\n",
    "                        print(\"--\"*30)\n",
    "                        print(f\"Entering analysis for:\")\n",
    "                        print(f\"Diverging_osmid: {diverging_osmid}. Type: {type(diverging_osmid)}.\")\n",
    "                        print(f\"Diverging_edge_id: {diverging_edge_id}. Type: {type(diverging_edge_id)}.\")\n",
    "                        \n",
    "                    # Extract its node\n",
    "                    if debugging_logs:\n",
    "                        print(f\"Looking for diverging_node in complementary_uncovered_nodes. Printing complementary_uncovered_nodes dtypes.\")\n",
    "                        print(complementary_uncovered_nodes.dtypes)\n",
    "                    diverging_node = complementary_uncovered_nodes.loc[complementary_uncovered_nodes.osmid == diverging_osmid].copy()\n",
    "                    if debugging_logs:\n",
    "                        print(f\"Diverging nodes found: {len(diverging_node)}.\")\n",
    "                    # Reset index (so that accessing its geometry is always .loc[0,'geometry'])\n",
    "                    diverging_node.reset_index(inplace=True, drop=True)\n",
    "                    if debugging_logs:\n",
    "                        print(f\"Diverging node's geometry: {diverging_node.loc[0,'geometry']}.\")\n",
    "                    # Add current diverging_node to original_diverging_nodes gdf (For GIS visualization purposes)\n",
    "                    original_diverging_nodes = pd.concat([original_diverging_nodes, diverging_node])\n",
    "    \n",
    "                    # 6.2 --- Retrieve current diverging_edge as the connection_edge\n",
    "                    # Select the edge that diverts from the diverging_node towards the base_network\n",
    "                    connection_edge = complementary_edges.loc[complementary_edges.edge_id == diverging_edge_id].copy()\n",
    "                    # Reset index (so that accessing its data is always .loc[0,'data'])\n",
    "                    connection_edge.reset_index(inplace=True,drop=True)\n",
    "                    # The connection_edge geometry will suffer modifications, save original\n",
    "                    original_connection_edge = connection_edge.copy() \n",
    "                    # Add current connection_edge to original_diverging_edges gdf (For GIS visualization purposes)\n",
    "                    original_diverging_edges = pd.concat([original_diverging_edges, connection_edge])\n",
    "    \n",
    "                    # 6.3 --- Clip connection_edge until it's mid_point is no longer in proximity to the base network.\n",
    "                    # ------- When this point is reached, assign its ending_point (previous mid_point) as a new contact_node.\n",
    "    \n",
    "    \n",
    "                    # 6.3.0 - Find the clipping orientation\n",
    "                    # --------- It is unsure whether the diverging_node is located at the coordinate 0 or at the last coordinate of the connection_edge.\n",
    "                    # --------- So it is necessary to identify which one is which.\n",
    "                    # Extract the edge's coordinates list\n",
    "                    connection_edge_coords = list(connection_edge['geometry'][0].coords)\n",
    "                    # Extract the starting_point's coordinates (Known to be the diverging_node)\n",
    "                    starting_point_coords = diverging_node.loc[0,'geometry'].coords[0]\n",
    "                    # Obtain the ending_point's coordinates (It is the previous iteration edge's mid_point)\n",
    "                    # (It is either the first or last coordinate of the connection_edge)\n",
    "                    if starting_point_coords == connection_edge_coords[0]:\n",
    "                        node_edge_relation = 'starting'\n",
    "                    elif starting_point_coords == connection_edge_coords[-1]:\n",
    "                        node_edge_relation = 'ending'\n",
    "                    else:\n",
    "                        print(f\"Problem with osmid: {osmid}. Diverging edge_id: {diverging_edge_id}.\")\n",
    "                        print(starting_point_coords)\n",
    "                        print(connection_edge_coords[0])\n",
    "                        print(connection_edge_coords[-1])\n",
    "                        intented_crash\n",
    "                        node_edge_relation = 'missing'\n",
    "                \n",
    "                except:\n",
    "                    print(f\"Failed while preparing the data to analyse {diverging_edge_id} diverging from osmid {diverging_osmid}.\")\n",
    "                    intented_crash\n",
    "            \n",
    "                # Kickstart while loop for current connection_edge\n",
    "                stop = False\n",
    "                shorten_i = 0\n",
    "                \n",
    "                while (stop == False):\n",
    "\n",
    "                    # Limit of attempts\n",
    "                    limit_of_attempts =50\n",
    "                    if shorten_i >= limit_of_attempts:\n",
    "                        if debugging_logs:\n",
    "                            print(f\"Tried shortening {limit_of_attempts} times (To 0.5% its original size) edge. Stopped.\")\n",
    "                            print(f\"Edge data: u {original_connection_edge.u.unique()[0]}, v {original_connection_edge.v.unique()[0]}.\")\n",
    "                        break\n",
    "                        \n",
    "                    # 6.3.1 --- Calculate the edge's mid_point.\n",
    "                    # --------- [Will become the clipping_point in function edge_clipping()].\n",
    "\n",
    "                    # Set clipping factor\n",
    "                    if node_edge_relation == 'starting':\n",
    "                        # This means that length will be measured starting from the diverging node.\n",
    "                        # We're looking to reduce to 90% that length each loop.\n",
    "                        interpolation_length = connection_edge.length * 0.90\n",
    "                    elif node_edge_relation == 'ending':\n",
    "                        # This means that length will be measured starting from the opposite node to the diverging node.\n",
    "                        # We're looking to cut the first 10% of that length each loop.\n",
    "                        interpolation_length = connection_edge.length * 0.10\n",
    "                    elif node_edge_relation == 'back_to_original':\n",
    "                        # Check \"No-shortening test\" for more information\n",
    "                        interpolation_length = connection_edge.length * 0.50\n",
    "                    else:\n",
    "                        interpolation_length = 'missing'\n",
    "                    \n",
    "                    # Calculate the connection_edge's mid_point\n",
    "                    connection_edge['mid_point'] = connection_edge.interpolate(interpolation_length)\n",
    "                    # Assign mid_point to its own gdf and drop column 'mid_point' from connection_edge gdf\n",
    "                    edge_mid_point = connection_edge[['edge_id','mid_point']].copy()\n",
    "                    edge_mid_point.rename(columns={'mid_point':'geometry'},inplace=True)\n",
    "                    connection_edge.drop(columns=['mid_point'],inplace=True)\n",
    "                \n",
    "                    # 6.3.2 --- Evaluate if the current mid_point is still in proximity to the base network\n",
    "                    # 6.3.2.a - Create contact-analysis buffer around the edge_mid_point\n",
    "                    # Reset edge_mid_point's index\n",
    "                    edge_mid_point.reset_index(inplace=True,drop=True) #--> Resets index without saving col 'index'\n",
    "                    # Save edge_mid_point's reseted index in a column named 'index'\n",
    "                    point_to_buffer = edge_mid_point.copy()\n",
    "                    point_to_buffer.reset_index(inplace=True) #--> Also creates a col 'index', same order since it is reseted\n",
    "                    # Create a gdf containing the contact-analysis buffer around the edge_mid_point\n",
    "                    mid_point_buffer = point_to_buffer.buffer(contact_analysis_dist)\n",
    "                    mid_point_buffer = gpd.GeoDataFrame(geometry=mid_point_buffer)\n",
    "                    mid_point_buffer.reset_index(inplace=True) #--> Also creates a col 'index', same order since it is reseted\n",
    "                    # Transfer data from edge_mid_point to it's buffer using the previously reseted index as merge col\n",
    "                    point_to_buffer.drop(columns=['geometry'],inplace=True)\n",
    "                    mid_point_buffer = pd.merge(mid_point_buffer,point_to_buffer,on='index') #--> Merges using common reseted col 'index'\n",
    "                    mid_point_buffer.drop(columns=['index'],inplace=True)\n",
    "                    \n",
    "                    # 6.3.2.b - Find if current mid_point it's still in proximity to base network\n",
    "                    # Buffers that touch any base edge\n",
    "                    buffer_touch = mid_point_buffer.sjoin(base_edges)\n",
    "                    \n",
    "                    if len(buffer_touch) > 0:\n",
    "                        \n",
    "                        # 6.3.3 --- If it still touches, reduce in size.\n",
    "                        # If the mid_point_buffer still touches the base edges, the line is most likely still overlaping with the base network.\n",
    "                        # Apply edge_clipping function to shorten the edge up until the current mid_point\n",
    "                        connection_edge = edge_clipping(starting_point_gdf = diverging_node,\n",
    "                                                        edge_gdf = connection_edge,\n",
    "                                                        clipping_point_gdf = edge_mid_point,\n",
    "                                                        projected_crs = projected_crs)\n",
    "                        # Keep connection_edge format\n",
    "                        connection_edge['edge_id'] = diverging_edge_id\n",
    "                        # Count shortening iteration\n",
    "                        shorten_i+=1\n",
    "                    \n",
    "                    else:\n",
    "\n",
    "                        # 6.3.3 --- If the buffer no longer touches any part of the base network, shortening was a success.\n",
    "                        # --------- However, before registering the shortened edge it it necessary to run various TESTS:\n",
    "\n",
    "                        # ------------------------------ NO-SHORTENING TEST START ------------------------------\n",
    "                        # ORIGIN OF PROBLEM:\n",
    "                        # Originally, the whole test was performed using the mid_point (hence the various relations to the name \"mid_point\").\n",
    "                        # However this meant that some edges were cut way too much and ended up far from the base_network, creating unnecesary long connections.\n",
    "                        # Now the clipping is performed by keeping 90% of the original edge each iteration.\n",
    "                        # CURRENT SITUATION:\n",
    "                        # The part of the code where edge_id_touch_lst is created still runs using mid_points, meaning that each diverging_edge_id examined here has its midpoint in proximity to the base network.\n",
    "                        # However, since the clipping and checking inside the loop is performed using not the midpoint, but the 90% length, there are cases where an edge is NOT shortened not even once.\n",
    "                        # The edge came here using the mid_point check, then when the 90%length's proximity to the base network was used it indicated that the point was no longer in contact with the base network.\n",
    "                        # However, the edge suffered no changes, its proximity to the base network was examined from different points.\n",
    "                        # SOLUTION:\n",
    "                        # If an edge ends here with shorten_i == 0, it has not been shortened.\n",
    "                        # Set node_edge_relation to \"original\", which triggers a mid_point (50%length) clipping each loop.\n",
    "                        if shorten_i == 0:\n",
    "                            node_edge_relation = \"back_to_original\"\n",
    "                            continue\n",
    "\n",
    "                        a=\"\"\"\n",
    "                        # ------------------------------ NO-SHORTENING TEST END ------------------------------\n",
    "                        \n",
    "                        # Double-once-shortening test STARTS ### ### ### ### ### ### ### ### ### ### ### ### \n",
    "                        # EXPLANATION:\n",
    "                        # A complementary_edge could be shortened twice, once from each starting_point. This is a \"double shortening\".\n",
    "                        # It occurs when an edge was shortened from node 1 (e.g. from 'u') and now is being shortened from node 2 (e.g. from 'v').\n",
    "                        # This process produces no problems, **unless both shortening processess undergo just 1 iteration**.\n",
    "                        # In that specific case, both lines start in their nodes ('u' or 'v') and end at the original line's midpoint.\n",
    "                        # This produces TWO different contact_nodes in a very similar location, that could result in TWO similar but different treatments and strange geometries.\n",
    "                        # In order to avoid this, this double_shortening check is conduced in order to make sure that \n",
    "                        # at the end of both processes, just ONE contact node is produced and used by both edges ending on it.\n",
    "\n",
    "                        # If this is NOT the first fabrication case and edge AND the current edge was shortened ONLY once:\n",
    "                        if (fabricated_count > 0) and (shorten_i==1):\n",
    "    \n",
    "                            # Load all original edge_ids that have been shortened ONLY once\n",
    "                            already_shortened_once = complementary_uncovered_edges.loc[complementary_uncovered_edges.clipping_i==1].copy()\n",
    "                            already_shortened_once_lst = list(already_shortened_once.original_edge_id.unique())\n",
    "\n",
    "                            # Check if current diverging_edge_id has already been shortened ONLY once\n",
    "                            if diverging_edge_id in already_shortened_once_lst:\n",
    "                                # If an edge reaches this part of the code, the edge was already shortened ONCE from one end, and was being shortened again ONCE from the other end.\n",
    "                                # Next step --> Do NOT produce a new contact osmid and new point for current edge. Use the existing one. \n",
    "                                # ------------> Register the edge under the existing contact_node instead of creating a new one.\n",
    "\n",
    "                                if debugging_logs:\n",
    "                                    print(f\"Edge {diverging_edge_id} was already shortened [once]. Shortening [once] again from osmid {diverging_osmid}.\")\n",
    "\n",
    "                                # 6.3.3.a1 - Identify the previously produced middle osmid.\n",
    "                                # Retrieve the ORIGINAL (current original_connection_edge's) 'u' and 'v'\n",
    "                                original_u = original_connection_edge.u.unique()[0]\n",
    "                                original_v = original_connection_edge.v.unique()[0]\n",
    "                                original_osmids = [original_u, original_v]\n",
    "                                if debugging_logs:\n",
    "                                    print(f\"Edge's original osmids (u,v) were {original_osmids}.\")\n",
    "                                \n",
    "                                # Retrieve the PREVIOUSLY PRODUCED (previously shortened edge) 'u' and 'v'\n",
    "                                prev_produced_edge = complementary_uncovered_edges.loc[complementary_uncovered_edges.original_edge_id == diverging_edge_id].copy()\n",
    "                                new_u = prev_produced_edge.u.unique()[0]\n",
    "                                new_v = prev_produced_edge.v.unique()[0]\n",
    "                                new_osmids = [new_u, new_v]\n",
    "                                if debugging_logs:\n",
    "                                    print(f\"The first shortening is using osmids (u,v) {new_osmids}.\")\n",
    "                                \n",
    "                                # Identify which osmid is in the new_osmids list but not in the original_osmids list.\n",
    "                                # (Which osmid was produced here, in step 6.3.3)\n",
    "                                for osmid_check in new_osmids:\n",
    "                                    if osmid_check not in original_osmids:\n",
    "                                        produced_osmid = osmid_check\n",
    "\n",
    "                                # 6.3.3.a2 - Add the connection_edge as a new edge to complementary_uncovered_edges gdf (But not the node, the node is already there).\n",
    "                                # Retrieve the diverging_osmid's position in the ORIGINAL connection_edge. (Whether 'u' or 'v')\n",
    "                                # In order to keep that original diverging_node in its position and add the produced_osmid in the other position.\n",
    "                                if diverging_osmid == original_u:\n",
    "                                    connection_edge_u = diverging_osmid\n",
    "                                    connection_edge_v = produced_osmid\n",
    "                                    if debugging_logs:\n",
    "                                        print(f\"The new shortening will use osmids (u,v) = {[connection_edge_u,connection_edge_v]}.\")\n",
    "                                elif diverging_osmid == original_v:\n",
    "                                    connection_edge_u = produced_osmid\n",
    "                                    connection_edge_v = diverging_osmid\n",
    "                                    if debugging_logs:\n",
    "                                        print(f\"The new shortening will use osmids (u,v) = {[connection_edge_u,connection_edge_v]}.\")\n",
    "                                else:\n",
    "                                    print(f\"ERROR while trying to set 'u' and 'v' for shortened connection edge {diverging_edge_id}.\")\n",
    "                                    print(f\"The new shortening was trying to use osmids (u,v) = {[connection_edge_u,connection_edge_v]}.\")\n",
    "                                    intended_crash\n",
    "                                # Use the same key that was used in the original_connection_edge\n",
    "                                connection_edge_key = original_connection_edge.key.unique()[0]\n",
    "                                # Retrieve the modified edge's (connection_edge's) geometry\n",
    "                                connection_edge_geom = connection_edge['geometry'].unique()[0]\n",
    "                                # Find current last position\n",
    "                                iloc_edge = len(complementary_uncovered_edges)\n",
    "                                # Register new edge\n",
    "                                complementary_uncovered_edges.loc[iloc_edge+1,'u'] = connection_edge_u\n",
    "                                complementary_uncovered_edges.loc[iloc_edge+1,'v'] = connection_edge_v\n",
    "                                complementary_uncovered_edges.loc[iloc_edge+1,'key'] = connection_edge_key\n",
    "                                complementary_uncovered_edges.loc[iloc_edge+1,'geometry'] = connection_edge_geom\n",
    "                                complementary_uncovered_edges.loc[iloc_edge+1,'edge_id'] = str(connection_edge_u)+str(connection_edge_v)+str(connection_edge_key)\n",
    "                                complementary_uncovered_edges.loc[iloc_edge+1,'original_edge_id'] = diverging_edge_id\n",
    "                                complementary_uncovered_edges.loc[iloc_edge+1,'clipping_i'] = shorten_i\n",
    "\n",
    "                                # 6.3.3.a3 - Finished registering. Stop while loop. Continue with the next diverging_edge_id of current osmid.\n",
    "                                if function_logs:\n",
    "                                    print(f\"6.0 - Reused contact node {produced_osmid} for original edge {diverging_edge_id}.\")\n",
    "                                \n",
    "                                fabricated_count += 1\n",
    "                                stop = True\n",
    "                                continue\n",
    "\n",
    "                            # Else, this is the first time that the edge is shortened once. \n",
    "                            # Not relevant, continue as usual.\n",
    "                            else:\n",
    "                                pass\n",
    "                                \n",
    "                        # Else, either this is the first edge to be shortened or shorten_i is not equall to 1. \n",
    "                        # Not relevant, continue as usual.\n",
    "                        else:\n",
    "                            pass\n",
    "                        # Double-once-shortening test ENDS ### ### ### ### ### ### ### ### ### ### ### ###\n",
    "                        \"\"\"\n",
    "\n",
    "                        # Overlapping edges test STARTS ### ### ### ### ### ### ### ### ### ### ### ###\n",
    "                        try:\n",
    "                            # Overlapping checker\n",
    "                            overlapping_exists = False\n",
    "                            \n",
    "                            # Retrieve the modified edge's (connection_edge's) geometry, create a VERY SMALL buffer and get its length\n",
    "                            connection_edge_geom = connection_edge['geometry'].unique()[0]\n",
    "                            connection_edge_buff = connection_edge_geom.buffer(0.10)\n",
    "                            connection_edge_length = connection_edge_geom.length\n",
    "                            \n",
    "                            # Find all edges that come out of the current diverging_osmid\n",
    "                            idx = (complementary_uncovered_edges[\"u\"]==diverging_osmid)|(complementary_uncovered_edges[\"v\"]==diverging_osmid)\n",
    "                            potential_overlapping = complementary_uncovered_edges.loc[idx].copy()\n",
    "                            # Remove itself (original_edge_id)\n",
    "                            drop_idx = potential_overlapping.edge_id==diverging_edge_id\n",
    "                            potential_overlapping = potential_overlapping.loc[~drop_idx].copy()\n",
    "                            \n",
    "                            if debugging_logs:\n",
    "                                print(f\"Reviewing overlapping with complementary edges found around small buffer: {potential_overlapping.edge_id.unique()}.\")\n",
    "                            \n",
    "                            # Get a list of all geometries where overlapping is going to be analysed\n",
    "                            potential_geoms = list(potential_overlapping.geometry.unique())\n",
    "                            # Analyse each edge's overlapping percentage\n",
    "                            for potential_geom in potential_geoms:                                    \n",
    "                                # Create a VERY SMALL buffer around already shortened edge and get its length\n",
    "                                potential_geom_buff = potential_geom.buffer(0.10)\n",
    "                                potential_geom_length = potential_geom.length\n",
    "                                # Calculate percentage of overlapping in shortest line\n",
    "                                if connection_edge_length < potential_geom_length:\n",
    "                                    overlapping_area_pct = (connection_edge_buff.intersection(potential_geom_buff).area) / (connection_edge_buff.area)\n",
    "                                else:\n",
    "                                    overlapping_area_pct = (potential_geom_buff.intersection(connection_edge_buff).area) / (potential_geom_buff.area)\n",
    "                                # If overlapping is above 75%, we are practically duplicating a connection. Drop.\n",
    "                                if overlapping_area_pct > 0.75:\n",
    "                                    if debugging_logs:\n",
    "                                        print(f\"Overlapping. Broke from internal loop. ({overlapping_area_pct*100}%).\")\n",
    "                                    overlapping_exists=True\n",
    "                                    break\n",
    "                            # If the analysis found overlapping, do not proceed.    \n",
    "                            if overlapping_exists:\n",
    "                                if debugging_logs:\n",
    "                                    print(\"Not adding shortened edge, exceeded overlap with existing path.\")\n",
    "                                # Find last position\n",
    "                                iloc_edge = len(dropped_overlapping_edges)+1\n",
    "                                # Add dropped connection to dropped_overlapping_edges gdf (For GIS visualization purposes)\n",
    "                                dropped_overlapping_edges.loc[iloc_edge,'geometry'] = connection_edge_geom\n",
    "                                dropped_overlapping_edges.loc[iloc_edge,'original_edge_id'] = diverging_edge_id\n",
    "                                dropped_overlapping_edges.loc[iloc_edge,'clipping_i'] = shorten_i\n",
    "                                stop = True\n",
    "                                continue\n",
    "                            else:\n",
    "                                if debugging_logs:\n",
    "                                    print(\"No overlapping. Continuing.\")\n",
    "                        except:\n",
    "                            print(f\"Overlapping failed on diverging_edge_id {diverging_edge_id}.\")\n",
    "                            print(f\"connection_edge_length: {connection_edge_length}.\")\n",
    "                            print(f\"connection_edge_buff area: {connection_edge_buff.area}.\")    \n",
    "                        # Overlapping shortened edges test ENDS ### ### ### ### ### ### ### ### ### ### ### ###\n",
    "                                \n",
    "                        # CONTINUATION OF NORMAL PROCESS:\n",
    "                        \n",
    "                        if debugging_logs:\n",
    "                            print(f\"New connection_edge passed all tests. Continuing creation process.\")\n",
    "                        \n",
    "                        # The current mid_point is not in proximity to the base network.\n",
    "                        # But the previous mid_point (The current connection_edge's endpoint) WAS KNOWN TO BE in proximity to the base network.\n",
    "                        # Next step --> Transform the last mid_point (The currend endpoint) into a contact_node and update the edge.\n",
    "\n",
    "                        # 6.3.3.b1 - Produce a unique osmid (That doesn't exist in either network) in order to add the edge and point\n",
    "                        # Produce a unique osmid\n",
    "                        produced_osmid = produce_osmid(base_nodes, complementary_nodes, previously_produced+1)\n",
    "                        # Save produced osmid to avoid trying numbers unnecessarily\n",
    "                        previously_produced = produced_osmid\n",
    "                        if debugging_logs:\n",
    "                            print(f\"Produced new osmid: {produced_osmid} for current diverging edge.\")\n",
    "                \n",
    "                        # 6.3.3.b2 - Find the connection_edge's ending_point. It will become a new node.\n",
    "                        # --------- It is unsure whether the diverging_node is located at the coordinate 0 or at the last coordinate of the connection_edge.\n",
    "                        # --------- So it is necessary to identify which one is which.\n",
    "                        # Extract the edge's coordinates list\n",
    "                        connection_edge_coords = list(connection_edge['geometry'][0].coords)\n",
    "                        # Extract the starting_point's coordinates (Known to be the diverging_node)\n",
    "                        starting_point_coords = diverging_node.loc[0,'geometry'].coords[0]\n",
    "                        # Obtain the ending_point's coordinates (It is the previous iteration edge's mid_point)\n",
    "                        # (It is either the first or last coordinate of the connection_edge)\n",
    "                        if starting_point_coords == connection_edge_coords[0]:\n",
    "                            ending_point_coords = connection_edge_coords[-1]\n",
    "                        elif starting_point_coords == connection_edge_coords[-1]:\n",
    "                            ending_point_coords = connection_edge_coords[0]\n",
    "                        else:\n",
    "                            print(f\"ERROR while trying to find the starting and ending point of shortened connection edge {diverging_edge_id}.\")\n",
    "                            print(f\"Diverging node osmid: {diverging_osmid}.\")\n",
    "                            print(f\"Diverging node (starting point) coords: {starting_point_coords}.\")\n",
    "                            print(f\"Connection edge's coords [0]: {connection_edge_coords[0]}.\")\n",
    "                            print(f\"Connection edge's coords [-1]: {connection_edge_coords[-1]}.\")\n",
    "                            intended_crash\n",
    "                        # -----\n",
    "                        if debugging_logs:\n",
    "                            print(f\"Found the line's ending points (opposite to the starting_point_coords).\")\n",
    "                        # -----\n",
    "                \n",
    "                        # 6.3.3.b3 - Add the ending_point as a new node to complementary_uncovered_nodes gdf\n",
    "                        # -----\n",
    "                        if debugging_logs:\n",
    "                            print(f\"Printing complementary_uncovered_nodes dtypes before adding new node.\")\n",
    "                            print(complementary_uncovered_nodes.dtypes)\n",
    "                        # ---\n",
    "                        # Create temporary gdf with new node's data\n",
    "                        new_point = Point(ending_point_coords)\n",
    "                        df_temporal = pd.DataFrame({'osmid': [int(produced_osmid)],\n",
    "                                                    'geometry': [new_point],\n",
    "                                                    'clipping_i': [int(shorten_i)]\n",
    "                                                   })\n",
    "                        gdf_temporal = gpd.GeoDataFrame(df_temporal, geometry='geometry', crs=projected_crs)\n",
    "                        # Force all datatypes to match the complementary_uncovered_nodes dtypes before merging\n",
    "                        dtypes_dict = complementary_uncovered_nodes.dtypes.to_dict() #Dict with complementary_uncovered_nodes dtypes\n",
    "                        filtered_dtypes = {col: dtypes_dict[col] for col in gdf_temporal.columns if col in dtypes_dict} # Filters for cols in gdf_temporal\n",
    "                        gdf_temporal = gdf_temporal.astype(filtered_dtypes) # Assigns those types to df_temporal\n",
    "                        # Concatenate to complementary_uncovered_nodes without altering original dtypes\n",
    "                        complementary_uncovered_nodes = pd.concat([complementary_uncovered_nodes, gdf_temporal], ignore_index=True)\n",
    "                        # Concatenate to contact_nodes\n",
    "                        contact_nodes = pd.concat([contact_nodes, gdf_temporal], ignore_index=True)\n",
    "                        # -----\n",
    "                        if debugging_logs:\n",
    "                            print(f\"Printing complementary_uncovered_nodes dtypes after adding new node.\")\n",
    "                            print(complementary_uncovered_nodes.dtypes)\n",
    "                        # ---\n",
    "                        \n",
    "                        # 6.3.3.b4 - Keep original 'u' / original 'v' in its position by checking the original_connection_edge. \n",
    "                        # Retrieve the diverging_osmid its position in the ORIGINAL connection_edge. (Whether 'u' or 'v')\n",
    "                        # In order to keep that original diverging_node in its position and add the produced_osmid in the other position.\n",
    "                        original_u = original_connection_edge.u.unique()[0]\n",
    "                        original_v = original_connection_edge.v.unique()[0]\n",
    "                        if diverging_osmid == original_u:\n",
    "                            connection_edge_u = diverging_osmid\n",
    "                            connection_edge_v = produced_osmid\n",
    "                        elif diverging_osmid == original_v:\n",
    "                            connection_edge_u = produced_osmid\n",
    "                            connection_edge_v = diverging_osmid\n",
    "                        else:\n",
    "                            print(f\"ERROR while trying to set 'u' and 'v' for shortened connection edge {diverging_edge_id}.\")\n",
    "                            intended_crash\n",
    "                        # Use the same key that was used in the original connection_edge\n",
    "                        connection_edge_key = original_connection_edge.key.unique()[0]\n",
    "                        # Retrieve the modified edge's (connection_edge's) geometry\n",
    "                        connection_edge_geom = connection_edge['geometry'].unique()[0]\n",
    "                        \n",
    "                        if debugging_logs:\n",
    "                            print(f\"Defined whether to assign new produced osmid to 'u' or 'v' based on the original connection edge.\")\n",
    "                            print(f\"Assigning connection_edge_u: {connection_edge_u}.\")\n",
    "                            print(f\"Assigning connection_edge_v: {connection_edge_v}.\")\n",
    "                            print(f\"Assigning connection_edge_key: {connection_edge_key}.\")\n",
    "                                                    \n",
    "                        # 6.3.3.b5 - Add the connection_edge as a new edge to complementary_uncovered_edges gdf\n",
    "                        # -----\n",
    "                        if debugging_logs:\n",
    "                            print(f\"Printing complementary_uncovered_edges dtypes before adding new edge.\")\n",
    "                            print(complementary_uncovered_edges.dtypes)\n",
    "                        # -----\n",
    "                        # Create temporary gdf with new edges's data\n",
    "                        df_temporal = pd.DataFrame({'u': [int(connection_edge_u)],\n",
    "                                                    'v': [int(connection_edge_v)],\n",
    "                                                    'key': [int(connection_edge_key)],\n",
    "                                                    'geometry': [connection_edge_geom],\n",
    "                                                    'edge_id': [str(connection_edge_u) + str(connection_edge_v) + str(connection_edge_key)],\n",
    "                                                    'original_edge_id': [str(diverging_edge_id)],\n",
    "                                                    'clipping_i': [int(shorten_i)]\n",
    "                                                   })\n",
    "                        gdf_temporal = gpd.GeoDataFrame(df_temporal, geometry='geometry', crs=projected_crs)\n",
    "                        # Force all datatypes to match the complementary_uncovered_edges dtypes before merging\n",
    "                        dtypes_dict = complementary_uncovered_edges.dtypes.to_dict() #Dict with complementary_uncovered_edges dtypes\n",
    "                        filtered_dtypes = {col: dtypes_dict[col] for col in gdf_temporal.columns if col in dtypes_dict} # Filters for cols in gdf_temporal\n",
    "                        gdf_temporal = gdf_temporal.astype(filtered_dtypes) # Assigns those types to gdf_temporal\n",
    "                        # Concatenate to complementary_uncovered_nodes without altering original dtypes\n",
    "                        complementary_uncovered_edges = pd.concat([complementary_uncovered_edges, gdf_temporal], ignore_index=True)\n",
    "                        # -----\n",
    "                        if debugging_logs:\n",
    "                            print(f\"Printing complementary_uncovered_edges dtypes after adding new edge.\")\n",
    "                            print(complementary_uncovered_edges.dtypes)\n",
    "                        # -----\n",
    "                        \n",
    "                        # Make sure the IDs remain integers throughout the process\n",
    "                        #complementary_uncovered_nodes['osmid'] = complementary_uncovered_nodes.osmid.astype(int)\n",
    "                        #contact_nodes['osmid'] = contact_nodes.osmid.astype(int)\n",
    "                        #complementary_uncovered_edges['u'] = complementary_uncovered_edges.u.astype(int)\n",
    "                        #complementary_uncovered_edges['v'] = complementary_uncovered_edges.v.astype(int)\n",
    "                        #complementary_uncovered_edges['key'] = complementary_uncovered_edges.v.astype(int)\n",
    "                        # -----\n",
    "                        #if debugging_logs:\n",
    "                        #    print(f\"Printing fixed complementary_uncovered_nodes dtypes.\")\n",
    "                        #    print(complementary_uncovered_nodes.dtypes)\n",
    "                        #    print(f\"Printing fixed complementary_uncovered_edges dtypes.\")\n",
    "                        #    print(complementary_uncovered_edges.dtypes)\n",
    "                        # -----\n",
    "\n",
    "                        a=\"\"\"\n",
    "                        # 6.3.3.b5 - Register current process\n",
    "                        # Registration - shortening_dict\n",
    "                        if str(shorten_i) in shortening_dict.keys():\n",
    "                            # If this is not the first time a shortening process with <shorten_i> iterations is registered, append.\n",
    "                            current_lst = shortening_dict[str(shorten_i)]\n",
    "                            shortening_dict[str(shorten_i)] = current_lst.append(diverging_edge_id)\n",
    "                        else:\n",
    "                            # If this is the first time a shortening process with <shorten_i> iterations is registered, create list\n",
    "                            shortening_dict[str(shorten_i)] = [diverging_edge_id]\n",
    "                            \n",
    "                        # Registration - diverging_osmid_dict\n",
    "                        if diverging_osmid in diverging_osmid_dict.keys():\n",
    "                            # If this is not the first time an edge from this osmid is shortened, append\n",
    "                            current_lst = diverging_osmid_dict[diverging_osmid]\n",
    "                            current_lst.append(diverging_edge_id)\n",
    "                            diverging_osmid_dict[diverging_osmid] = current_lst\n",
    "                        else:\n",
    "                            # If this is the first time an edge from this osmid is shortened, create list\n",
    "                            diverging_osmid_dict[diverging_osmid] = [diverging_edge_id]\"\"\"\n",
    "\n",
    "                        # Registration - Process count\n",
    "                        fabricated_count += 1\n",
    "                    \n",
    "                        # 6.3.3.b6 - Finished registering. Stop while loop. Continue with the next diverging_edge_id of current osmid.\n",
    "                        stop = True\n",
    "        \n",
    "        # LOG CODE - Progress logs\n",
    "        # Finished reviewing current osmid. Continue with next osmid in non_contact_osmids.\n",
    "        osmid_count+=1\n",
    "        # LOG CODE - Progress logs\n",
    "\n",
    "    if function_logs:\n",
    "        print(f\"Finished. Fabricated {fabricated_count}.\")\n",
    "\n",
    "    a=\"\"\"\n",
    "    if debugging_logs:\n",
    "        print(f\"Printing diverging_osmid_dict.\")\n",
    "        print(diverging_osmid_dict)\n",
    "        \"\"\"\n",
    "    # Final format\n",
    "    if len(dropped_overlapping_edges)>0:\n",
    "        dropped_overlapping_edges = dropped_overlapping_edges.set_crs(projected_crs)\n",
    "    \n",
    "    return complementary_uncovered_nodes, complementary_uncovered_edges, contact_nodes, dropped_overlapping_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69910181-3c7b-478e-8a42-7bdf5ff68daf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### __Functions -__ edge_clipping [Used in Part01_Step02, Part02_Step01, Part02_Step03, Part02_Step04]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40471baa-499f-46c1-95b8-14d99283f71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_clipping(starting_point_gdf, edge_gdf, clipping_point_gdf, projected_crs=\"EPSG:6372\", return_all=False, function_logs=False):\n",
    "    \n",
    "    \"\"\" This function clips an edge by considering a starting point and a clipping point.\n",
    "    \n",
    "\tArgs:\n",
    "\t\tstarting_point_gdf (geopandas.GeoDataFrame): GeoDataFrame containing the starting point of the LineString, helps identify each split part. \n",
    "                                                     Requires column 'geometry' with a Point.\n",
    "        edge_gdf (geopandas.GeoDataFrame): GeoDataFrame containing the edge to be clipped. \n",
    "                                           Requires column 'geometry' with a LineString.\n",
    "        clipping_point_gdf (geopandas.GeoDataFrame): GeoDataFrame containing the clipping point of the LineString.\n",
    "                                                     Requires column 'geometry' with a Point.\n",
    "        projected_crs (str, optional): string containing projected crs to be used depending on area of interest. Defaults to \"EPSG:6372\".\n",
    "        return_all (bool,optional): Boolean that defines whether the clipped edge is returned from starting_point to clipping_point only (One line) (False)\n",
    "                                    or returned from starting_point to clipping_point and then from clipping_point to ending_point (Two lines) (True). \n",
    "                                    Defaults to False.\n",
    "        function_logs (bool,optional): Boolean that (if True) prints logs during the functions execution. Defaults to False.\n",
    "                                                 \n",
    "\tReturns:\n",
    "        split_edge_gdf (geopandas.GeoDataFrame): GeoDataFrame with either one (return_all=False) or two (return_all=True) edges.\n",
    "\t\"\"\"\n",
    "    \n",
    "    # 1.0 --------------- Split the edge using the clipping_point. Creates two separate edges.\n",
    "    # Extract the edge's geometry (LineString)\n",
    "    edge_geom = edge_gdf['geometry'].unique()[0]\n",
    "    # Extract the clipping_point's geometry (Point)\n",
    "    clipping_point_geom = clipping_point_gdf['geometry'][0]\n",
    "    # Project the clipping_point onto the edge (In case it's not drawn exactly in the LineString)\n",
    "    projected_point = edge_geom.interpolate(edge_geom.project(clipping_point_geom))\n",
    "    \n",
    "    # ----- ----- Added code since having trouble clipping with a point ----- -----\n",
    "    # Create a VERY SMALL buffer around the projected_point\n",
    "    projected_point = projected_point.buffer(1e-9)\n",
    "    # ----- ----- Added code since having trouble clipping with a point ----- -----\n",
    "    \n",
    "    # Split the edge where the projected clipping_point is\n",
    "    split_lines = split(edge_geom, projected_point)\n",
    "    # Convert the split lines to a GeoDataFrame\n",
    "    split_gdf = gpd.GeoDataFrame(geometry=list(split_lines.geoms), crs=projected_crs)\n",
    "    \n",
    "    # ----- ----- Added code since having trouble clipping with a point ----- -----\n",
    "    # Drop the small split line located inside the VERY SMALL buffer in order to keep the two split lines outside it.\n",
    "    if len(split_gdf) ==3 :\n",
    "        split_gdf['length'] = split_gdf.length\n",
    "        split_gdf = split_gdf.loc[split_gdf.length != split_gdf.length.min()]\n",
    "    split_gdf.reset_index(inplace=True,drop=True)\n",
    "    # ----- ----- Added code since having trouble clipping with a point ----- -----\n",
    "\n",
    "    \n",
    "    # 2.0 --------------- Find which split LineString in split_gdf contains the LineString where the starting_point (input arg) is located.\n",
    "    # ------------------- This linestring is called \"split_edge_geom_start\", the other one is \"split_edge_geom_end\".\n",
    "\n",
    "    # Extract the starting_point's coordinates (Function input)\n",
    "    starting_point_coords = starting_point_gdf.loc[0,'geometry'].coords[0]\n",
    "    if function_logs:\n",
    "        print(f\"edge_clipping(): Extracted starting_point_coords: {starting_point_coords}.\")\n",
    "        \n",
    "    # Find which line from split_gdf has the starting_point_coords by...\n",
    "    # ... checking if the starting_point_coords are in line 0\n",
    "    if starting_point_coords in list(split_gdf.loc[0,'geometry'].coords):\n",
    "        split_edge_geom_start = split_gdf.loc[0,'geometry'] # Line geometry related to starting_point\n",
    "        split_edge_geom_end = split_gdf.loc[1,'geometry'] # Second line geometry\n",
    "        if function_logs:\n",
    "            print(f\"edge_clipping(): Found line containing coords equal to starting_point_coords {starting_point_coords}.\")\n",
    "            print(f\"edge_clipping(): Line found: {list(split_gdf.loc[0,'geometry'].coords)}.\")\n",
    "            \n",
    "    # ... or, checking if the starting_point_coords are in line 1   \n",
    "    elif starting_point_coords in list(split_gdf.loc[1,'geometry'].coords):\n",
    "        split_edge_geom_start = split_gdf.loc[1,'geometry'] # Line geometry related to starting_point\n",
    "        split_edge_geom_end = split_gdf.loc[0,'geometry'] # Second line geometry\n",
    "        if function_logs:\n",
    "            print(f\"edge_clipping(): Found line containing coords equal to starting_point_coords {starting_point_coords}.\")\n",
    "            print(f\"edge_clipping(): Line found: {list(split_gdf.loc[0,'geometry'].coords)}.\")\n",
    "            \n",
    "    # ... or else, measure distances and assume the starting_point is the closest one.\n",
    "    else:\n",
    "        # Line 0 data\n",
    "        line_0_coords = list(split_gdf.loc[0,'geometry'].coords)\n",
    "        line_0_firstcoords_dist = distance_between_points(line_0_coords[0], starting_point_coords)\n",
    "        line_0_lastcoords_dist = distance_between_points(line_0_coords[-1], starting_point_coords)\n",
    "        # Line 1 data\n",
    "        line_1_coords = list(split_gdf.loc[1,'geometry'].coords)\n",
    "        line_1_firstcoords_dist = distance_between_points(line_1_coords[0], starting_point_coords)\n",
    "        line_1_lastcoords_dist = distance_between_points(line_1_coords[-1], starting_point_coords)\n",
    "        # Compare distances and find min distance\n",
    "        distances_lst = [line_0_firstcoords_dist,line_0_lastcoords_dist,line_1_firstcoords_dist,line_1_lastcoords_dist]\n",
    "        minimum_distance = min(distances_lst)\n",
    "        minimum_distance_position = distances_lst.index(minimum_distance)\n",
    "        # Find line related to the starting_point\n",
    "        # First two distances in list correspond to line 0. If true, Line 0 is related to the starting_point.\n",
    "        if (minimum_distance_position == 0) | (minimum_distance_position == 1):\n",
    "            split_edge_geom_start = split_gdf.loc[0,'geometry'] # Line geometry related to starting_point\n",
    "            split_edge_geom_end = split_gdf.loc[1,'geometry'] # Second line geometry\n",
    "        # Second two distances in list correspond to line 1. If true, Line 1 is related to the starting_point.\n",
    "        elif (minimum_distance_position == 2) | (minimum_distance_position == 3):\n",
    "            split_edge_geom_start = split_gdf.loc[1,'geometry'] # Line geometry related to starting_point\n",
    "            split_edge_geom_end = split_gdf.loc[0,'geometry'] # Second line geometry\n",
    "        if function_logs:\n",
    "            print(f\"edge_clipping(): Found line containing coords close to starting_point_coords {starting_point_coords}. Distance: {minimum_distance}.\")\n",
    "            print(f\"edge_clipping(): Line found: {list(split_gdf.loc[0,'geometry'].coords)}.\")\n",
    "        \n",
    "    a=\"\"\" [SUBSTITUTED BY CHANGES ABOVE]\n",
    "    # 2.0 --------------- Find which split LineString in split_gdf contains the LineString where the starting_point is located\n",
    "    \n",
    "    # Extract the edge's first and last coordinates\n",
    "    edge_coords = list(edge_gdf['geometry'][0].coords)\n",
    "    first_point_coords = edge_coords[0]\n",
    "    last_point_coords = edge_coords[-1]\n",
    "\n",
    "    if function_logs:\n",
    "        print(f\"edge_clipping(): Extracted first_point_coords: {first_point_coords}.\")\n",
    "        print(f\"edge_clipping(): Extracted last_point_coords: {last_point_coords}.\")\n",
    "    \n",
    "    # Extract the starting_point's coordinates (Function input)\n",
    "    starting_point_coords = starting_point_gdf.loc[0,'geometry'].coords[0]\n",
    "    if function_logs:\n",
    "        print(f\"edge_clipping(): Extracted starting_point_coords: {starting_point_coords}.\")\n",
    "        \n",
    "    # Try identifying which (first or last) is the starting_point by checking equality in coordinates\n",
    "    if starting_point_coords == first_point_coords:\n",
    "        # Starting point is first_point\n",
    "        starting_point_coords = first_point_coords\n",
    "    elif starting_point_coords == last_point_coords:\n",
    "        # Starting point is last_point\n",
    "        starting_point_coords = last_point_coords\n",
    "    # Else, measure distance and assume the starting_point is the closest one.\n",
    "    # (this case applies when starting_point is not drawn exactly in the LineString)\n",
    "    else:\n",
    "        # Distance from starting_point to first point\n",
    "        first_point_distance = distance_between_points(first_point_coords, starting_point_coords)\n",
    "        # Distance from starting_point to last point\n",
    "        last_point_distance = distance_between_points(last_point_coords, starting_point_coords)\n",
    "        # Find which one is the starting_point\n",
    "        if first_point_distance < last_point_distance:\n",
    "            # Starting point is first_point\n",
    "            starting_point_coords = first_point_coords\n",
    "        else:\n",
    "            # Starting point is last_point\n",
    "            starting_point_coords = last_point_coords\n",
    "    \n",
    "    # 3.0 --------------- Select split LineString where the starting_point is.\n",
    "    \n",
    "    # Find which line from split_gdf has the starting_point_coords\n",
    "    # Extract both lines (starting and ending line)\n",
    "    if starting_point_coords in list(split_gdf.loc[0,'geometry'].coords):\n",
    "        split_edge_geom_start = split_gdf.loc[0,'geometry'] # Extract first line geometry\n",
    "        split_edge_geom_end = split_gdf.loc[1,'geometry'] # Second line geometry\n",
    "        \n",
    "    elif starting_point_coords in list(split_gdf.loc[1,'geometry'].coords):\n",
    "        split_edge_geom_start = split_gdf.loc[1,'geometry'] # Extract second line geometry\n",
    "        split_edge_geom_end = split_gdf.loc[0,'geometry'] # Second line geometry\n",
    "\n",
    "    else:\n",
    "        print(f\"First line's coords:{list(split_gdf.loc[0,'geometry'].coords)}.\")\n",
    "        print(f\"Second line: {list(split_gdf.loc[1,'geometry'].coords)}.\")\n",
    "        print(f\"Starting point coords: {starting_point_coords}.\")\n",
    "    \"\"\"\n",
    "        \n",
    "    # Convert to a GeoDataFrame\n",
    "    split_edge_gdf = gpd.GeoDataFrame()\n",
    "    split_edge_gdf.loc[0,'geometry'] = split_edge_geom_start\n",
    "    \n",
    "    # 4.0 --------------- (Optional) Include split other LineString.\n",
    "    if return_all:\n",
    "        split_edge_gdf.loc[1,'geometry'] = split_edge_geom_end\n",
    "        split_edge_gdf.loc[0,'relation'] = 'starting'\n",
    "        split_edge_gdf.loc[1,'relation'] = 'ending'\n",
    "\n",
    "    # Final format\n",
    "    split_edge_gdf = split_edge_gdf.set_crs(projected_crs)\n",
    "\n",
    "    return split_edge_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1ebfed-f141-4d70-bc7d-50e94c476e48",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### __Functions -__ find_intersection_nodes [Used in Part02_Step01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cee364-43bf-4da1-af26-0b5194b5c4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_intersection_nodes(ntw_01_nodes, ntw_01_edges, ntw_02_nodes, ntw_02_edges, projected_crs):\n",
    "\n",
    "    \"\"\" This function finds the intersection points existing between two different networks, and turns them\n",
    "        into nodes by assigning them a unique osmid.\n",
    "    \n",
    "\tArgs:\n",
    "\t\tntw_01_nodes (geopandas.GeoDataFrame): GeoDataFrame containing the nodes from network 01.\n",
    "                                                Requires a unique identifier 'osmid'.\n",
    "        ntw_01_edges (geopandas.GeoDataFrame): GeoDataFrame containing the edges from network 01. \n",
    "                                                   Requires the unique identifiers 'u ,'v' and 'key'.\n",
    "        ntw_02_nodes (geopandas.GeoDataFrame): GeoDataFrame containing the nodes from network 02.\n",
    "                                                Requires a unique identifier 'osmid'.\n",
    "        ntw_02_edges (geopandas.GeoDataFrame): GeoDataFrame containing the edges from network 02. \n",
    "                                                   Requires the unique identifiers 'u ,'v' and 'key'.\n",
    "        projected_crs (str, optional): String containing projected crs to be used depending on area of interest. \n",
    "                                        Defaults to \"EPSG:6372\".\n",
    "                                                 \n",
    "\tReturns:\n",
    "        intersection_nodes (geopandas.GeoDataFrame): GeoDataFrame containing the new nodes where both networks intersect.\n",
    "        \n",
    "\t\"\"\"\n",
    "    \n",
    "    # 1.0 --------------- Intersect both network's edges to create the intersection_points.\n",
    "    # ------------------- These nodes contain data from the edges that were intersected.\n",
    "\n",
    "    # ------------------- INPUT USED - READ AND FILTER EDGES\n",
    "    ntw_01_edges = ntw_01_edges.copy()\n",
    "    ntw_01_edges = ntw_01_edges.to_crs(projected_crs)\n",
    "    ntw_01_edges = ntw_01_edges[['u','v','key','geometry']]\n",
    "\n",
    "    ntw_02_edges = ntw_02_edges.copy()\n",
    "    ntw_02_edges = ntw_02_edges.to_crs(projected_crs)\n",
    "    ntw_02_edges = ntw_02_edges[['u','v','key','geometry']]\n",
    "    # ------------------- INPUT USED - READ AND FILTER EDGES\n",
    "    \n",
    "    # Intersect ntw_01_edges and ntw_02_edges to create intersection_points\n",
    "    # Each intersection_point contains data from the edges being intersected.\n",
    "    # For ntw_01_edges, columns: u_1, v_1, key_1\n",
    "    # For ntw_02_edges, columns: u_2, v_2, key_2\n",
    "    intersection_points = ntw_01_edges.overlay(ntw_02_edges,how='intersection',keep_geom_type=False)\n",
    "    # Explode multipoints\n",
    "    # If an edge from network \"x\" cuts an edge from network \"y\" twice or more [e.g. a straight edge cutting a curved edge], \n",
    "    # the resulting geometry is a MultiPoint (One point in the first intersection, another for the second and so on).\n",
    "    # In order to avoid assigning the same data to both points, exploding is neccesary.\n",
    "    intersection_points = intersection_points.explode(index_parts=False)\n",
    "    intersection_points.reset_index(inplace=True,drop=True)\n",
    "\n",
    "    print(\"Intersected networks and created intersection_points.\")\n",
    "\n",
    "    # 2.0 --------------- Assign unique osmid to each new intersection_point to turn it into a intersection_node.\n",
    "\n",
    "    # ------------------- INPUT USED - READ AND FILTER EDGES\n",
    "    ntw_01_nodes = ntw_01_nodes.copy()\n",
    "    ntw_01_nodes = ntw_01_nodes.to_crs(projected_crs)\n",
    "    ntw_01_nodes = ntw_01_nodes[['osmid','geometry']]\n",
    "\n",
    "    ntw_02_nodes = ntw_02_nodes.copy()\n",
    "    ntw_02_nodes = ntw_02_nodes.to_crs(projected_crs)\n",
    "    ntw_02_nodes = ntw_02_nodes[['osmid','geometry']]\n",
    "    # ------------------- INPUT USED - READ AND FILTER EDGES\n",
    "\n",
    "    intersection_nodes = intersection_points.copy()\n",
    "    del intersection_points\n",
    "    # Set all osmids to 0\n",
    "    intersection_nodes['osmid'] = 0\n",
    "    # Restart previously_produced\n",
    "    previously_produced = 0\n",
    "    # For each intersection between networks (new node)\n",
    "    for index,row in intersection_nodes.iterrows():\n",
    "        # Produce a unique osmsid (That doesn't exist in either network)\n",
    "        produced_osmid = produce_osmid(ntw_01_nodes, ntw_02_nodes, previously_produced)\n",
    "        # Assign osmid to current intersection_node\n",
    "        intersection_nodes.loc[index,'osmid'] = produced_osmid\n",
    "        # Since ntw_01_nodes and ntw_02_nodes are not being updated, next time try with next possible osmid\n",
    "        previously_produced = produced_osmid+1\n",
    "\n",
    "    print(\"Set unique osmids to all intersection_points. Turned into intersection_nodes.\")\n",
    "\n",
    "    # Set unique identifiers to int\n",
    "    intersection_nodes['u_1'] = intersection_nodes['u_1'].astype('int')\n",
    "    intersection_nodes['v_1'] = intersection_nodes['v_1'].astype('int')\n",
    "    intersection_nodes['key_1'] = intersection_nodes['key_1'].astype('int')\n",
    "    intersection_nodes['u_2'] = intersection_nodes['u_2'].astype('int')\n",
    "    intersection_nodes['v_2'] = intersection_nodes['v_2'].astype('int')\n",
    "    intersection_nodes['key_2'] = intersection_nodes['key_2'].astype('int')\n",
    "    intersection_nodes['osmid'] = intersection_nodes['osmid'].astype('int')\n",
    "\n",
    "    # 3.0 --------------- Unify geometry duplicates under the same osmid (MULTIPLE INTERSECTION ADAPTATION)\n",
    "    # EXPLANATION\n",
    "    # Function find_intersection_nodes() was originaly designed to create a dataframe that stablishes intersection points between\n",
    "    # 1 (one) edge of a network and 1 (one) edge of another network. The dataframe stores the IDs of each intersected edge and the intersecting node.\n",
    "    # However, cases were found where three or more edges intersect exactly at the same point (particularly due to the use of tessellations-generated networks).\n",
    "    # The result is having two (or more) intersection_nodes located exactly at the same point.\n",
    "    # e.g. one intersecting edge_1 from network \"x\" and edge_1 from network \"y\", \n",
    "    # and the other intersecting edge_2 from network \"x\" and edge_1 from network \"y\".\n",
    "\n",
    "    # Following that example, in order to avoid duplicated nodes with different osmids,\n",
    "    # this MULTIPLE INTERSECTION ADAPTATION unifies geometry duplicates under the same osmid.\n",
    "    # (It shouldn't be done by just dropping duplicates since the dataframe has the intersected edges's data.)\n",
    "    dup_geometries_lst = list(intersection_nodes.loc[intersection_nodes.duplicated(subset=['geometry'])].geometry.unique())\n",
    "    for dup_geom in dup_geometries_lst:\n",
    "        # Rows to change\n",
    "        idx = intersection_nodes.geometry==dup_geom\n",
    "        # Osmid to use in all intersection cases (Doesn't matter which)\n",
    "        chosen_osmid = list(intersection_nodes.loc[idx].osmid.unique())[0]\n",
    "        # Unify those rows's osmid\n",
    "        intersection_nodes.loc[idx,'osmid'] = chosen_osmid\n",
    "\n",
    "    return intersection_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689f097c-2f62-4bc7-b48e-515a9748f573",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### __Functions -__ networks_intersections_update [Used in Part02_Step01, Part02_Step03, Part02_Step04]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba79822-f61b-49ec-8b9c-bc55389da30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_intersections_update(current_ntw_nodes, current_ntw_edges, intersection_nodes, projected_crs, consider_ntw_origin=False,\n",
    "                                 intersection_logs=False, clipping_logs=False):\n",
    "\n",
    "    \"\"\" This function takes points with osmid located over existing edges (intersection_nodes) and updates\n",
    "        a network. The intersection_nodes become new nodes and each intersected edge get split \n",
    "        into two separate edges with new 'u', 'v' and 'key' data.\n",
    "    \n",
    "\tArgs:\n",
    "\t\tcurrent_ntw_nodes (geopandas.GeoDataFrame): GeoDataFrame containing the nodes from the network to update.\n",
    "                                                    Requires a unique identifier 'osmid'.\n",
    "        current_ntw_edges (geopandas.GeoDataFrame): GeoDataFrame containing the edges from the network to update.\n",
    "                                                    Requires the unique identifiers 'u ,'v' and 'key'.\n",
    "        intersection_nodes (geopandas.GeoDataFrame): GeoDataFrame containing the points in where each split is performed.\n",
    "                                                    Requires points with 'osmid', and the edge to split ('u','v' and 'key').\n",
    "        projected_crs (str, optional): String containing projected crs to be used depending on area of interest. \n",
    "                                        Defaults to \"EPSG:6372\".\n",
    "        consider_ntw_origin (bool,optional): Boolean that (if True) reads and keeps the network origin of both nodes and edges. \n",
    "                                         Requires column 'ntw_origin' in both nodes and edges gdf.Defaults to False.\n",
    "        intersection_logs (bool,optional): Boolean that (if True) prints logs during the current function's execution. Defaults to False.\n",
    "        clipping_logs (bool,optional): Boolean that (if True) prints logs during the edge_clipping() function's execution. Defaults to False.\n",
    "\n",
    "                                                 \n",
    "\tReturns:\n",
    "        updated_ntw_nodes (geopandas.GeoDataFrame): GeoDataFrame containing the updated nodes for the network.\n",
    "        updated_ntw_edges (geopandas.GeoDataFrame): GeoDataFrame containing the updated edges for the network. \n",
    "        \n",
    "\t\"\"\" \n",
    "    \n",
    "    print(f\"network_intersections_update(): Updating network...\")\n",
    "    \n",
    "    # ------------------- INPUT USED - READ NODES AND EDGES\n",
    "    current_ntw_nodes = current_ntw_nodes.copy()\n",
    "    current_ntw_nodes = current_ntw_nodes.to_crs(projected_crs)\n",
    "    # Set an identifier to make it easier to locate nodes that resulted from an intersection between networks\n",
    "    current_ntw_nodes['intersecting'] = 0\n",
    "    # Make sure unique identifiers ('u','v', and 'key') are integers to avoid problems when creating edge_ids.\n",
    "    # [ROUNDING CAUSES ISSUES WITH BIG OSMIDS, CHANGE TO STRING IDs WHEN TIME AVAILABLE]\n",
    "    #current_ntw_nodes['osmid'] = current_ntw_nodes['osmid'].apply(lambda x: int(round(float(x),0)))\n",
    "\n",
    "    current_ntw_edges = current_ntw_edges.copy()\n",
    "    current_ntw_edges = current_ntw_edges.to_crs(projected_crs)\n",
    "    # Set an identifier to make it easier to locate edges that were split\n",
    "    current_ntw_edges['intersecting'] = 0\n",
    "    # Make sure unique identifiers ('u','v', and 'key') are integers to avoid problems when creating edge_ids. \n",
    "    # [ROUNDING CAUSES ISSUES WITH BIG OSMIDS, CHANGE TO STRING IDs WHEN TIME AVAILABLE]\n",
    "    #current_ntw_edges['u'] = current_ntw_edges['u'].apply(lambda x: int(round(float(x),0)))\n",
    "    #current_ntw_edges['v'] = current_ntw_edges['v'].apply(lambda x: int(round(float(x),0)))\n",
    "    #current_ntw_edges['key'] = current_ntw_edges['key'].apply(lambda x: int(round(float(x),0)))\n",
    "\n",
    "    # Register all edge_ids by origin (if requested)\n",
    "    if consider_ntw_origin:\n",
    "        print(f\"network_intersections_update(): Registering all edge_ids by origin...\")\n",
    "        # Create 'edge_id' column if not available\n",
    "        created_edge_id = False\n",
    "        if 'edge_id' not in current_ntw_edges.columns:\n",
    "            created_edge_id = True\n",
    "            current_ntw_edges = src.create_unique_edge_id(current_ntw_edges)\n",
    "        # Register unique edge_ids by origin\n",
    "        edge_origin_dct = {} # Dict. that stores all edge_ids for each network origin available\n",
    "        for origin in current_ntw_edges.ntw_origin.unique():\n",
    "            # Find unique edge_ids by origin\n",
    "            origin_edge_ids = set(current_ntw_edges.loc[current_ntw_edges.ntw_origin==origin]['edge_id'].unique())\n",
    "            # Register\n",
    "            edge_origin_dct[origin]=origin_edge_ids\n",
    "        # Delete 'edge_id' column if created here\n",
    "        if created_edge_id:\n",
    "            current_ntw_edges.drop(columns=['edge_id'],inplace=True)\n",
    "    \n",
    "\n",
    "    # ------------------- INPUT USED - READ NODES AND EDGES\n",
    "    \n",
    "    # MULTIPLE INTERSECTION ADAPTATION: (EXPLANATION behind 'performed_intersections' dictionary)\n",
    "    # Explanation: Function find_intersection_nodes() was originaly designed to create a dataframe that stablishes intersection points between\n",
    "    # 1 (one) edge of a network and 1 (one) edge of another network. \n",
    "    # The dataframe stores the IDs (u,v,key) of each intersected edge and the ID (osmid) of the intersecting node.\n",
    "    \n",
    "    # However, cases were found where three or more edges intersect exactly at the same point (particularly due to the use of tessellations-generated networks).\n",
    "    # The result is having two (or more) intersection_nodes located exactly at the same point. e.g:\n",
    "    # One intersecting edge_1 from network \"x\" crossing with edge_1 from network \"y\", \n",
    "    # and the other intersecting edge_2 from network \"x\" also crossing with edge_1 from network \"y\" (at the same point).\n",
    "\n",
    "    # Following that example, in order to avoid intersecting edge_1 from network \"y\" twice with the same node,\n",
    "    # the code labeled as \"MULTIPLE INTERSECTION ADAPTATION\" saves already performed intersections.\n",
    "    # This way, already split edges are not attempted to be split again.\n",
    "    performed_intersections = {}\n",
    "\n",
    "    # LOG CODE - Progress logs\n",
    "    # Will create progress logs when progress reaches these percentages:\n",
    "    progress_logs = [0,10,20,30,40,50,60,70,80,90,100] # for log statistics\n",
    "    progress_count = 0\n",
    "    # LOG CODE - Progress logs\n",
    "\n",
    "    # The whole process iterates over each intersection between both networks (input arg 'intersection_nodes' Dataframe):\n",
    "    print(f\"network_intersections_update(): Starting iterating over each intersection_node.\")\n",
    "    for idx, node in intersection_nodes.iterrows():\n",
    "\n",
    "        # LOG CODE - Progress logs\n",
    "        # Measures current progress, prints if passed a checkpoint of progress_logs list.\n",
    "        current_progress = (progress_count / len(intersection_nodes))*100\n",
    "        for checkpoint in progress_logs:\n",
    "            if (current_progress >= checkpoint):\n",
    "                print(f\"network_intersections_update(): Iterating over intersection_nodes. {checkpoint}% done.\")\n",
    "                progress_logs.remove(checkpoint)\n",
    "                break\n",
    "        # LOG CODE - Progress logs\n",
    "        \n",
    "        # Current intersection_node's data\n",
    "        intersection_node_osmid = node['osmid'] # Intersection point\n",
    "        intersected_u = node['u'] # Edge that's being intersected\n",
    "        intersected_v = node['v'] # Edge that's being intersected\n",
    "        intersected_key = node['key'] # Edge that's being intersected\n",
    "        intersected_retain_how = node['retain_how'] # Which parts of the edge will be kept\n",
    "\n",
    "        # Helps debug:\n",
    "        #check_lst = [1493,1490,1502,1498]\n",
    "        #if intersection_node_osmid not in check_lst:\n",
    "        #    continue\n",
    "\n",
    "        if intersection_logs:\n",
    "            print(\"--\"*30)\n",
    "            print(f\"network_intersections_update(): Iterating over intersection_node_osmid {intersection_node_osmid}.\")\n",
    "\n",
    "        # 1.1 --------------- MULTIPLE INTERSECTION ADAPTATION CHECK\n",
    "        # ------------------- This additional step reviews the performed_intersections dictionary to check if \n",
    "        # ------------------- the current intersection_node has already split the currently intersected edge.\n",
    "        # ------------------- If it has, it avoids doing it again. If it hasn't, it registers the intersection to be performed.\n",
    "\n",
    "        # Dictionary format explanation:\n",
    "        # Each original_edge_id has a dictionary that stores which intersection_node (dict. key) created a split and the resulting edge_ids (dict. values).\n",
    "        # {original_edge_id:{intersection_node_osmid:[new_edge_ids]}}\n",
    "\n",
    "        # Find original unique edge_id of the edge being currently intersected\n",
    "        original_edge_id = str(intersected_u)+str(intersected_v)+str(intersected_key)\n",
    "                \n",
    "        # Check and/or update dictionary\n",
    "        if original_edge_id in performed_intersections.keys():\n",
    "            # This original edge_id has already been intersected (by any given intersection_node), \n",
    "            # --> extract its dictionary to check if the intersection was performed with the current intersection_node.\n",
    "            original_edge_dictionary = performed_intersections[original_edge_id]\n",
    "            if intersection_node_osmid in original_edge_dictionary.keys():\n",
    "                # Case A: This original edge_id has already been intersected BY THIS intersection_node.\n",
    "                # --> Do not intersect again (continue)\n",
    "                if intersection_logs:\n",
    "                    print(f\"network_intersections_update(): Skipping intersection between original edge {original_edge_id} and osmid {intersection_node_osmid}. It already happened.\")\n",
    "                continue\n",
    "            else:\n",
    "                # Case B: The current original_edge has already been intersected BY ANOTHER intersection node.\n",
    "                # --> Save the current intersection_node as a new node intersecting this original edge_id.\n",
    "                original_edge_dictionary[intersection_node_osmid] = [] #Inserting the new intersection with an empty list of resulting edge_ids (to be created)\n",
    "                performed_intersections[original_edge_id] = original_edge_dictionary # Updating original_edge_dictionary inside the general dictionary\n",
    "                if intersection_logs:\n",
    "                    print(f\"network_intersections_update(): Registering new intersection for original edge {original_edge_id} using osmid {intersection_node_osmid}.\")\n",
    "        else:\n",
    "            # Case C: The current edge_id has NOT been intersected BY ANY intersection node (First time)            \n",
    "            # --> Add to dictionary and create list\n",
    "            performed_intersections[original_edge_id] = {intersection_node_osmid:[]} # Inserting new dict. key (original_edge_id), its dictionary with the node and an empty list.\n",
    "            if intersection_logs:\n",
    "                print(f\"network_intersections_update(): First time registering intersection between edge {original_edge_id} and osmid {intersection_node_osmid}.\")\n",
    "        \n",
    "        # 1.2 --------------- Split the current_ntw's intersected edge using the intersection_node as clipping_point. \n",
    "        # ------------------- This split (Using function edge_clipping()) creates two separate edges:\n",
    "        # ------------------- The first edge will be related to the starting_point_gdf (We'll set intersected edge 'u')\n",
    "        # ------------------- The second edge will be related to the opposite side (Will be intersected edge 'v')\n",
    "    \n",
    "            \n",
    "        # 1.2.1 - Identify and extract current intersection node as a gdf (Becomes clipping_point_gdf in function edge_clipping)\n",
    "        # ------- MULTIPLE INTERSECTION ADAPTATION:\n",
    "        # ------- In cases where the intersection_nodes always intersect two edges only, osmid is only needed to identify the current node.\n",
    "        # ------- Else, more data and dropping duplicates is required. [Filtering through idx should also work, but couldn't convert Series to GDF]\n",
    "        intersection_node_idx = (intersection_nodes.u==intersected_u)&(intersection_nodes.v==intersected_v)&(intersection_nodes.key==intersected_key)&(intersection_nodes.osmid==intersection_node_osmid)\n",
    "        intersection_node = intersection_nodes.loc[intersection_node_idx].copy()\n",
    "        intersection_node.drop_duplicates(inplace=True)\n",
    "        intersection_node.reset_index(inplace=True,drop=True)\n",
    "        if intersection_logs:\n",
    "            print(f\"network_intersections_update(): Printing intersection node for osmid with original intersected edge data {intersection_node_osmid}.\")\n",
    "            print(intersection_node)\n",
    "        \n",
    "        # 1.2.2 - Extract current_ntw's intersected edge (Becomes edge_gdf in function edge_clipping)\n",
    "        # ------- The 'try' section tries to extract it using 'u', 'v' and 'key'. But that data may have changed inside this function.\n",
    "        # ------- The 'except' section finds the edge using a geometrical approach and the performed_intersections dictionary (if needed).\n",
    "        try:\n",
    "            # 1.2.2a TRY: Load the edge registered as intersected in the intersection_nodes gdf.\n",
    "            # This works if this is the first time that the edge is being intersected since it looks for\n",
    "            # the edge's original u('intersected_u'), v('intersected_v') and key('intersected_key')\n",
    "            intersected_edge = current_ntw_edges.loc[(current_ntw_edges['u'] == intersected_u) & \n",
    "                                                     (current_ntw_edges['v'] == intersected_v) &\n",
    "                                                     (current_ntw_edges['key'] == intersected_key)].copy()\n",
    "            intersected_edge.reset_index(inplace=True,drop=True)\n",
    "            if len(intersected_edge) == 0:\n",
    "                # If it has len=0, it means that the edge no longer exists (deleted in following steps in this function).\n",
    "                # This happens because that edge had another intersection along its lenght and that original unsplit edge was split and deleted.\n",
    "                # ----- EXCEPTION. DOES NOT STOP EXECUTION -----\n",
    "                if intersection_logs:\n",
    "                    print(f\"network_intersections_update(): Could not find the original intersected edge {original_edge_id}.\")\n",
    "                intended_crash\n",
    "                # ----- EXCEPTION. DOES NOT STOP EXECUTION -----\n",
    "                \n",
    "        except:\n",
    "            # 1.2.2b EXCEPT: Find the new (already split) edge by buffering the current intersection_node \n",
    "            # and (if necessary) looking for new_edges created from this original_edge_id in the performed_intersections dictionary.\n",
    "            if intersection_logs:\n",
    "                print(f\"network_intersections_update(): Searching for already split edge originating from edge with u {intersected_u}, v {intersected_v} and key {intersected_key}.\")\n",
    "            # Create a VERY SMALL buffer around the intersection_node\n",
    "            intersection_node_buffer = intersection_node.buffer(1e-9)\n",
    "            intersection_node_buffer = gpd.GeoDataFrame(geometry=intersection_node_buffer)\n",
    "            # Find which edges cross that VERY SMALL buffer to identify potential edges to split\n",
    "            edge_data = intersection_node_buffer.sjoin(current_ntw_edges)\n",
    "            if len(edge_data) == 0: #Not likely\n",
    "                # CASE A: Found no edges touching the intersection_node_buffer. (Problem)\n",
    "                # ----- ERROR. STOPS EXECUTION -----\n",
    "                print(f\"\"\"\n",
    "                network_intersections_update(): ERROR: Problem on intersection_node {intersection_node_osmid}. Found 0 edges.\n",
    "                Check GIS for geometry inconsistencies.\n",
    "                \"\"\")\n",
    "                intended_crash\n",
    "                # ----- ERROR. STOPS EXECUTION -----\n",
    "            \n",
    "            elif len(edge_data) == 1: #Desired case\n",
    "                # CASE A: Found one edge touching the intersection_node_buffer.\n",
    "                intersected_u = edge_data.u.unique()[0]\n",
    "                intersected_v = edge_data.v.unique()[0]\n",
    "                intersected_key = edge_data.key.unique()[0]\n",
    "                if intersection_logs:\n",
    "                    print(f\"network_intersections_update(): Found one edge near the intersection_node.\")\n",
    "                \n",
    "            else: #Multi intersection\n",
    "                # CASE C: Found multiple edges touching the intersection_node_buffer.\n",
    "                # ------- MULTIPLE INTERSECTION ADAPTATION\n",
    "                # ------- It is possible that the node is intersecting two (or more) edges from the same network.\n",
    "                # ------- Also, some of them (Appart from the current original_edge_id) may also have already been clipped.\n",
    "                # ------- It is necessary to identify the edge that should be intersected.\n",
    "                # ------- Approach: Use the data saved in the performed_intersections dictionary to identify which edge from edge_data\n",
    "                # ----------------- derives from the original_edge_id.\n",
    "                if intersection_logs:\n",
    "                    print(f\"network_intersections_update(): Found multiple edges near the intersection_node.\")\n",
    "                \n",
    "                # Create a unique edge_id for all potential edges\n",
    "                # [ROUNDING CAUSES ISSUES WITH BIG OSMIDS, CHANGE TO STRING IDs WHEN TIME AVAILABLE]\n",
    "                #edge_data['u'] = edge_data['u'].apply(lambda x: int(round(float(x),0)))\n",
    "                #edge_data['v'] = edge_data['v'].apply(lambda x: int(round(float(x),0)))\n",
    "                #edge_data['key'] = edge_data['key'].apply(lambda x: int(round(float(x),0)))\n",
    "                edge_data = src.create_unique_edge_id(edge_data)\n",
    "                if intersection_logs:\n",
    "                    print(f\"network_intersections_update(): Edges found: {list(edge_data.edge_id.unique())}\")\n",
    "\n",
    "                # Retrieve all the new_edge_ids generated from the current original_edge_id.\n",
    "                potential_edge_ids = []\n",
    "                original_edge_dictionary = performed_intersections[original_edge_id] #Dictionary with all {node:[new_edge_ids]}\n",
    "                for node in original_edge_dictionary.keys():\n",
    "                    generated_lst = original_edge_dictionary[node]\n",
    "                    for generated_edge_id in generated_lst:\n",
    "                        potential_edge_ids.append(generated_edge_id)\n",
    "                if intersection_logs:\n",
    "                    print(f\"network_intersections_update(): Previously created (split) edges from the current original_edge_id: {potential_edge_ids}.\")\n",
    "                \n",
    "                # Filter the currently found edge_data\n",
    "                edge_data = edge_data.loc[edge_data.edge_id.isin(potential_edge_ids)].copy()\n",
    "                # Find the edge to be split\n",
    "                if len(edge_data) == 1: # Expected result\n",
    "                    intersected_u = edge_data.u.unique()[0]\n",
    "                    intersected_v = edge_data.v.unique()[0]\n",
    "                    intersected_key = edge_data.key.unique()[0]\n",
    "                else: # Unlikely problem\n",
    "                    # With the new MULTIPLE INTERSECTION ADAPTATION, if any intersection_node reaches this part of the code, it may be a problem.\n",
    "                    # It means that there either 0 or +2 edges that derived from the original_edge_id and are located where the intersection_node is.\n",
    "                    # --> Skip this posible conection.\n",
    "                    # ----- EXCEPTION. DOES NOT STOP EXECUTION -----\n",
    "                    print(f\"\"\"\n",
    "                    Error on intersection_node {intersection_node_osmid}:\n",
    "                    Could not select intersected_edge after looking at {len(edge_data)} edges while searching for substitute for original edge {original_edge_id}.\n",
    "                    ERROR: Printing filtered edges found near intersection_node (edge_data={len(edge_data)}) and returning current_ntw_nodes and current_ntw_edges.\n",
    "                    {edge_data}\n",
    "                    \"\"\")\n",
    "                    # ----- EXCEPTION. DOES NOT STOP EXECUTION -----\n",
    "                    \n",
    "                    # LOG CODE - Progress logs\n",
    "                    # Finished reviewing current osmid. Continue with next osmid in non_contact_osmids.\n",
    "                    progress_count+=1\n",
    "                    # LOG CODE - Progress logs\n",
    "                    continue  #Skip to next connection\n",
    "            \n",
    "            # Retrieve the found edge\n",
    "            intersected_edge = current_ntw_edges.loc[(current_ntw_edges['u'] == intersected_u) & \n",
    "                                                     (current_ntw_edges['v'] == intersected_v) &\n",
    "                                                     (current_ntw_edges['key'] == intersected_key)].copy()\n",
    "            intersected_edge.reset_index(inplace=True,drop=True)\n",
    "            if intersection_logs:\n",
    "                print(f\"network_intersections_update(): Intersection_node {intersection_node_osmid} found already intersected edge u {intersected_u}, v {intersected_v}, key {intersected_key}.\")\n",
    "\n",
    "        # 1.2.3 - Clip the intersected_edge with the intersection_node\n",
    "        # ------- This creates a GeoDataFrame (split_edge_gdf) that will later be concatenated to current_ntw_edges\n",
    "\n",
    "        if intersection_logs:\n",
    "            print(f\"network_intersections_update(): Clipping intersected_edge. \")\n",
    "        \n",
    "        # Extract current_ntw's intersected edge's u node as a point\n",
    "        # (Always becomes starting_point_gdf in function edge_clipping when using 'return_all')\n",
    "        u_node = current_ntw_nodes.loc[(current_ntw_nodes['osmid'] == intersected_u)].copy()\n",
    "        u_node.reset_index(inplace=True,drop=True)\n",
    "        # Extract current_ntw's intersected edge's v node as a point\n",
    "        v_node = current_ntw_nodes.loc[(current_ntw_nodes['osmid'] == intersected_v)].copy()\n",
    "        v_node.reset_index(inplace=True,drop=True)\n",
    "        # Apply edge_clipping function and assign the corresponding 'u', 'v' or 'key' data.\n",
    "        try:\n",
    "            if intersected_retain_how == 'both':\n",
    "                # Clip edge\n",
    "                split_edge_gdf = edge_clipping(starting_point_gdf = u_node,\n",
    "                                               edge_gdf = intersected_edge,\n",
    "                                               clipping_point_gdf = intersection_node,\n",
    "                                               projected_crs = projected_crs,\n",
    "                                               return_all = True,\n",
    "                                               function_logs = clipping_logs)\n",
    "                if intersection_logs:\n",
    "                    print(f\"network_intersections_update(): Intersection_node {intersection_node_osmid} split the edge and created {len(split_edge_gdf)} new edges.\")\n",
    "                # Assign data\n",
    "                # When return_all=True in function edge_clipping, \n",
    "                # assigns 'starting' to the edge related to the starting_point_gdf\n",
    "                # and 'ending' to edge on the opposite side.\n",
    "                \n",
    "                # Create temporary gdf with new edges's data\n",
    "                # (For each key, first element of list corresponds to the 'starting' edge, second element to the 'ending' edge)\n",
    "                df_temporal = pd.DataFrame({'u': [int(intersected_u), \n",
    "                                                  int(intersection_node_osmid)],\n",
    "                                            'v': [int(intersection_node_osmid),\n",
    "                                                  int(intersected_v)],\n",
    "                                            'key': [int(0),\n",
    "                                                    int(0)],\n",
    "                                            'geometry': [split_edge_gdf.loc[split_edge_gdf.relation=='starting']['geometry'].unique()[0],\n",
    "                                                        split_edge_gdf.loc[split_edge_gdf.relation=='ending']['geometry'].unique()[0]],\n",
    "                                           })\n",
    "                gdf_temporal = gpd.GeoDataFrame(df_temporal, geometry='geometry', crs=projected_crs)\n",
    "                # Force all datatypes to match the datatypes of the gdf to where the data will be merged\n",
    "                dtypes_dict = current_ntw_edges.dtypes.to_dict() #Dict with current_ntw_edges dtypes\n",
    "                filtered_dtypes = {col: dtypes_dict[col] for col in gdf_temporal.columns if col in dtypes_dict} # Filters for cols in gdf_temporal\n",
    "                gdf_temporal = gdf_temporal.astype(filtered_dtypes) # Assigns those types to gdf_temporal           \n",
    "                # Rename gdf\n",
    "                split_edge_gdf = gdf_temporal.copy()\n",
    "                \n",
    "                a=\"\"\"\n",
    "                # Identify split edge 1\n",
    "                u_idx = split_edge_gdf.relation=='starting'\n",
    "                split_edge_gdf.loc[u_idx,'u'] = intersected_u # We assigned 'u' as starting_point_gdf\n",
    "                split_edge_gdf.loc[u_idx,'v'] = intersection_node_osmid # Intersection\n",
    "                split_edge_gdf.loc[u_idx,'key'] = 0 #Since this 'u' and 'v' relation is new, key=0\n",
    "                # Identify split edge 2\n",
    "                v_idx = split_edge_gdf.relation=='ending'\n",
    "                split_edge_gdf.loc[v_idx,'u'] = intersection_node_osmid # Intersection\n",
    "                split_edge_gdf.loc[v_idx,'v'] = intersected_v # Opposite side\n",
    "                split_edge_gdf.loc[v_idx,'key'] = 0 #Since this 'u' and 'v' relation is new, key=0\n",
    "\n",
    "                print(f\"Printing split_edge_gdf dtypes after adding relations data.\")\n",
    "                print(split_edge_gdf.dtypes)\n",
    "                \n",
    "                # Make sure unique identifiers ('u','v', and 'key') are integers to avoid problems when creating edge_ids.\n",
    "                split_edge_gdf['u'] = split_edge_gdf['u'].apply(lambda x: int(round(float(x),0)))\n",
    "                split_edge_gdf['v'] = split_edge_gdf['v'].apply(lambda x: int(round(float(x),0)))\n",
    "                split_edge_gdf['key'] = split_edge_gdf['key'].apply(lambda x: int(round(float(x),0)))\n",
    "                if intersection_logs:\n",
    "                    print(f\"network_intersections_update(): Printing created edges.\")\n",
    "                    print(split_edge_gdf)\n",
    "\n",
    "                print(f\"Printing split_edge_gdf dtypes after fixing dtypes.\")\n",
    "                print(split_edge_gdf.dtypes)\n",
    "                \"\"\"\n",
    "                \n",
    "                # ------- MULTIPLE INTERSECTION ADAPTATION - Update performed_intersections dictionary with new edges.\n",
    "                # Prepare new_edge_ids to register in dictionary\n",
    "                new_edge_id_1 = str(intersected_u)+str(intersection_node_osmid)+str(0)\n",
    "                new_edge_id_2 = str(intersection_node_osmid)+str(intersected_v)+str(0)\n",
    "                if intersection_logs:\n",
    "                    print(f\"network_intersections_update(): Printing new edge_ids: {new_edge_id_1}, {new_edge_id_2}.\")\n",
    "                # Read from general dictionary the edge_ids previously created by this intersection_node in this original_edge\n",
    "                already_created_edge_ids = performed_intersections[original_edge_id][intersection_node_osmid]\n",
    "                # Add to the list the new generated edge_ids\n",
    "                already_created_edge_ids.append(new_edge_id_1)\n",
    "                already_created_edge_ids.append(new_edge_id_2)\n",
    "                # Insert data to general dictionary\n",
    "                performed_intersections[original_edge_id][intersection_node_osmid] = already_created_edge_ids\n",
    "            \n",
    "            elif intersected_retain_how == 'u':\n",
    "                # Clip edge\n",
    "                split_edge_gdf = edge_clipping(starting_point_gdf = u_node,\n",
    "                                               edge_gdf = intersected_edge,\n",
    "                                               clipping_point_gdf = intersection_node,\n",
    "                                               projected_crs = projected_crs,\n",
    "                                               return_all = False,\n",
    "                                               function_logs = clipping_logs)\n",
    "                if intersection_logs:\n",
    "                    print(f\"network_intersections_update(): Intersection_node {intersection_node_osmid} split the edge and created {len(split_edge_gdf)} new edges.\")\n",
    "                \n",
    "                # Create temporary gdf with new edges's data\n",
    "                df_temporal = pd.DataFrame({'u': [int(intersected_u)],\n",
    "                                            'v': [int(intersection_node_osmid)],\n",
    "                                            'key': [int(0)],\n",
    "                                            'geometry': [split_edge_gdf['geometry'].unique()[0]],\n",
    "                                           })\n",
    "                gdf_temporal = gpd.GeoDataFrame(df_temporal, geometry='geometry', crs=projected_crs)\n",
    "                # Force all datatypes to match the datatypes of the gdf to where the data will be merged\n",
    "                dtypes_dict = current_ntw_edges.dtypes.to_dict() #Dict with current_ntw_edges dtypes\n",
    "                filtered_dtypes = {col: dtypes_dict[col] for col in gdf_temporal.columns if col in dtypes_dict} # Filters for cols in gdf_temporal\n",
    "                gdf_temporal = gdf_temporal.astype(filtered_dtypes) # Assigns those types to gdf_temporal            \n",
    "                # Rename gdf\n",
    "                split_edge_gdf = gdf_temporal.copy()\n",
    "                \n",
    "                a=\"\"\"\n",
    "                # Assign data\n",
    "                split_edge_gdf.loc[0,'u'] = intersected_u\n",
    "                split_edge_gdf.loc[0,'v'] = intersection_node_osmid # Intersection\n",
    "                split_edge_gdf.loc[0,'key'] = 0 #Since this 'u' and 'v' relation is new, key=0\n",
    "                # Make sure unique identifiers ('u','v', and 'key') are integers to avoid problems when creating edge_ids.\n",
    "                split_edge_gdf['u'] = split_edge_gdf['u'].apply(lambda x: int(round(float(x),0)))\n",
    "                split_edge_gdf['v'] = split_edge_gdf['v'].apply(lambda x: int(round(float(x),0)))\n",
    "                split_edge_gdf['key'] = split_edge_gdf['key'].apply(lambda x: int(round(float(x),0)))\n",
    "                \"\"\"\n",
    "                \n",
    "                # ------- MULTIPLE INTERSECTION ADAPTATION - Update performed_intersections dictionary with new edges.\n",
    "                # Prepare new_edge_ids to register in dictionary\n",
    "                new_edge_id_1 = str(intersected_u)+str(intersection_node_osmid)+str(0)\n",
    "                # Read from general dictionary the edge_ids previously created by this intersection_node in this original_edge\n",
    "                already_created_edge_ids = performed_intersections[original_edge_id][intersection_node_osmid]\n",
    "                # Add to the list the new generated edge_ids\n",
    "                already_created_edge_ids.append(new_edge_id_1)\n",
    "                # Insert data to general dictionary\n",
    "                performed_intersections[original_edge_id][intersection_node_osmid] = already_created_edge_ids\n",
    "                \n",
    "            elif intersected_retain_how == 'v':\n",
    "                # Clip edge\n",
    "                split_edge_gdf = edge_clipping(starting_point_gdf = v_node,\n",
    "                                               edge_gdf = intersected_edge,\n",
    "                                               clipping_point_gdf = intersection_node,\n",
    "                                               projected_crs = projected_crs,\n",
    "                                               return_all = False,\n",
    "                                               function_logs = clipping_logs)\n",
    "                if intersection_logs:\n",
    "                    print(f\"network_intersections_update(): Intersection_node {intersection_node_osmid} split the edge and created {len(split_edge_gdf)} new edges.\")\n",
    "                    \n",
    "                # Create temporary gdf with new edges's data\n",
    "                df_temporal = pd.DataFrame({'u': [int(intersection_node_osmid)],\n",
    "                                            'v': [int(intersected_v)],\n",
    "                                            'key': [int(0)],\n",
    "                                            'geometry': [split_edge_gdf['geometry'].unique()[0]],\n",
    "                                           })\n",
    "                gdf_temporal = gpd.GeoDataFrame(df_temporal, geometry='geometry', crs=projected_crs)\n",
    "                # Force all datatypes to match the datatypes of the gdf to where the data will be merged\n",
    "                dtypes_dict = current_ntw_edges.dtypes.to_dict() #Dict with current_ntw_edges dtypes\n",
    "                filtered_dtypes = {col: dtypes_dict[col] for col in gdf_temporal.columns if col in dtypes_dict} # Filters for cols in gdf_temporal\n",
    "                gdf_temporal = gdf_temporal.astype(filtered_dtypes) # Assigns those types to gdf_temporal\n",
    "                # Rename gdf\n",
    "                split_edge_gdf = gdf_temporal.copy()\n",
    "\n",
    "                a=\"\"\"\n",
    "                # Assign data\n",
    "                split_edge_gdf.loc[0,'u'] = intersection_node_osmid # Intersection\n",
    "                split_edge_gdf.loc[0,'v'] = intersected_v\n",
    "                split_edge_gdf.loc[0,'key'] = 0 #Since this 'u' and 'v' relation is new, key=0\n",
    "                # Make sure unique identifiers ('u','v', and 'key') are integers to avoid problems when creating edge_ids.\n",
    "                split_edge_gdf['u'] = split_edge_gdf['u'].apply(lambda x: int(round(float(x),0)))\n",
    "                split_edge_gdf['v'] = split_edge_gdf['v'].apply(lambda x: int(round(float(x),0)))\n",
    "                split_edge_gdf['key'] = split_edge_gdf['key'].apply(lambda x: int(round(float(x),0)))\n",
    "                \"\"\"\n",
    "                # ------- MULTIPLE INTERSECTION ADAPTATION - Update performed_intersections dictionary with new edges.\n",
    "                # Prepare new_edge_ids to register in dictionary\n",
    "                new_edge_id_1 = str(intersection_node_osmid)+str(intersected_v)+str(0)\n",
    "                # Read from general dictionary the edge_ids previously created by this intersection_node in this original_edge\n",
    "                already_created_edge_ids = performed_intersections[original_edge_id][intersection_node_osmid]\n",
    "                # Add to the list the new generated edge_ids\n",
    "                already_created_edge_ids.append(new_edge_id_1)\n",
    "                # Insert data to general dictionary\n",
    "                performed_intersections[original_edge_id][intersection_node_osmid] = already_created_edge_ids\n",
    "                \n",
    "            else:\n",
    "                print(f\"ERROR splitting edge with u {intersected_u}, v {intersected_v} and key {intersected_key}.\")\n",
    "                print(\"Make sure to include in gdf intersection_nodes column 'retain_how' with either 'u','v' or 'both'.\")\n",
    "                intended_crash\n",
    "        except:\n",
    "            print(f\"Failed edge_clipping on intersection_node {intersection_node_osmid}.\")\n",
    "            print(f\"Failed intersected_u: {intersected_u}.\")\n",
    "            print(f\"Failed intersected_v: {intersected_v}.\")\n",
    "            print(f\"Failed intersected_key: {intersected_key}.\")\n",
    "\n",
    "            # LOG CODE - Progress logs\n",
    "            # Finished reviewing current osmid. Continue with next osmid in non_contact_osmids.\n",
    "            progress_count+=1\n",
    "            # LOG CODE - Progress logs\n",
    "            continue\n",
    "    \n",
    "        # 1.3 --------------- Update current_ntw\n",
    "        # ------------------- The intersection_node is concatenated into current_ntw_nodes.\n",
    "        # ------------------- The split edge(s) is(are) concatenated into current_ntw_edges, the old edge is deleted.\n",
    "\n",
    "        # 1.3.1 - Register node\n",
    "        # Set an identifier to make it easier to locate nodes that resulted from an intersection between networks\n",
    "        intersection_node['intersecting'] = 1\n",
    "        # Prepare node for concatenation\n",
    "        intersection_node = intersection_node[['osmid','intersecting','geometry']]\n",
    "        # Register network origin (if requested)\n",
    "        if consider_ntw_origin:\n",
    "            intersection_node['ntw_origin'] = 'ntw_join' #Intersecting node was created to join both networks.\n",
    "        \n",
    "        # Add new node\n",
    "        current_ntw_nodes = pd.concat([current_ntw_nodes,intersection_node])\n",
    "        # Reset index\n",
    "        current_ntw_nodes.reset_index(inplace=True,drop=True)\n",
    "        if intersection_logs:\n",
    "                print(f\"network_intersections_update(): Concatenated {len(intersection_node)} new nodes to output.\")\n",
    "\n",
    "        # 1.3.1 - Register edge(s)\n",
    "        # Keep all edges except the edge that was split\n",
    "        # (Must remove to avoid duplicating edge's geometries)\n",
    "        current_ntw_edges = current_ntw_edges.loc[~((current_ntw_edges['u'] == int(intersected_u)) &\n",
    "                                                    (current_ntw_edges['v'] == int(intersected_v)) &\n",
    "                                                    (current_ntw_edges['key'] == int(intersected_key)))].copy()\n",
    "        # Prepare edges for concatenation\n",
    "        split_edge_gdf = split_edge_gdf[['u','v','key','geometry']]\n",
    "        # Set an identifier to make it easier to locate edges that were split\n",
    "        split_edge_gdf['intersecting'] = 1\n",
    "        # Register network origin (if requested)\n",
    "        if consider_ntw_origin:\n",
    "            # Set origin to 'not_found'\n",
    "            original_edge_id_origin = 'not_found'\n",
    "            # Replace by data in dictionary\n",
    "            for origin in edge_origin_dct:\n",
    "                if original_edge_id in edge_origin_dct[origin]:\n",
    "                    original_edge_id_origin = origin\n",
    "            # Register in output\n",
    "            split_edge_gdf['ntw_origin'] = original_edge_id_origin # Registered origin depends on edge_origin_dct    \n",
    "            \n",
    "        # Add new edge\n",
    "        current_ntw_edges = pd.concat([current_ntw_edges,split_edge_gdf])\n",
    "        # Reset index\n",
    "        current_ntw_edges.reset_index(inplace=True,drop=True)\n",
    "        if intersection_logs:\n",
    "            print(f\"network_intersections_update(): Concatenated {len(split_edge_gdf)} new edges to output.\")\n",
    "\n",
    "        # LOG CODE - Progress logs\n",
    "        # Finished reviewing current osmid. Continue with next osmid in non_contact_osmids.\n",
    "        progress_count+=1\n",
    "        # LOG CODE - Progress logs\n",
    "\n",
    "    # 1.4 --------------- Format final output\n",
    "    # ------------------- Filters for columns of interest and sets column types\n",
    "    if consider_ntw_origin:\n",
    "        updated_ntw_nodes = current_ntw_nodes[['osmid','intersecting','ntw_origin','geometry']].copy()\n",
    "    else:\n",
    "        updated_ntw_nodes = current_ntw_nodes[['osmid','intersecting','geometry']].copy()\n",
    "    # Set unique identifiers to int\n",
    "    updated_ntw_nodes['osmid'] = updated_ntw_nodes['osmid'].astype('int')\n",
    "    del current_ntw_nodes\n",
    "\n",
    "    if consider_ntw_origin:\n",
    "        updated_ntw_edges = current_ntw_edges[['u','v','key','intersecting','ntw_origin','geometry']].copy()\n",
    "    else:\n",
    "        updated_ntw_edges = current_ntw_edges[['u','v','key','intersecting','geometry']].copy()\n",
    "    # Set unique identifiers to int\n",
    "    updated_ntw_edges['u'] = updated_ntw_edges['u'].astype('int')\n",
    "    updated_ntw_edges['v'] = updated_ntw_edges['v'].astype('int')\n",
    "    updated_ntw_edges['key'] = updated_ntw_edges['key'].astype('int')\n",
    "    del current_ntw_edges\n",
    "\n",
    "    print(f\"Finished updating network.\")\n",
    "    \n",
    "    # After iterating over both networks, return result\n",
    "    return updated_ntw_nodes, updated_ntw_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35b8531-fd1f-4877-a7a8-2f7b656c6291",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### __Functions -__ drop_intersection_network_duplicates [Used after networks_intersections_update in Part02_Step01 and Part02_Step04]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e15fad8-4440-4e49-86b0-d2b964385c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_intersection_network_duplicates(nodes_gdf,edges_gdf,function_logs=False):\n",
    "\n",
    "    \"\"\" This function was created as a complement to function network_intersections_update().\n",
    "        Whenever three or more lines intersect exactly at the same point, function network_intersections_update() creates duplicated nodes and edges.\n",
    "        Those duplicates cannot be easly dropped since lines can be mirrored. Example:\n",
    "        Line 1: u=1, v=2, key=0, geom=((1,1),(2,1))\n",
    "        Line 2: u=2, v=1, key=0, geom=((2,1),(1,1))\n",
    "        \n",
    "        This function solves for those duplicated network geometries.\n",
    "    \n",
    "\tArgs:\n",
    "\t\tnodes_gdf (geopandas.GeoDataFrame): GeoDataFrame containing the nodes from the recently updated network.\n",
    "                                                    Requires a unique identifier 'osmid'.\n",
    "        edges_gdf (geopandas.GeoDataFrame): GeoDataFrame containing the edges from the recently updated network.\n",
    "                                                    Requires the unique identifiers 'u ,'v' and 'key'.\n",
    "        function_logs (bool,optional): Boolean that (if True) prints logs during the current function's execution. Defaults to False.\n",
    "\n",
    "\tReturns:\n",
    "        nodes_gdf (geopandas.GeoDataFrame): GeoDataFrame containing the updated nodes without duplicates.\n",
    "        edges_gdf (geopandas.GeoDataFrame): GeoDataFrame containing the updated edges without duplicates.\n",
    "        \n",
    "\t\"\"\" \n",
    "    ##### Time\n",
    "    time_1 = time.time()\n",
    "    ##### Time\n",
    "    \n",
    "    # Copy input to avoid rewritting\n",
    "    nodes_gdf = nodes_gdf.copy()\n",
    "    edges_gdf = edges_gdf.copy()\n",
    "\n",
    "    # 1.0 --------------- Dropping duplicates on nodes\n",
    "    # Dropping duplicates on nodes by using osmid and geometry\n",
    "    dup_osmids_lst = list(nodes_gdf.loc[nodes_gdf.duplicated('osmid')].osmid.unique())\n",
    "    if len(dup_osmids_lst) > 0:\n",
    "        print(f\"WARNING: Found the following duplicated osmids: {dup_osmids_lst}. NOT ALL WILL BE DELETED. Checking details is advised.\")\n",
    "        \n",
    "    current_len = len(nodes_gdf)\n",
    "    nodes_gdf.drop_duplicates(subset=['osmid','geometry'],inplace=True)\n",
    "    updated_len = len(nodes_gdf)\n",
    "    \n",
    "    if (current_len-updated_len) > 0:\n",
    "        print(f\"DROPPED {current_len-updated_len} nodes that had the same osmid and geometry.\")\n",
    "\n",
    "    # 2.0 --------------- Dropping duplicates on edges\n",
    "    \n",
    "    # 2.1 --------------- Identify potential duplicates by comparing edge_id in a regular and inverted order\n",
    "    # Unique edge id with regular order ('u','v','key')\n",
    "    edges_gdf = src.create_unique_edge_id(edges_gdf)\n",
    "    edges_gdf.rename(columns={'edge_id':'edge_id_1'},inplace=True)\n",
    "    \n",
    "    # Dropping duplicates on nodes by using edge_id_1 (normal 'u','v','key' order).\n",
    "    dup_edgeids_lst = list(edges_gdf.loc[edges_gdf.duplicated('edge_id_1')].edge_id_1.unique())\n",
    "    if len(dup_edgeids_lst) > 0:\n",
    "        print(f\"WARNING: Found the following duplicated edge_ids: {dup_edgeids_lst}. NOT ALL WILL BE DELETED. Checking details is advised.\")\n",
    "        \n",
    "    current_len = len(edges_gdf)\n",
    "    edges_gdf.drop_duplicates(subset=['edge_id_1','geometry'],inplace=True)\n",
    "    updated_len = len(edges_gdf)\n",
    "\n",
    "    if (current_len-updated_len) > 0:\n",
    "        print(f\"DROPPED {current_len-updated_len} edges that had the same edge_id and geometry.\")\n",
    "\n",
    "    ##### Time\n",
    "    time_2 = time.time()\n",
    "    if function_logs:\n",
    "        print(f\"drop_intersection_network_duplicates(): {time_2-time_1} seconds on dropping simple nodes (osmid, geometry) and edges (edge_id, geometry).\")\n",
    "    ##### Time\n",
    "    \n",
    "    # Unique edge id with inverted order ('v','u','key')\n",
    "    edges_gdf = src.create_unique_edge_id(edges_gdf,order='vukey')\n",
    "    edges_gdf.rename(columns={'edge_id':'edge_id_2'},inplace=True)\n",
    "    dup_inverted_lst = []\n",
    "    \n",
    "    # Identify edges where edge_id is in both regular and inverted order\n",
    "    edge_id_1_set = set(edges_gdf.edge_id_1.unique())\n",
    "    edge_id_2_set = set(edges_gdf.edge_id_2.unique())\n",
    "    dup_inverted_lst = list(edge_id_1_set & edge_id_2_set)\n",
    "\n",
    "    ##### Time\n",
    "    time_3 = time.time()\n",
    "    if function_logs:\n",
    "        print(f\"drop_intersection_network_duplicates(): {time_3-time_2} seconds on identifying potential mirrored edges.\")\n",
    "    ##### Time\n",
    "\n",
    "    # 2.2 --------------- Verify potential duplicates and register one of them to be dropped\n",
    "    # Verify those edges are duplicated\n",
    "    confirmed_dup_edge_lst = []\n",
    "    already_dropped_dict = {}\n",
    "    if function_logs:\n",
    "        print(f\"drop_intersection_network_duplicates(): Examination of {len(dup_inverted_lst)} edges that had the same edge_id in both regular and inverted order.\")\n",
    "    for edge_id in dup_inverted_lst:\n",
    "        # Classify one as regular and the other as inverted\n",
    "        regular_edge = edges_gdf.loc[edges_gdf.edge_id_1==edge_id]\n",
    "        inverted_edge = edges_gdf.loc[edges_gdf.edge_id_2==edge_id]\n",
    "\n",
    "        # Check if the mirror relation was already registered by searching its 'u' and 'v'\n",
    "        value_1 = regular_edge.u.unique()[0]\n",
    "        value_2 = regular_edge.v.unique()[0]\n",
    "        # If value_1 is already registered in the dictionary and it contains value_2, continue.\n",
    "        if (value_1 in already_dropped_dict.keys()) and (value_2 in already_dropped_dict[value_1]):\n",
    "            if function_logs:\n",
    "                print(f\"drop_intersection_network_duplicates(): Relation {value_1}<-->{value_2} already registered.\")\n",
    "            continue\n",
    "        # If value_2 is already registered in the dictionary and it contains value_1, continue.\n",
    "        elif (value_2 in already_dropped_dict.keys()) and (value_1 in already_dropped_dict[value_2]):\n",
    "            if function_logs:\n",
    "                print(f\"drop_intersection_network_duplicates(): Relation {value_1}<-->{value_2} already registered.\")\n",
    "            continue\n",
    "        # If it has not been registered, check edge's comparison parameters\n",
    "        else:\n",
    "            # Half-loop roads\n",
    "            # (Two roads may share start and end point if they each form half a circle.\n",
    "            # When creating edge_id_1 and edge_id_2 the same edge_id may be found in both columns.\n",
    "            # It is highly unlikely that both have the same length.)\n",
    "            # --> Discard different roads by length\n",
    "            regular_edge_length = regular_edge.length.unique()[0]\n",
    "            inverted_edge_length = inverted_edge.length.unique()[0]\n",
    "            if regular_edge_length != inverted_edge_length:\n",
    "                continue\n",
    "            \n",
    "            # Full-loop roads \n",
    "            # (One road may have the same start-end and end-start if it is a loop. Also, it would have the same length)\n",
    "            # --> Discard same road by looking for different indexes (If it is the same, it is being compared to itself)\n",
    "            regular_edge_index = regular_edge.index[0]\n",
    "            inverted_edge_index = inverted_edge.index[0]\n",
    "            if regular_edge_index == inverted_edge_index:\n",
    "                continue\n",
    "            \n",
    "            # If any given edge_id reaches this part, it has a duplicate and it has not been registered.\n",
    "            # Confirm that one of the edges using this edge_id will be dropped\n",
    "            confirmed_dup_edge_lst.append(edge_id)\n",
    "            # Save the relation that's being dropped\n",
    "            if value_1 not in already_dropped_dict.keys():\n",
    "                already_dropped_dict[value_1] = list([value_2])\n",
    "                if function_logs:\n",
    "                    print(f\"drop_intersection_network_duplicates(): Saved relationship {value_1}<-->{value_2} to be dropped.\")\n",
    "            else:\n",
    "                already_dropped_dict[value_1] = already_dropped_dict[value_1].append(value_2)\n",
    "                if function_logs:\n",
    "                    print(f\"drop_intersection_network_duplicates(): Saved relationship {value_1}<-->{value_2} to be dropped.\")\n",
    "\n",
    "    ##### Time\n",
    "    time_4 = time.time()\n",
    "    if function_logs:\n",
    "        print(f\"drop_intersection_network_duplicates(): {time_4-time_3} seconds on confirming dups.\")\n",
    "    ##### Time\n",
    "            \n",
    "    # 2.3 --------------- Drop confirmed relations\n",
    "    current_len = len(edges_gdf)\n",
    "    edges_gdf = edges_gdf.loc[~edges_gdf.edge_id_2.isin(confirmed_dup_edge_lst)]\n",
    "    updated_len = len(edges_gdf)\n",
    "\n",
    "    if (current_len-updated_len) > 0:\n",
    "        print(f\"DROPPED {current_len-updated_len} edges that had the same edge_id but one was inverted.\")\n",
    "        print(f\"DROPPED the following mirrored edge_ids: {confirmed_dup_edge_lst}.\")\n",
    "    # Drop columns used for dropping duplicates inside drop_intersection_network_duplicates()\n",
    "    edges_gdf.drop(columns=['edge_id_1','edge_id_2'],inplace=True)\n",
    "\n",
    "    ##### Time\n",
    "    time_5 = time.time()\n",
    "    if function_logs:\n",
    "        print(f\"drop_intersection_network_duplicates(): {time_5-time_4} seconds on dropping dups.\")\n",
    "    ##### Time\n",
    "\n",
    "    return nodes_gdf,edges_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b271b44a-f3d4-4f4a-ae0a-8d74d8df0c5a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## __Part 01 - Step 01__ - Load and preprocess networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7457dc-f520-4891-82c1-a90747fb0ef4",
   "metadata": {},
   "source": [
    "This step __loads input networks__ (Boeing nodes and edges, Tessellations nodes and edges) __and transforms their ID data (nodes 'osmid', edges 'u' and 'v') into coordinates__ using function src.network_entities(). Inside that function a modification was made so that a unique key is assigned to each edge, starting from 0 and increasing by one each time an edge with the same 'u' and 'v' is found.\n",
    "\n",
    "The resulting networks id data (nodes 'osmid', edges 'u' and 'v') are then modified again to __ensure that there are no duplicates in those columns__, even after concatenating both nodes gdfs and both edges gdfs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8a4265-f351-4cb1-a8ee-140b659703b8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### __Input data__ - Boeing network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cbf5de-1fc9-4158-b09c-001c10431bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load nodes data\n",
    "boeing_nodes = gpd.read_file(boeing_nodes_dir)\n",
    "\n",
    "# Set CRS\n",
    "if boeing_nodes.crs != projected_crs:\n",
    "    try:\n",
    "        boeing_nodes = boeing_nodes.set_crs(projected_crs)\n",
    "    except:\n",
    "        boeing_nodes = boeing_nodes.to_crs(projected_crs)\n",
    "\n",
    "# Filter and rename data\n",
    "print(list(boeing_nodes.columns))\n",
    "boeing_nodes.reset_index(inplace=True)\n",
    "boeing_nodes = boeing_nodes[['ID','geometry']]\n",
    "boeing_nodes.rename(columns={'ID':'osmid'},inplace=True)\n",
    "\n",
    "# Show\n",
    "print(boeing_nodes.crs)\n",
    "print(boeing_nodes.shape)\n",
    "boeing_nodes.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175d939d-e43b-4b7f-8061-589a8e5ca40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load edges data\n",
    "boeing_edges = gpd.read_file(boeing_edges_dir)\n",
    "\n",
    "# Set CRS\n",
    "if boeing_edges.crs != projected_crs:\n",
    "    try:\n",
    "        boeing_edges = boeing_edges.set_crs(projected_crs)\n",
    "    except:\n",
    "        boeing_edges = boeing_edges.to_crs(projected_crs)\n",
    "\n",
    "# Filter and rename data\n",
    "print(list(boeing_edges.columns))\n",
    "boeing_edges.reset_index(inplace=True)\n",
    "boeing_edges['key'] = 0\n",
    "boeing_edges = boeing_edges[['from','to','key','geometry']]\n",
    "boeing_edges.rename(columns={'from':'u','to':'v'},inplace=True)\n",
    "\n",
    "# Show\n",
    "print(boeing_edges.crs)\n",
    "print(boeing_edges.shape)\n",
    "boeing_edges.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a2d620-ff16-40a5-950d-024cc4f29b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Time\n",
    "time_1 = time.time()\n",
    "##### Time\n",
    "\n",
    "# Transform Boeing nodes and edges ID data to coordinates\n",
    "boeing_coord_nodes, boeing_coord_edges = src.network_entities(boeing_nodes,\n",
    "                                                              boeing_edges,\n",
    "                                                              projected_crs,\n",
    "                                                              expand_coords=(True,100))\n",
    "\n",
    "##### Time\n",
    "time_2 = time.time()\n",
    "print(f\"TIME: {time_2-time_1} seconds on transforming the boeing network.\")\n",
    "##### Time\n",
    "\n",
    "# Nodes format\n",
    "if 'osmid' not in boeing_coord_nodes.columns:\n",
    "    boeing_coord_nodes.reset_index(inplace=True)\n",
    "boeing_coord_nodes['osmid'] = boeing_coord_nodes.osmid.astype(int)\n",
    "# Edges format\n",
    "if 'u' not in boeing_coord_edges.columns:\n",
    "    boeing_coord_edges.reset_index(inplace=True)\n",
    "boeing_coord_edges['u'] = boeing_coord_edges.u.astype(int)\n",
    "boeing_coord_edges['v'] = boeing_coord_edges.v.astype(int)\n",
    "# Set projected crs\n",
    "boeing_coord_nodes = boeing_coord_nodes.to_crs(projected_crs)\n",
    "boeing_coord_edges = boeing_coord_edges.to_crs(projected_crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f070ede-e9d0-4c29-8419-771d750c5898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for duplicates that could pose problems\n",
    "print(\"--\"*30)\n",
    "print(\"--- BOEING NETWORK DUPLICATED NODES (OSMIDS) CHECK:\")\n",
    "# Analyze\n",
    "duplicated_nodes_ids = len(boeing_coord_nodes.loc[boeing_coord_nodes.duplicated(subset=['osmid'])])\n",
    "if duplicated_nodes_ids>0:\n",
    "    raise DuplicatedOsmidsError(\n",
    "    \"\"\"Function src.network_entities() created two nodes with the same osmid.\n",
    "    If src.network_entities's argument \"expand_coords\" is set to False, try setting to (True,100).\n",
    "    The problem is probably due to two nodes being too close to each other. When assigning osmid, both get assigned the same osmid.\n",
    "    If src.network_entities's argument \"expand_coords\" was set to True, augment second value to increase coordinate diferenciation.\n",
    "    (Increases size of 'osmid', 'u' and 'v' values)\n",
    "    \"\"\")\n",
    "# Show\n",
    "print(boeing_coord_nodes.shape)\n",
    "print(boeing_coord_nodes.crs)\n",
    "print(boeing_coord_nodes.dtypes)\n",
    "print(f\"DUPLICATED OSMIDS ON NODES: {duplicated_nodes_ids}.\")\n",
    "print(\"--\"*30)\n",
    "\n",
    "print(\"--- BOEING NETWORK DUPLICATED EDGES (U, V, KEY) CHECK:\")\n",
    "duplicated_edges_ids = len(boeing_coord_edges.loc[boeing_coord_edges.duplicated(subset=['u','v','key'],keep=False)])\n",
    "if duplicated_edges_ids>0:\n",
    "    raise DuplicatedEdgeIdsError(\"\"\"\n",
    "    Function src.network_entities() includes function resolve_duplicates_indexes(), which should have solved this issue.\n",
    "    \"\"\")\n",
    "# Show\n",
    "print(boeing_coord_edges.shape)\n",
    "print(boeing_coord_edges.crs)\n",
    "print(boeing_coord_edges.dtypes)\n",
    "print(f\"DUPLICATED EDGE_IDS ON EDGES: {duplicated_edges_ids}.\")\n",
    "print(\"--\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8036d13-8ed0-41e0-bd42-e7520beb6442",
   "metadata": {},
   "outputs": [],
   "source": [
    "boeing_coord_nodes.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1c538c-3b08-4c8c-bdff-8d7585e9318d",
   "metadata": {},
   "outputs": [],
   "source": [
    "boeing_coord_edges.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f5b205-9bb6-4b36-b3f5-58b4e034c711",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### __Input data__ - Tessellations network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff24113-3aa1-4d74-b059-93f46761948b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load nodes data\n",
    "tess_nodes = gpd.read_file(tess_nodes_dir)\n",
    "\n",
    "# Set CRS\n",
    "if tess_nodes.crs != projected_crs:\n",
    "    try:\n",
    "        tess_nodes = tess_nodes.set_crs(projected_crs)\n",
    "    except:\n",
    "        tess_nodes = tess_nodes.to_crs(projected_crs)\n",
    "\n",
    "# Filter and rename data\n",
    "print(list(tess_nodes.columns))\n",
    "tess_nodes.reset_index(inplace=True)\n",
    "tess_nodes = tess_nodes[['osmid','geometry']]\n",
    "\n",
    "# Show\n",
    "print(tess_nodes.crs)\n",
    "print(tess_nodes.shape)\n",
    "tess_nodes.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e87577-6dc7-4805-9e14-da8b946b9dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load edges data\n",
    "tess_edges = gpd.read_file(tess_edges_dir)\n",
    "\n",
    "# Set CRS\n",
    "if tess_edges.crs != projected_crs:\n",
    "    try:\n",
    "        tess_edges = tess_edges.set_crs(projected_crs)\n",
    "    except:\n",
    "        tess_edges = tess_edges.to_crs(projected_crs)\n",
    "\n",
    "# Filter and rename data\n",
    "print(list(tess_edges.columns))\n",
    "tess_edges.reset_index(inplace=True)\n",
    "tess_edges = tess_edges[['u','v','key','geometry']]\n",
    "\n",
    "# Show\n",
    "print(tess_edges.crs)\n",
    "print(tess_edges.shape)\n",
    "tess_edges.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217caf31-4793-4565-bbea-60fcded07e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Time\n",
    "time_1 = time.time()\n",
    "##### Time\n",
    "\n",
    "# Transform Tessellations nodes and edges ID data to coordinates\n",
    "tess_coord_nodes, tess_coord_edges = src.network_entities(tess_nodes,\n",
    "                                                          tess_edges,\n",
    "                                                          projected_crs,\n",
    "                                                          expand_coords=(True,100))\n",
    "##### Time\n",
    "time_2 = time.time()\n",
    "print(f\"TIME: {time_2-time_1} seconds on transforming the tessellations network.\")\n",
    "##### Time\n",
    "\n",
    "# Nodes format\n",
    "if 'osmid' not in tess_coord_nodes.columns:\n",
    "    tess_coord_nodes.reset_index(inplace=True)\n",
    "tess_coord_nodes['osmid'] = tess_coord_nodes.osmid.astype(int)\n",
    "# Edges format\n",
    "if 'u' not in tess_coord_edges.columns:\n",
    "    tess_coord_edges.reset_index(inplace=True)\n",
    "\n",
    "tess_coord_edges['u'] = tess_coord_edges.u.astype(int)\n",
    "tess_coord_edges['v'] = tess_coord_edges.v.astype(int)\n",
    "# Set projected crs\n",
    "tess_coord_nodes = tess_coord_nodes.to_crs(projected_crs)\n",
    "tess_coord_edges = tess_coord_edges.to_crs(projected_crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ad098d-1413-4044-8739-a3afbcc16486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for duplicates that could pose problems\n",
    "print(\"--\"*30)\n",
    "print(\"--- TESSELLATIONS NETWORK DUPLICATED NODES (OSMIDS) CHECK:\")\n",
    "# Analyze\n",
    "duplicated_nodes_ids = len(tess_coord_nodes.loc[tess_coord_nodes.duplicated(subset=['osmid'])])\n",
    "if duplicated_nodes_ids>0:\n",
    "    raise DuplicatedOsmidsError(\n",
    "    \"\"\"Function src.network_entities() created two nodes with the same osmid.\n",
    "    If src.network_entities's argument \"expand_coords\" is set to False, try setting to (True,100).\n",
    "    The problem is probably due to two nodes being too close to each other. When assigning osmid, both get assigned the same osmid.\n",
    "    If src.network_entities's argument \"expand_coords\" was set to True, augment second value to increase coordinate diferenciation.\n",
    "    (Increases size of 'osmid', 'u' and 'v' values)\n",
    "    \"\"\")\n",
    "# Show\n",
    "print(tess_coord_nodes.shape)\n",
    "print(tess_coord_nodes.crs)\n",
    "print(tess_coord_nodes.dtypes)\n",
    "print(f\"DUPLICATED OSMIDS ON NODES: {duplicated_nodes_ids}.\")\n",
    "print(\"--\"*30)\n",
    "\n",
    "print(\"--- TESSELLATIONS NETWORK DUPLICATED EDGES (U, V, KEY) CHECK:\")\n",
    "duplicated_edges_ids = len(tess_coord_edges.loc[tess_coord_edges.duplicated(subset=['u','v','key'],keep=False)])\n",
    "if duplicated_edges_ids>0:\n",
    "    raise DuplicatedEdgeIdsError(\"\"\"\n",
    "    Function src.network_entities() includes function resolve_duplicates_indexes(), which should have solved this issue.\n",
    "    \"\"\")\n",
    "# Show\n",
    "print(tess_coord_edges.shape)\n",
    "print(tess_coord_edges.crs)\n",
    "print(tess_coord_edges.dtypes)\n",
    "print(f\"DUPLICATED EDGE_IDS ON EDGES: {duplicated_edges_ids}.\")\n",
    "print(\"--\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e3b887-9fa9-494e-9cd4-af65381ac75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tess_coord_nodes.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a366b1ef-cebc-4355-9599-e1ca6ddd2e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "tess_coord_edges.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa09cd0-b91e-40f9-859c-aa0b2806d3f9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### __Input data__ - Network revision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16efdb48-ed35-4e1c-9a20-4a105b268d69",
   "metadata": {},
   "source": [
    "This important step ensures that there are __no repeated osmids and__ that each edge has __unique 'u', 'v' and 'key' data__ on both networks (Not only on each network).\n",
    "\n",
    "Repeated IDs occur when a node from network 'a' is located at one meter or less of a node from network 'b'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28eaa73c-82d3-49d7-967e-830fc5bfeb9f",
   "metadata": {},
   "source": [
    "#### __Input data - Network revision -__ Looks for 'osmid' duplicates on nodes and fixes both nodes and edges if found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82b00d5-d3cd-4842-9816-ede2a8c4a9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "boeing_nodes = boeing_coord_nodes.copy()\n",
    "boeing_edges = boeing_coord_edges.copy()\n",
    "tess_nodes = tess_coord_nodes.copy()\n",
    "tess_edges = tess_coord_edges.copy()\n",
    "\n",
    "# Make sure all osmids are integers\n",
    "boeing_nodes['osmid'] = boeing_nodes['osmid'].astype('int')\n",
    "tess_nodes['osmid'] = tess_nodes['osmid'].astype('int')\n",
    "\n",
    "# Current existing osmids\n",
    "boeing_nodes_osmids = list(boeing_nodes.osmid.unique())\n",
    "tess_nodes_osmids = list(tess_nodes.osmid.unique())\n",
    "\n",
    "# Find duplicate cases\n",
    "slow_way = \"\"\"\n",
    "dup_osmids = []\n",
    "for osmid in tess_nodes_osmids:\n",
    "    if osmid in boeing_nodes_osmids:\n",
    "        dup_osmids.append(osmid)\n",
    "\"\"\"\n",
    "# Optimized way:\n",
    "dup_osmids = list(set(tess_nodes_osmids) & set(boeing_nodes_osmids))\n",
    "\n",
    "if len(dup_osmids) > 0:\n",
    "\n",
    "    print(f\"Producing unique osmids to solve {len(dup_osmids)} nodes that share osmid between boeing and tess networks.\")\n",
    "\n",
    "    # Used in function produce_osmid():\n",
    "    previously_produced = 0 # Must be located before starting iterating over osmids\n",
    "\n",
    "    # For each duplicated node found, modify the tess_nodes and tess_edges:\n",
    "    # (Could modify either network)\n",
    "    for current_osmid in dup_osmids:\n",
    "\n",
    "        # 1.0 --------------- Change the node's osmid\n",
    "        # Produce a unique osmid (That doesn't exist in either network)\n",
    "        produced_osmid = produce_osmid(tess_nodes, boeing_nodes, previously_produced)\n",
    "        # Replace osmid in current tess_node\n",
    "        osmid_idx = tess_nodes.osmid == current_osmid\n",
    "        tess_nodes.loc[osmid_idx,'osmid'] = produced_osmid\n",
    "        print(f\"Changed current existing osmid {current_osmid} for osmid {produced_osmid}.\")\n",
    "        # Save already produced osmid to avoid trying the same numbers again and again unnecessarily\n",
    "        previously_produced = produced_osmid\n",
    "\n",
    "        # 2.0 --------------- Change the 'u' or 'v' data of the edges that connect to that node\n",
    "        # Load the edges where that node is used\n",
    "        u_change_idx = (tess_edges.u==current_osmid)\n",
    "        tess_edges.loc[u_change_idx,'u'] = produced_osmid\n",
    "        print(f\"Updated 'u' on edges with osmid {produced_osmid}.\")\n",
    "\n",
    "        v_change_idx = (tess_edges.v==current_osmid)\n",
    "        tess_edges.loc[v_change_idx,'v'] = produced_osmid\n",
    "        print(f\"Updated 'v' on edges with osmid {produced_osmid}.\")\n",
    "\n",
    "else:\n",
    "    print(\"Found no duplicated osmids between boeing and tess networks\")\n",
    "\n",
    "# Final revision\n",
    "joined_nodes = pd.concat([boeing_nodes,tess_nodes])\n",
    "duplicated_nodes_ids = len(tess_coord_nodes.loc[tess_coord_nodes.duplicated(subset=['osmid'],keep=False)])\n",
    "print(f\"DUPLICATED NODES (Must be 0): {duplicated_nodes_ids}.\")\n",
    "\n",
    "joined_edges = pd.concat([boeing_edges,tess_edges])\n",
    "duplicated_edges_ids = len(boeing_coord_edges.loc[boeing_coord_edges.duplicated(subset=['u','v','key'],keep=False)])\n",
    "print(f\"DUPLICATED EDGES (Must be 0): {duplicated_edges_ids}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75eb03b1-6981-4ebb-990d-40e280032912",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### __Input data__ - Preprocessed input visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d69d822-6ca2-4f6b-bc3e-a1b99eb7c283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig,ax = plt.subplots(1,1,figsize=(10,20))\n",
    "\n",
    "boeing_nodes.plot(ax=ax,zorder=3,color='yellow',markersize=1)\n",
    "boeing_edges.plot(ax=ax,zorder=2,color='yellow',linewidth=1)\n",
    "tess_nodes.plot(ax=ax,zorder=1,color='firebrick',markersize=1)\n",
    "tess_edges.plot(ax=ax,zorder=0,color='firebrick',linewidth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51d42c3-7b62-4bb9-88b3-08669135e3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "if localsave_01_01:\n",
    "    # Current step output directory\n",
    "    output_fold = output_dir + 'part01_step01_preprocess/'\n",
    "    \n",
    "    # Create folder if it doesn't exist\n",
    "    if os.path.exists(output_fold) == False:\n",
    "        os.mkdir(output_fold)\n",
    "    \n",
    "    # Save result\n",
    "    boeing_nodes.to_file(output_fold + \"boeing_nodes.gpkg\")\n",
    "    boeing_edges.to_file(output_fold + \"boeing_edges.gpkg\")\n",
    "    tess_nodes.to_file(output_fold + \"tess_nodes.gpkg\")\n",
    "    tess_edges.to_file(output_fold + \"tess_edges.gpkg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e382d762-028d-4ada-9836-be4c41198d4c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## __Part 01 - Step 02__ - Identify the parts of the Tessellations network that should be added to the Boeing network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec811ced-8d9f-43a0-add6-9eb4e5a7d5da",
   "metadata": {},
   "source": [
    "This step identifies identifies the parts of the Tessellations network that should be added to the boeing network through the following steps:\n",
    "\n",
    "base_network: boeing\n",
    "complementary_network: tessellations\n",
    "\n",
    "1. Extract the midpoint of each complementary edge\n",
    "2. Create a contact-analysis buffer around mid_points\n",
    "3. Find mid_points whose buffer does not intersect with any part of the base network\n",
    "4. Select the complementary_nodes that connect to the uncovered edges found\n",
    "5. Find the nodes that would be used to connect the uncovered part of the complementary network to the base network\n",
    "6. Identify and shorten edges that extend into the base network and would be useful to create further connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ee7804-8917-417c-a09a-bbbdcc9af38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_datafor_part01_step02:\n",
    "    boeing_nodes = gpd.read_file(output_dir + \"part01_step01_preprocess/boeing_nodes.gpkg\")\n",
    "    boeing_edges = gpd.read_file(output_dir + \"part01_step01_preprocess/boeing_edges.gpkg\")\n",
    "    tess_nodes = gpd.read_file(output_dir + \"part01_step01_preprocess/tess_nodes.gpkg\")\n",
    "    tess_edges = gpd.read_file(output_dir + \"part01_step01_preprocess/tess_edges.gpkg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07e32ef-24b2-4a37-be67-8625041bead5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### __Part 01 - Step 02 -__ Identify the uncovered parts of the network and restore edges that were shortened from both sides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f39511d-553c-4186-a4c3-a34b9156df5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Time\n",
    "time_1 = time.time()\n",
    "##### Time\n",
    "\n",
    "# Run identify_uncovered function\n",
    "distance = 10\n",
    "complementary_uncovered_nodes, complementary_uncovered_edges, contact_nodes, dropped_overlapping_edges = identify_uncovered(base_nodes = boeing_nodes,\n",
    "                                                                                                                            base_edges = boeing_edges,\n",
    "                                                                                                                            complementary_nodes = tess_nodes,\n",
    "                                                                                                                            complementary_edges = tess_edges,\n",
    "                                                                                                                            contact_analysis_dist = distance,\n",
    "                                                                                                                            projected_crs = projected_crs)\n",
    "\n",
    "##### Time\n",
    "time_2 = time.time()\n",
    "print(f\"TIME: {time_2-time_1} seconds on identifying uncovered parts.\")\n",
    "##### Time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e87a78d-554b-4aa1-bdfc-a5caaf1f28b5",
   "metadata": {},
   "source": [
    "#### Restore edges that were shortened from both sides (u, v) to the original complete edge. (tmp fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da6f33d-e1b8-442d-a483-2418ba03083e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOUBLE EDGE SHORTENING CANCELATION (TEMPORAL FIX) ### ### ### ### ### ### ### ### ### ### ### ###\n",
    "# Edges that were shortened from both sides (u and v) are causing problems. \n",
    "# Due to the lack of time to adequately solve the problem, they are being dropped and replaced by the whole original edge.\n",
    "\n",
    "### LOCATE THE EDGES THAT WERE CLIPPED FROM BOTH SIDES\n",
    "# Drop NaNs in the output edges\n",
    "double_clipping = complementary_uncovered_edges.replace({'original_edge_id': ''}, np.nan).dropna(subset=['original_edge_id']).copy()\n",
    "# Look for duplicated original_edge_ids in the output edges\n",
    "double_clipping = double_clipping.loc[double_clipping.duplicated('original_edge_id',keep=False)]\n",
    "\n",
    "### FIND THE ORIGINAL EDGES FROM THE TESSELLATIONS NETWORK USING THE original_edge_id COLUMN\n",
    "# Select original_edge_ids of interest\n",
    "double_original_edge_ids = list(double_clipping.original_edge_id.unique())\n",
    "# Create a unique edge_id in the tessellation edges (tess_edges)\n",
    "tess_edges_tmp = src.create_unique_edge_id(tess_edges,order='uvkey')\n",
    "# Find the original edges\n",
    "double_original_edges = tess_edges_tmp.loc[tess_edges_tmp.edge_id.isin(double_original_edge_ids)].copy()\n",
    "\n",
    "### REPLACE THOSE DOUBLE CLIPPED EDGES WITH THE WHOLE EDGE AND DROP THE CREATED NODES RESULTING FROM THE CLIPPING\n",
    "## List of nodes to drop\n",
    "# u and v values in currently clipped edges\n",
    "unique_lst_clipped = list(set(double_clipping['u']).union(double_clipping['v']))\n",
    "# u and v values in original edges\n",
    "unique_lst_original = list(set(double_original_edges['u']).union(double_original_edges['v']))\n",
    "# u and v values in clipped edges that are NOT in original edges\n",
    "unique_in_clipped_not_original = list(set(unique_lst_clipped) - set(unique_lst_original))\n",
    "## Drop those nodes\n",
    "complementary_uncovered_nodes_clean = complementary_uncovered_nodes.loc[~complementary_uncovered_nodes.osmid.isin(unique_in_clipped_not_original)].copy()\n",
    "contact_nodes_clean = contact_nodes.loc[~contact_nodes.osmid.isin(unique_in_clipped_not_original)].copy()\n",
    "\n",
    "## Drop the double clipped edges and add the original edges\n",
    "complementary_uncovered_edges_clean = complementary_uncovered_edges.loc[~complementary_uncovered_edges.original_edge_id.isin(double_original_edge_ids)].copy()\n",
    "double_original_edges['clipping_i'] = 0\n",
    "complementary_uncovered_edges_clean = pd.concat([complementary_uncovered_edges_clean, double_original_edges])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c474389-8f2d-4ab5-adb2-72ede24fb4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore variable names for consistency with other steps and parts of the process\n",
    "complementary_uncovered_nodes = complementary_uncovered_nodes_clean.copy()\n",
    "complementary_uncovered_edges = complementary_uncovered_edges_clean.copy()\n",
    "contact_nodes = contact_nodes_clean.copy()\n",
    "\n",
    "del complementary_uncovered_nodes_clean\n",
    "del complementary_uncovered_edges_clean\n",
    "del contact_nodes_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5854627e-e97a-4a66-814d-8f7e11886e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if localsave_01_02:\n",
    "    # Current step output directory\n",
    "    output_fold = output_dir + 'part01_step02_identifyuncovered/'\n",
    "    \n",
    "    # Create folder if it doesn't exist\n",
    "    if os.path.exists(output_fold) == False:\n",
    "        os.mkdir(output_fold)\n",
    "    \n",
    "    # Save result\n",
    "    complementary_uncovered_nodes.to_file(output_fold + \"complementary_uncovered_nodes.gpkg\")\n",
    "    complementary_uncovered_edges.to_file(output_fold + \"complementary_uncovered_edges.gpkg\")\n",
    "    contact_nodes.to_file(output_fold + \"contact_nodes.gpkg\")\n",
    "    dropped_overlapping_edges.to_file(output_fold + \"dropped_overlapping_edges.gpkg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd116ec1-e4ac-4c4f-a63b-6c2ce231830f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## __Part 02 - Step 01 -__ Networks intersection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce20cbdc-4967-4bb2-9878-680091eb866c",
   "metadata": {},
   "source": [
    "This step __finds the intersection points existing__ between the base network and the uncovered parts of the complementary network, and then modifies both network in order to __incorporate the intersection points as new nodes__ on both networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a1a63f-713f-466e-9ab3-b89a306f8fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_datafor_part02_step01:\n",
    "    boeing_nodes = gpd.read_file(output_dir + \"part01_step01_preprocess/boeing_nodes.gpkg\")\n",
    "    boeing_edges = gpd.read_file(output_dir + \"part01_step01_preprocess/boeing_edges.gpkg\")\n",
    "    complementary_uncovered_nodes = gpd.read_file(output_dir + \"part01_step02_identifyuncovered/complementary_uncovered_nodes.gpkg\")\n",
    "    complementary_uncovered_edges = gpd.read_file(output_dir + \"part01_step02_identifyuncovered/complementary_uncovered_edges.gpkg\")\n",
    "    contact_nodes = gpd.read_file(output_dir + \"part01_step02_identifyuncovered/contact_nodes.gpkg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8294245-0e96-4de9-849d-d2837a7b590b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### __Part 02 - Step 01 -__ Find intersections between both networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794a937f-1e35-4299-b728-e15fba8f9df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Time\n",
    "time_1 = time.time()\n",
    "##### Time\n",
    "\n",
    "intersection_nodes = find_intersection_nodes(ntw_01_nodes = boeing_nodes,\n",
    "                                             ntw_01_edges = boeing_edges,\n",
    "                                             ntw_02_nodes = complementary_uncovered_nodes,\n",
    "                                             ntw_02_edges = complementary_uncovered_edges,\n",
    "                                             projected_crs = projected_crs)\n",
    "\n",
    "##### Time\n",
    "time_2 = time.time()\n",
    "print(f\"TIME: {time_2-time_1} seconds on finding all intersection nodes.\")\n",
    "##### Time\n",
    "\n",
    "# Show\n",
    "print(intersection_nodes.shape)\n",
    "intersection_nodes.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b30b8f-788e-45c0-8610-4f8d2ffdebc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MULTIPLE INTERSECTIONs (Intersections where three or more lines cross at the same point)\n",
    "mult_inters_nodes = intersection_nodes.loc[intersection_nodes.duplicated('osmid',keep=False)].copy()\n",
    "print(f\"Mult. int. osmids: {mult_inters_nodes.osmid.unique()}.\")\n",
    "\n",
    "# Save them (GIS Visualization)\n",
    "save_mult_intrs_nodes = False\n",
    "if save_mult_intrs_nodes:\n",
    "    mult_inters_nodes.to_file(output_dir + \"part02_step01_ntwsintersection/multiple_intersection_nodes.gpkg\")\n",
    "    \n",
    "# Show\n",
    "mult_inters_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae990df8-cd43-4d3a-936f-3a5325ee7e3a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### __Part 02 - Step 01 -__ Update the base network with the new intersections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e37261f-8f2a-42c6-9c7a-046e0e1b2fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate intersection_nodes in order to apply function network_intersections_update() for the base network\n",
    "intersection_nodes_base = intersection_nodes[['osmid','u_1','v_1','key_1','geometry']].copy()\n",
    "intersection_nodes_base.rename(columns={'u_1':'u',\n",
    "                                        'v_1':'v',\n",
    "                                        'key_1':'key'},inplace=True)\n",
    "# Specify which edge to keep after spliting each edge with the intersection_nodes_base\n",
    "# ('both' keeps both sides of split, 'u' or 'v' only keeps the side connected to that node.)\n",
    "# ('both' activates 'return_all' in function edge_clipping())\n",
    "intersection_nodes_base['retain_how'] = 'both'\n",
    "\n",
    "##### Time\n",
    "time_1 = time.time()\n",
    "##### Time\n",
    "# Apply network_intersections_update() function\n",
    "base_nodes_i, base_edges_i = network_intersections_update(current_ntw_nodes = boeing_nodes,\n",
    "                                                          current_ntw_edges = boeing_edges,\n",
    "                                                          intersection_nodes = intersection_nodes_base,\n",
    "                                                          projected_crs = projected_crs,\n",
    "                                                          intersection_logs = False,\n",
    "                                                          clipping_logs=False)\n",
    "##### Time\n",
    "time_2 = time.time()\n",
    "print(f\"TIME: {time_2-time_1} seconds on updating the network.\")\n",
    "##### Time\n",
    "\n",
    "# MULTIPLE INTERSECTION ADAPTATION: drop duplicates\n",
    "# Duplicated nodes and edges may occur when three or more lines cross at the same point.\n",
    "# Regular .drop_duplicates() cannot be used since edges could be mirrored.\n",
    "# Example:\n",
    "#        Line 1: u=1, v=2, key=0, geom=((1,1),(2,1))\n",
    "#        Line 2: u=2, v=1, key=0, geom=((2,1),(1,1))\n",
    "# Drop nodes and edges duplicates\n",
    "base_nodes_i,base_edges_i = drop_intersection_network_duplicates(base_nodes_i,base_edges_i)\n",
    "\n",
    "##### Time\n",
    "time_3 = time.time()\n",
    "print(f\"TIME: {time_3-time_2} seconds on dropping network duplicates.\")\n",
    "##### Time\n",
    "\n",
    "# Show\n",
    "print(base_nodes_i.shape)\n",
    "base_nodes_i.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f18e4f-6ed9-4dcd-94db-54d76f3629ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show\n",
    "print(base_edges_i.shape)\n",
    "base_edges_i.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d718b93d-7a0a-44d9-9660-cb3bb12dd4a8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### __Part 02 - Step 01 -__ Update the complementary network with the new intersections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8891ee48-4395-41e7-b68e-615b72ee4c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate intersection_nodes in order to apply function network_intersections_update() for the complementary network.\n",
    "intersection_nodes_comp = intersection_nodes[['osmid','u_2','v_2','key_2','geometry']].copy()\n",
    "intersection_nodes_comp.rename(columns={'u_2':'u',\n",
    "                                        'v_2':'v',\n",
    "                                        'key_2':'key'},inplace=True)\n",
    "\n",
    "# Specify which edge to keep after spliting each edge with the intersection_nodes_comp\n",
    "# ('both' keeps both sides of split, 'u' or 'v' only keeps the side connected to that node.)\n",
    "# ('both' activates 'return_all' in function edge_clipping())\n",
    "intersection_nodes_comp['retain_how'] = 'both'\n",
    "\n",
    "##### Time\n",
    "time_1 = time.time()\n",
    "##### Time\n",
    "\n",
    "# Apply network_intersections_update() function\n",
    "comp_nodes_i, comp_edges_i = network_intersections_update(current_ntw_nodes = complementary_uncovered_nodes,\n",
    "                                                          current_ntw_edges = complementary_uncovered_edges,\n",
    "                                                          intersection_nodes = intersection_nodes_comp,\n",
    "                                                          projected_crs = projected_crs,\n",
    "                                                          intersection_logs = False,\n",
    "                                                          clipping_logs = False)\n",
    "\n",
    "##### Time\n",
    "time_2 = time.time()\n",
    "print(f\"TIME: {time_2-time_1} seconds on updating the network.\")\n",
    "##### Time\n",
    "\n",
    "# MULTIPLE INTERSECTION ADAPTATION: drop duplicates\n",
    "# Duplicated nodes and edges may occur when three or more lines cross at the same point.\n",
    "# Regular .drop_duplicates() cannot be used since edges could be mirrored.\n",
    "# Example:\n",
    "#        Line 1: u=1, v=2, key=0, geom=((1,1),(2,1))\n",
    "#        Line 2: u=2, v=1, key=0, geom=((2,1),(1,1))\n",
    "# Drop nodes and edges duplicates\n",
    "comp_nodes_i,comp_edges_i = drop_intersection_network_duplicates(comp_nodes_i,comp_edges_i)\n",
    "\n",
    "##### Time\n",
    "time_3 = time.time()\n",
    "print(f\"TIME: {time_3-time_2} seconds on dropping network duplicates.\")\n",
    "##### Time\n",
    "\n",
    "# Show\n",
    "print(comp_nodes_i.shape)\n",
    "comp_nodes_i.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff14b948-adf9-4ee9-8210-79c28aed6304",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(comp_nodes_i.shape)\n",
    "comp_nodes_i.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3fbe7f-4efa-4949-bdf8-86637ecc82dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(comp_edges_i.shape)\n",
    "comp_edges_i.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73ebc7f-7c00-4b19-990a-4278d024bd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "if localsave_02_01:\n",
    "    # Current step output directory\n",
    "    output_fold = output_dir + 'part02_step01_ntwsintersection/'\n",
    "    \n",
    "    # Create folder if it doesn't exist\n",
    "    if os.path.exists(output_fold) == False:\n",
    "        os.mkdir(output_fold)\n",
    "    \n",
    "    # Save result\n",
    "    intersection_nodes.to_file(output_fold + \"networks_intersections.gpkg\") \n",
    "    base_nodes_i.to_file(output_fold + \"intersected_base_nodes.gpkg\")\n",
    "    base_edges_i.to_file(output_fold + \"intersected_base_edges.gpkg\")\n",
    "    comp_nodes_i.to_file(output_fold + \"intersected_comp_nodes.gpkg\")\n",
    "    comp_edges_i.to_file(output_fold + \"intersected_comp_edges.gpkg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed726c72-cdd3-46ad-9c23-4c284e3d7fff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## __Part 02 - Step 02 -__ Connection identification between networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8e1001-5939-4c08-8202-439f1b84d538",
   "metadata": {},
   "source": [
    "Through identifying the __nearest base_nodes_i and nearest base_edges_i to each contact_node__, this section creates a gdf that __stablishes the relationships__ between each contact_node (That all derive complementary network) and the node or edge on the boeing network were the connection will be later created.\n",
    "\n",
    "If the connection is to be made __from contact_node to base_edge__, it identifies the best intersection point and saves the point with a new osmid in order to split the boeing_edge and __create a new base node__ there in an update network process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415b65ab-d8b2-4720-a1f1-8ecb93a4321b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_datafor_part02_step02:\n",
    "    # From Part01 - Step02\n",
    "    contact_nodes = gpd.read_file(output_dir + \"part01_step02_identifyuncovered/contact_nodes.gpkg\")\n",
    "    # From Part02 - Step01\n",
    "    base_nodes_i = gpd.read_file(output_dir + \"part02_step01_ntwsintersection/intersected_base_nodes.gpkg\")\n",
    "    base_edges_i = gpd.read_file(output_dir + \"part02_step01_ntwsintersection/intersected_base_edges.gpkg\")\n",
    "    comp_nodes_i = gpd.read_file(output_dir + \"part02_step01_ntwsintersection/intersected_comp_nodes.gpkg\")\n",
    "    comp_edges_i = gpd.read_file(output_dir + \"part02_step01_ntwsintersection/intersected_comp_edges.gpkg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccb9e4d-ab0d-4844-9dc4-cc8712f59db3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### __Part 02 - Step 02 -__ Find nearest base_nodes_i and nearest base_edges_i to each contact_node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd051128-7a58-479e-bbaa-a7318b7ca67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve contact_nodes\n",
    "print(contact_nodes.shape)\n",
    "contact_nodes.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96976026-bd8e-478f-aed9-118f2a30ed76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find nearest base_node to each contact_node\n",
    "nearest_nodes = gpd.sjoin_nearest(contact_nodes, base_nodes_i, distance_col='node_distance')\n",
    "# Merge nearest data\n",
    "contact_nodes_1 = contact_nodes.merge(nearest_nodes[['osmid_right','node_distance']],left_index=True,right_index=True,how='outer')\n",
    "# Save disk space\n",
    "del nearest_nodes\n",
    "\n",
    "# Show\n",
    "print(contact_nodes_1.dtypes)\n",
    "print(contact_nodes_1.shape)\n",
    "contact_nodes_1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7acc450-5468-4222-bfea-2315cb2c96e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find nearest base_edge to each contact_node\n",
    "nearest_edges = gpd.sjoin_nearest(contact_nodes, base_edges_i, distance_col='edge_distance')\n",
    "# It is unlikely that two edges are exacly at the same distance.\n",
    "# If this happens it most likely means that are at the same distance because they meet at a node\n",
    "# and the closest distance is exactly that point.\n",
    "# --> Keep the first occurance\n",
    "nearest_edges = nearest_edges.drop_duplicates(subset='osmid')\n",
    "\n",
    "# Merge nearest data\n",
    "contact_nodes_2 = contact_nodes_1.merge(nearest_edges[['u','v','key','edge_distance']],left_index=True,right_index=True,how='outer')\n",
    "# Save disk space\n",
    "del nearest_edges\n",
    "\n",
    "# Show\n",
    "print(contact_nodes_2.dtypes)\n",
    "print(contact_nodes_2.shape)\n",
    "contact_nodes_2.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c89ff71-874e-42d2-8954-418efd79ba27",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### __Part 02 - Step 02 -__ Define the best relation between networks for each contact_node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fa7a51-6d32-4dbc-9046-210b4c62583f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find individual relations between contact_nodes_2 (renamed as connecting_nodes) and network 01 (base network).\n",
    "# (Produces \"connected_nodes\" gdf, where these relations are saved, and also\n",
    "# produces \"intersection_nodes_2\" gdf, which stores new nodes and edges to be split on network 01)\n",
    "\n",
    "##### Time\n",
    "time_1 = time.time()\n",
    "##### Time\n",
    "\n",
    "extended_logs = False # Used during Dev. Prints logs for each node.\n",
    "\n",
    "# Rename gdfs to keep original (Used during Dev)\n",
    "ntw_01_nodes = base_nodes_i.copy()\n",
    "ntw_01_edges = base_edges_i.copy()\n",
    "ntw_02_nodes = comp_nodes_i.copy()\n",
    "ntw_02_edges = comp_edges_i.copy()\n",
    "connecting_nodes = contact_nodes_2.copy()\n",
    "\n",
    "# Reset produced osmids (Used in function produce_osmid())\n",
    "previously_produced = 0\n",
    "\n",
    "# List of nodes created by intersecting both networks\n",
    "# (They are used to discard the creation of new intersections whenever \n",
    "#  nerby (20m) there's already an existing intersection between both networks)\n",
    "intersecting_idx = ntw_01_nodes.intersecting==1\n",
    "intersecting_osmids = list(ntw_01_nodes.loc[intersecting_idx].osmid.unique())\n",
    "\n",
    "# Create connected_nodes, a DataFrame that will store the final defined connections between networks\n",
    "# ----- OUTPUT DTYPES MANAGEMENT FOR connected_nodes DATAFRAME -----\n",
    "# (e.g. node x from network 02 will be connecting to node y from network 01)\n",
    "connected_nodes_cols = {\"connecting_ntw02_osmid\": \"int64\",\n",
    "                        \"connection_type\": \"string\",\n",
    "                        \"connection_ntw01_osmid\":\"int64\"\n",
    "                       }\n",
    "connected_nodes = pd.DataFrame(columns=connected_nodes_cols.keys()).astype(connected_nodes_cols)\n",
    "# ----- OUTPUT DTYPES MANAGEMENT FOR connected_nodes DATAFRAME -----\n",
    "i = 0 #Row count of the created df\n",
    "\n",
    "# Create intersection_nodes_2, a GeoDataFrame that will be used in function network_intersections_update()\n",
    "# to perform a second round of intersections on network_01 (after this process)\n",
    "# ----- OUTPUT DTYPES MANAGEMENT FOR intersection_nodes_2 GEODATAFRAME -----\n",
    "intersection_nodes_2_cols = {\"osmid\": \"int64\", # Node that is the intersection (clipping point)\n",
    "                             \"u\": \"int64\", # Node that is the intersection (clipping point)\n",
    "                             \"v\": \"int64\", # Edge that is intersected\n",
    "                             \"key\": \"int64\", # Edge that is intersected\n",
    "                             \"edge_id\": \"string\" # Edge id that is intersected (for 2.1 -- New nodes check)\n",
    "                            }\n",
    "intersection_nodes_2 = gpd.GeoDataFrame(columns=list(intersection_nodes_2_cols.keys()) + [\"geometry\"], crs=projected_crs).astype({**intersection_nodes_2_cols, \"geometry\": \"geometry\"})\n",
    "# Reorder columns\n",
    "intersection_nodes_2 = intersection_nodes_2[[\"osmid\", \"geometry\", # Node to be created to serve as a connection point in ntw_01\n",
    "                                             \"u\", \"v\", \"key\", \"edge_id\"]] # Edge from ntw_01 to be split by the created node\n",
    "# ----- OUTPUT DTYPES MANAGEMENT FOR intersection_nodes_2 GEODATAFRAME -----\n",
    "new_node_idx = 0 #Row count of the created gdf\n",
    "\n",
    "# Iterate over each connecting_node (contact_nodes from identify_uncovered, with nearest_node from ntw_01 and nearest_edge from ntw_01 data)\n",
    "for idx, node in connecting_nodes.iterrows():\n",
    "\n",
    "    # Extract current connecting_node's data\n",
    "    # Current connecting_node data [From ntw_02]\n",
    "    cn_osmid = int(node.osmid)\n",
    "    cn_geometry = node.geometry\n",
    "    # nearest_node data [From ntw_01]\n",
    "    cn_nearest_osmid = int(node.osmid_right)\n",
    "    cn_node_distance = node.node_distance \n",
    "    # nearest_edge data [From ntw_01]\n",
    "    cn_nearest_u = int(node.u)\n",
    "    cn_nearest_v = int(node.v)\n",
    "    cn_nearest_key = int(node.key)\n",
    "    cn_edge_distance = node.edge_distance\n",
    "\n",
    "    # Development checks -----------------------------------\n",
    "    #osmid_checks = [89]\n",
    "    #if cn_osmid not in osmid_checks:\n",
    "    #    extended_logs=False\n",
    "    #    previous_cn_osmid = cn_osmid\n",
    "    #else:\n",
    "    #    print(f\"Previous cn_osmid: {previous_cn_osmid}.\")\n",
    "    #    print(f\"Current cn_osmid: {cn_osmid}.\")\n",
    "    #    extended_logs=True\n",
    "    # Development checks -----------------------------------\n",
    "    \n",
    "    # 1.0 --------------- CASE 1: Close intersection, discards connecting_node\n",
    "    # ------------------- This case analyses the edges that connect to the current connecting_node.\n",
    "    # ------------------- If there's already an intersection to the base network in 20 meters or less,\n",
    "    # ------------------- assume this intersection could sustitute the connection that was going to be created.\n",
    "    # ------------------- (Meaning, there's already a connection nearby, creating another would be redundant)\n",
    "    # ------------------- --> ignore connecting_node (continue).\n",
    "\n",
    "    # Identify the edges that connect to the current connecting_node\n",
    "    connecting_node_edges = ntw_02_edges.loc[(ntw_02_edges.u==cn_osmid) | (ntw_02_edges.v==cn_osmid)].copy()\n",
    "    # Extract the list of nodes ('u' and 'v') that those edges connect to\n",
    "    connecting_node_osmids = set(list(connecting_node_edges.u.unique()) + list(connecting_node_edges.v.unique()))\n",
    "    connecting_node_osmids.remove(cn_osmid) #Remove itself\n",
    "    # From those connecting_node_osmids, extract those that are also intersecting_osmid (That intersect the network 01)    \n",
    "    opposite_intersecting_osmid = [osmid for osmid in connecting_node_osmids if osmid in intersecting_osmids]\n",
    "    # If they exist:\n",
    "    if len(opposite_intersecting_osmid)>0:\n",
    "        # Set connection to False until distance is verified\n",
    "        existing_connection = False\n",
    "        # Analyse the length of the edges connecting to the opposite_intersecting_osmids. \n",
    "        # For any edge connecting to an intersecting_node, if the length of that edge is less than 20 meters, \n",
    "        # register that there's already an existing connection nerby.\n",
    "        edges_to_analyse = connecting_node_edges.loc[(connecting_node_edges.u.isin(opposite_intersecting_osmid)) | \n",
    "                                                     (connecting_node_edges.v.isin(opposite_intersecting_osmid))].copy()\n",
    "        edges_to_analyse['length'] = edges_to_analyse.length\n",
    "        for edge_length in list(edges_to_analyse.length.unique()):\n",
    "            if edge_length < 20:\n",
    "                existing_connection = True\n",
    "                break\n",
    "        # If the process found an existing connection, skip current connecting_node.\n",
    "        # Else, goes to CASE 2.\n",
    "        if existing_connection == True:\n",
    "            # Connection registration\n",
    "            # ----- OUTPUT REGISTRATION FOR connected_nodes DATAFRAME -----\n",
    "            # Create temporary df with the current case's data\n",
    "            df_temporal = pd.DataFrame({'connecting_ntw02_osmid': [int(cn_osmid)],\n",
    "                                        'connection_type': ['existing'],\n",
    "                                        'connection_ntw01_osmid': [int(0)], #Will not be connecting to network 01, just fills with 0\n",
    "                                       })\n",
    "            # Force all datatypes to match the datatypes of the df to where the data will be merged\n",
    "            dtypes_dict = connected_nodes.dtypes.to_dict() #Dict with TARGET dtypes\n",
    "            filtered_dtypes = {col: dtypes_dict[col] for col in df_temporal.columns if col in dtypes_dict} # Filters for cols in case of mismatch\n",
    "            df_temporal = df_temporal.astype(filtered_dtypes) # Assigns those types\n",
    "            # Concatenate to TARGET without altering original dtypes\n",
    "            connected_nodes = pd.concat([connected_nodes, df_temporal], ignore_index=True)\n",
    "            # ----- OUTPUT REGISTRATION FOR connected_nodes DATAFRAME -----\n",
    "            i+=1\n",
    "            if extended_logs:\n",
    "                print(f\"CASE 1: Connecting node {cn_osmid} already connects to an existing intersection in less than 20 meters.\")\n",
    "            continue # Next connecting_node\n",
    "    \n",
    "    # 2.0 --------------- CASE 2: Distance analysis\n",
    "    # ------------------- If there's no existing intersection nerby, register connection to be created.\n",
    "    \n",
    "    # 2.1 --------------- New nodes check (If complies with a set of conditions, updates cn_nearest_osmid and cn_node_distance)\n",
    "    # ------------------- The for loop iterating over connecting_nodes has a code section where new nodes can be created.\n",
    "    # ------------------- This happens when a connection between a conecting_node and a ntw_01_edge is stablished.\n",
    "    # ------------------- New nodes can also serve as a connection with between networks\n",
    "    # ------------------- Considering them prevents multiple connections being created when an existing can be used.\n",
    "    \n",
    "    # Extract nearest edge's unique_id [From ntw_01]\n",
    "    cn_edge_id = str(cn_nearest_u)+str(cn_nearest_v)+str(cn_nearest_key)\n",
    "    # If there are already new_nodes registered, analyse distance to them. Else, skip.\n",
    "    if new_node_idx > 0:\n",
    "        # Check if this edge has already been used to stablish a connection with another connecting_node.\n",
    "        # If so, a new_node will be created over this edge and that new_node could be used as a connection instead of creating another one.\n",
    "        # So, if this edge is already registered in intersection_nodes_2, consider the new_node.\n",
    "        intersected_edges = list(intersection_nodes_2['edge_id'].unique())\n",
    "        if cn_edge_id in intersected_edges:\n",
    "            # Obtain current conecting_node's coordinates [From ntw_02]\n",
    "            conecting_nodes_coords = cn_geometry.coords[0]\n",
    "            # Retrieve new_nodes (registered_nodes)\n",
    "            registered_nodes = intersection_nodes_2.loc[intersection_nodes_2['edge_id'] == cn_edge_id].copy()\n",
    "            registered_nodes.reset_index(inplace=True,drop=True)\n",
    "            for node_idx in range(len(registered_nodes)):\n",
    "                # Obtain registered node's coords [Of any new_node]\n",
    "                registered_node = registered_nodes.iloc[node_idx]\n",
    "                registered_node_coords = registered_node.geometry.coords[0]\n",
    "                # Calculate distance between both nodes\n",
    "                new_node_distance = distance_between_points(conecting_nodes_coords, registered_node_coords)\n",
    "                # If new_node_distance is less than distance to current nearest node, update nearest node data\n",
    "                if new_node_distance < cn_node_distance:\n",
    "                    cn_nearest_osmid = registered_node.osmid\n",
    "                    cn_node_distance = new_node_distance\n",
    "    \n",
    "    # 2.2 --------------- CASE 2a: Close node connection\n",
    "    # If distance to nearest node is less or equal to 5 meters, register that connection (Ignores closest edge, takes closest node)\n",
    "    if cn_node_distance<=5:\n",
    "        # Connection registration\n",
    "        # ----- OUTPUT REGISTRATION FOR connected_nodes DATAFRAME -----\n",
    "        # Create temporary df with the current case's data\n",
    "        df_temporal = pd.DataFrame({'connecting_ntw02_osmid': [int(cn_osmid)],\n",
    "                                    'connection_type': ['node'],\n",
    "                                    'connection_ntw01_osmid': [int(cn_nearest_osmid)],\n",
    "                                   })\n",
    "        # Force all datatypes to match the datatypes of the df to where the data will be merged\n",
    "        dtypes_dict = connected_nodes.dtypes.to_dict() #Dict with TARGET dtypes\n",
    "        filtered_dtypes = {col: dtypes_dict[col] for col in df_temporal.columns if col in dtypes_dict} # Filters for cols in case of mismatch\n",
    "        df_temporal = df_temporal.astype(filtered_dtypes) # Assigns those types\n",
    "        # Concatenate to TARGET without altering original dtypes\n",
    "        connected_nodes = pd.concat([connected_nodes, df_temporal], ignore_index=True)\n",
    "        # ----- OUTPUT REGISTRATION FOR connected_nodes DATAFRAME -----\n",
    "        i+=1\n",
    "        if extended_logs:\n",
    "            print(f\"CASE 2a: Connecting node {cn_osmid} connects to a node in less than 5 meters.\")\n",
    "        continue # Next connecting_node\n",
    "\n",
    "    # If distance to nearest node is greater than 5 meters, analyse distance to edge\n",
    "    elif cn_node_distance>5:\n",
    "        \n",
    "        # 2.3 --------------- CASE 2b: Edge connection\n",
    "        # If the nearest_edge is closer than the (nearest_node-5 meters)\n",
    "        # (Meaning, if the nearest_edge is closer even when we give 5 meter preference to the node over the edge) \n",
    "        # --> Create and register join with nearest_edge (nearest_edge gets split to create a node for the connection between networks)\n",
    "        node_distance_pref = cn_node_distance-5\n",
    "        if cn_edge_distance < node_distance_pref:\n",
    "\n",
    "            # 2.3.1 ------------- Create new node's data\n",
    "            # Extract the nearest edge's geometry (LineString)\n",
    "            nearest_edge = ntw_01_edges.loc[(ntw_01_edges.u==cn_nearest_u) & \n",
    "                                            (ntw_01_edges.v==cn_nearest_v) &\n",
    "                                            (ntw_01_edges.key==cn_nearest_key)].copy()\n",
    "            nearest_edge.reset_index(inplace=True,drop=True)\n",
    "            edge_geom = nearest_edge['geometry'].unique()[0]\n",
    "            # Project current connecting_node into the edge's geometry\n",
    "            projected_point = edge_geom.interpolate(edge_geom.project(cn_geometry))\n",
    "            # Produce unique osmid\n",
    "            produced_osmid = produce_osmid(ntw_01_nodes, ntw_02_nodes, previously_produced)\n",
    "            # Since ntw_01_nodes and ntw_02_nodes are not being updated, next time (whenever another node falls in this case)\n",
    "            # will try function produce_osmid() with next possible osmid.\n",
    "            previously_produced = produced_osmid+1\n",
    "\n",
    "            # 2.3.2 ------------- Register new_node's data and edge to be split by it.\n",
    "            # ------------------- Will be used in function network_intersections_update()\n",
    "            \n",
    "            # ----- OUTPUT REGISTRATION FOR intersection_nodes_2 GEODATAFRAME -----\n",
    "            # Create temporary df with the current case's data\n",
    "            df_temporal = pd.DataFrame({'osmid': [int(produced_osmid)], # Node that is the intersection (clipping point)\n",
    "                                        'geometry': [projected_point], # Node that is the intersection (clipping point)\n",
    "                                        'u': [int(cn_nearest_u)], # Edge that is intersected\n",
    "                                        'v':[int(cn_nearest_v)], # Edge that is intersected\n",
    "                                        'key':[int(cn_nearest_key)], # Edge that is intersected\n",
    "                                        'edge_id':[cn_edge_id] # Edge id that is intersected (for 2.1 -- New nodes check)\n",
    "                                       }\n",
    "                                      )\n",
    "            if int(produced_osmid) == 17554:\n",
    "                print(f\"connecting_ntw02_osmid: {int(cn_osmid)}.\")\n",
    "                print(f\"connection_ntw01_osmid: {int(produced_osmid)}.\")\n",
    "            \n",
    "            gdf_temporal = gpd.GeoDataFrame(df_temporal, geometry='geometry', crs=projected_crs)\n",
    "            # Force all datatypes to match the datatypes of the gdf to where the data will be merged\n",
    "            dtypes_dict = connected_nodes.dtypes.to_dict() #Dict with TARGET dtypes\n",
    "            filtered_dtypes = {col: dtypes_dict[col] for col in df_temporal.columns if col in dtypes_dict} # Filters for cols in case of mismatch\n",
    "            gdf_temporal = gdf_temporal.astype(filtered_dtypes) # Assigns those types\n",
    "            # Concatenate to TARGET without altering original dtypes\n",
    "            intersection_nodes_2 = pd.concat([intersection_nodes_2, gdf_temporal], ignore_index=True)\n",
    "            # ----- OUTPUT REGISTRATION FOR intersection_nodes_2 GEODATAFRAME -----\n",
    "            new_node_idx+=1\n",
    "\n",
    "            # 2.3.3 ------------- Connection registration\n",
    "            # ----- OUTPUT REGISTRATION FOR connected_nodes DATAFRAME -----\n",
    "            # Create temporary df with the current case's data\n",
    "            df_temporal = pd.DataFrame({'connecting_ntw02_osmid': [int(cn_osmid)],\n",
    "                                        'connection_type': ['node'],\n",
    "                                        'connection_ntw01_osmid': [int(produced_osmid)],\n",
    "                                       })\n",
    "            # Force all datatypes to match the datatypes of the df to where the data will be merged\n",
    "            dtypes_dict = connected_nodes.dtypes.to_dict() #Dict with TARGET dtypes\n",
    "            filtered_dtypes = {col: dtypes_dict[col] for col in df_temporal.columns if col in dtypes_dict} # Filters for cols in case of mismatch\n",
    "            df_temporal = df_temporal.astype(filtered_dtypes) # Assigns those types\n",
    "            # Concatenate to TARGET without altering original dtypes\n",
    "            connected_nodes = pd.concat([connected_nodes, df_temporal], ignore_index=True)\n",
    "            # ----- OUTPUT REGISTRATION FOR connected_nodes DATAFRAME -----\n",
    "            i+=1\n",
    "            if extended_logs:\n",
    "                print(f\"CASE 2b: Connecting node {cn_osmid} produced a new node in an edge of the network 01.\")\n",
    "            continue # Next connecting_node\n",
    "\n",
    "        # 2.4 --------------- CASE 2c: Node connection\n",
    "        # If the (nearest_node-5 meters) is closer than the nearest_edge\n",
    "        # --> Register node connection\n",
    "        else:\n",
    "            # Connection registration\n",
    "            # ----- OUTPUT REGISTRATION FOR connected_nodes DATAFRAME -----\n",
    "            # Create temporary df with the current case's data\n",
    "            df_temporal = pd.DataFrame({'connecting_ntw02_osmid': [int(cn_osmid)],\n",
    "                                        'connection_type': ['node'],\n",
    "                                        'connection_ntw01_osmid': [int(cn_nearest_osmid)],\n",
    "                                       })\n",
    "            # Force all datatypes to match the datatypes of the df to where the data will be merged\n",
    "            dtypes_dict = connected_nodes.dtypes.to_dict() #Dict with TARGET dtypes\n",
    "            filtered_dtypes = {col: dtypes_dict[col] for col in df_temporal.columns if col in dtypes_dict} # Filters for cols in case of mismatch\n",
    "            df_temporal = df_temporal.astype(filtered_dtypes) # Assigns those types\n",
    "            # Concatenate to TARGET without altering original dtypes\n",
    "            connected_nodes = pd.concat([connected_nodes, df_temporal], ignore_index=True)\n",
    "            # ----- OUTPUT REGISTRATION FOR connected_nodes DATAFRAME -----\n",
    "            i+=1\n",
    "            if extended_logs:\n",
    "                print(f\"CASE 2c: Connecting node {cn_osmid} connects to a node in less than 5 meters.\")\n",
    "            continue # Next connecting_node\n",
    "\n",
    "# Final format (Set unique identifiers to int) to connected_nodes's columns\n",
    "no_longer_needed = \"\"\" [SOLVED USING OUTPUT DTYPES MANAGEMENT AND REGISTRATION]\n",
    "connected_nodes['connecting_ntw02_osmid'] = connected_nodes['connecting_ntw02_osmid'].astype('int')\n",
    "connected_nodes['connection_ntw01_osmid'].fillna(value=0, inplace=True)# Existing connections do not register osmid, set to 0\n",
    "connected_nodes['connection_ntw01_osmid'] = connected_nodes['connection_ntw01_osmid'].astype('int')\n",
    "# Final format (Set unique identifiers to int) to intersection_nodes_2's columns\n",
    "intersection_nodes_2['osmid'] = intersection_nodes_2['osmid'].astype('int')\n",
    "intersection_nodes_2['u'] = intersection_nodes_2['u'].astype('int')\n",
    "intersection_nodes_2['v'] = intersection_nodes_2['v'].astype('int')\n",
    "intersection_nodes_2['key'] = intersection_nodes_2['key'].astype('int')\n",
    "\"\"\"\n",
    "\n",
    "##### Time\n",
    "time_2 = time.time()\n",
    "print(f\"TIME: {time_2-time_1} seconds on defining the best relations.\")\n",
    "##### Time\n",
    "\n",
    "# Show\n",
    "print(connected_nodes.dtypes)\n",
    "print(connected_nodes.shape)\n",
    "connected_nodes.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda8beea-f284-4128-a897-cf7f4f0077a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show gdf that will be used to update network 01 (base_network)\n",
    "print(intersection_nodes_2.dtypes)\n",
    "print(intersection_nodes_2.shape)\n",
    "\n",
    "print(\"--\"*30)\n",
    "print(\"These intersections (A specific osmid crossing at a specific edge_id) should not have duplicates.\")\n",
    "print(f\"INTERSECTIONS DUPLICATES: {len(intersection_nodes_2.loc[intersection_nodes_2.duplicated(subset=['osmid','edge_id'])])}.\")\n",
    "print(\"--\"*30)\n",
    "intersection_nodes_2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160e7656-6921-4a81-9417-8616bbedfa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "if localsave_02_02:\n",
    "    # Current step output directory\n",
    "    output_fold = output_dir + 'part02_step02_connectidentif/'\n",
    "    \n",
    "    # Create folder if it doesn't exist\n",
    "    if os.path.exists(output_fold) == False:\n",
    "        os.mkdir(output_fold)\n",
    "    \n",
    "    # Save result\n",
    "    connected_nodes.to_csv(output_fold + \"connected_nodes_df.csv\",index=False)\n",
    "    intersection_nodes_2.to_file(output_fold + \"intersection_nodes_2.gpkg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c91f9fe-8b78-4705-ba15-06a80a3048b6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## __Part 02 - Step 03 -__ Network concatenation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f632a642-f59b-4165-93b9-6351d47abc1b",
   "metadata": {},
   "source": [
    "This step __updates the base network__ (In order to include the new nodes created in order to join a contact_node and a base_edge) and __joins (concatenates) both network's nodes and edges__ into a single gdf each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45466b94-5ef5-4503-808f-ab78231bb8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_datafor_part02_step03:\n",
    "    # From Part02 - Step01\n",
    "    base_nodes_i = gpd.read_file(output_dir + \"part02_step01_ntwsintersection/intersected_base_nodes.gpkg\")\n",
    "    base_edges_i = gpd.read_file(output_dir + \"part02_step01_ntwsintersection/intersected_base_edges.gpkg\")\n",
    "    comp_nodes_i = gpd.read_file(output_dir + \"part02_step01_ntwsintersection/intersected_comp_nodes.gpkg\")\n",
    "    comp_edges_i = gpd.read_file(output_dir + \"part02_step01_ntwsintersection/intersected_comp_edges.gpkg\")\n",
    "    # From Part02 - Step02\n",
    "    intersection_nodes_2 = gpd.read_file(output_dir + \"part02_step02_connectidentif/intersection_nodes_2.gpkg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c78a36-02e0-47ec-9049-59f9d0eca540",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### __Part 02 - Step 03 -__ Re-update the network 01 (Base network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78554f6c-b8a8-46eb-9702-dea78be558bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply network_intersections_update() function\n",
    "\n",
    "##### Time\n",
    "time_1 = time.time()\n",
    "##### Time\n",
    "\n",
    "# Specify which edge to keep after spliting each edge with the intersection_nodes_2\n",
    "# ('both' keeps both sides of split, 'u' or 'v' only keeps the side connected to that node.)\n",
    "# ('both' activates 'return_all' in function edge_clipping())\n",
    "intersection_nodes_2['retain_how'] = 'both'\n",
    "\n",
    "# (Second round of intersections, derived from new nodes created from network_02 to edges on network_01)\n",
    "base_nodes_i2, base_edges_i2 = network_intersections_update(current_ntw_nodes = base_nodes_i,\n",
    "                                                            current_ntw_edges = base_edges_i,\n",
    "                                                            intersection_nodes = intersection_nodes_2,\n",
    "                                                            projected_crs = projected_crs,\n",
    "                                                            intersection_logs = False,\n",
    "                                                            clipping_logs=False)\n",
    "\n",
    "# Nodes marked as \"intersecting\" on this round are nodes created specifically in order to connect the\n",
    "# network_02 to an edge on network_01. Distinguish them (For GIS visualization)\n",
    "base_nodes_i2.rename(columns={'intersecting':'ntw_join'},inplace=True)\n",
    "\n",
    "##### Time\n",
    "time_2 = time.time()\n",
    "print(f\"TIME: {time_2-time_1} seconds on re-updating the base network.\")\n",
    "##### Time\n",
    "\n",
    "# Show\n",
    "print(base_nodes_i2.dtypes)\n",
    "print(base_nodes_i2.shape)\n",
    "base_nodes_i2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e31817-7ebb-472c-a39c-dbf31ac67c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show base_nodes tail detail\n",
    "print(\"--\"*30)\n",
    "print(f\"NODES DUPLICATES: {len(base_nodes_i2.loc[base_nodes_i2.duplicated(subset=['osmid'])])}.\")\n",
    "print(\"--\"*30)\n",
    "base_nodes_i2.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3832e0-42b5-4625-8e24-687fa3610a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show base_edges\n",
    "print(base_edges_i2.dtypes)\n",
    "base_edges_i2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03dc260-f077-4cf6-82d5-5e9413a49612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show base_edges tail detail\n",
    "test_base_edges_i2 = base_edges_i2.copy()\n",
    "test_base_edges_i2 = src.create_unique_edge_id(test_base_edges_i2)\n",
    "print(\"--\"*30)\n",
    "print(f\"EDGES DUPLICATES: {len(test_base_edges_i2.loc[test_base_edges_i2.duplicated(subset=['edge_id'])])}.\")\n",
    "print(\"--\"*30)\n",
    "del test_base_edges_i2\n",
    "base_edges_i2.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a1096a-301b-475f-b140-929d8f0e30e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show complementary_nodes\n",
    "print(comp_nodes_i.dtypes)\n",
    "print(comp_nodes_i.shape)\n",
    "comp_nodes_i.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1285af67-eed3-4fa5-88f0-3e45935a5341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show complementary_edges\n",
    "print(comp_edges_i.dtypes)\n",
    "print(comp_edges_i.shape)\n",
    "comp_edges_i.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac9f990-4b00-41b3-aceb-309491575c98",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### __Part 02 - Step 03 -__ Concatenate both networks (Not yet physically connected by edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd0c48c-9570-4716-ade0-240dcde4b8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate nodes\n",
    "\n",
    "# Prepare data for concatenation\n",
    "ntw_01_nodes_prep = base_nodes_i2[['osmid','ntw_join','geometry']].copy()\n",
    "ntw_02_nodes_prep = comp_nodes_i[['osmid','geometry']].copy()\n",
    "\n",
    "# Add missing 'ntw_join' col to ntw_02_nodes_prep\n",
    "ntw_02_nodes_prep['ntw_join'] = 0\n",
    "\n",
    "# Differenciate networks's origin\n",
    "ntw_01_nodes_prep['ntw_origin'] = 'ntw_01'\n",
    "ntw_02_nodes_prep['ntw_origin'] = 'ntw_02'\n",
    "\n",
    "# Concatenate nodes\n",
    "concatenated_nodes = pd.concat([ntw_01_nodes_prep,ntw_02_nodes_prep])\n",
    "node_count_1 = len(concatenated_nodes)\n",
    "\n",
    "# Drop duplicated nodes\n",
    "concatenated_nodes.drop_duplicates(subset=['osmid','geometry'],inplace=True)\n",
    "node_count_2 = len(concatenated_nodes)\n",
    "print(f\"Both networks where sharing nodes where there is an intersection between networks. Duplicates were expected but no longer needed.\")\n",
    "print(f\"Dropped {node_count_1-node_count_2} nodes duplicated when intersecting networks.\")\n",
    "\n",
    "# Reset index\n",
    "concatenated_nodes.reset_index(inplace=True,drop=True)\n",
    "\n",
    "# Show\n",
    "print(concatenated_nodes.dtypes)\n",
    "print(concatenated_nodes.shape)\n",
    "concatenated_nodes.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4827429d-d84d-4bdc-a921-1adc035f266f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate edges\n",
    "\n",
    "# Prepare data for concatenation\n",
    "ntw_01_edges_prep = base_edges_i2[['u','v','key','geometry']].copy()\n",
    "ntw_02_edges_prep = comp_edges_i[['u','v','key','geometry']].copy()\n",
    "\n",
    "# Add missing 'ntw_join' col to both gdfs\n",
    "ntw_01_edges_prep['ntw_join'] = 0\n",
    "ntw_02_edges_prep['ntw_join'] = 0\n",
    "\n",
    "# Differenciate networks\n",
    "ntw_01_edges_prep['ntw_origin'] = 'ntw_01'\n",
    "ntw_02_edges_prep['ntw_origin'] = 'ntw_02'\n",
    "\n",
    "# Concatenate edges\n",
    "concatenated_edges = pd.concat([ntw_01_edges_prep,ntw_02_edges_prep])\n",
    "\n",
    "# Drop duplicated edges ('u','v','key' and 'geometry' (There should be 0))\n",
    "edge_count_1 = len(concatenated_edges)\n",
    "concatenated_edges.drop_duplicates(subset=['u','v','key','geometry'],inplace=True)\n",
    "edge_count_2 = len(concatenated_edges)\n",
    "print(f\"There should be 0 duplicated ('u','v','key','geometry') edges. Dropped {edge_count_1-edge_count_2}.\")\n",
    "\n",
    "# Fix shared edge_ids by re-assigning all 'key's for duplicated edges.\n",
    "# An edge from network 01 might go from the same u to the same v as an edge from the network 02 if any (or both) of them were split by an intersection.\n",
    "# If this happens, they would both have the same 'u', 'v' and 'key', but different paths (geometry).\n",
    "print(f\"After concatenating both networks, found {len(concatenated_edges.loc[concatenated_edges.duplicated(subset=['u','v','key'])])} edges with the same unique ID ('u', 'v' and 'key'). Fixing edge IDs\")\n",
    "# Calculate edges lentgh\n",
    "concatenated_edges['length'] = concatenated_edges.to_crs(projected_crs).length\n",
    "# Add key column, all zeros\n",
    "concatenated_edges['key'] = 0\n",
    "# Fix keys\n",
    "concatenated_edges = src.resolve_duplicates_indexes(concatenated_edges, projected_crs)\n",
    "print(f\"Fixed edge IDs. Duplicates: {len(concatenated_edges.loc[concatenated_edges.duplicated(subset=['u','v','key'])])}.\")\n",
    "\n",
    "# Show\n",
    "print(concatenated_edges.dtypes)\n",
    "print(concatenated_edges.shape)\n",
    "concatenated_edges.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982ed308-fcbe-4ac9-b6a4-75a6e13c3856",
   "metadata": {},
   "outputs": [],
   "source": [
    "if localsave_02_03:\n",
    "    # Current step output directory\n",
    "    output_fold = output_dir + 'part02_step03_ntwsconcat/'\n",
    "    \n",
    "    # Create folder if it doesn't exist\n",
    "    if os.path.exists(output_fold) == False:\n",
    "        os.mkdir(output_fold)\n",
    "    \n",
    "    # Save result\n",
    "    concatenated_nodes.to_file(output_fold + \"concatenated_nodes.gpkg\")\n",
    "    concatenated_edges.to_file(output_fold + \"concatenated_edges.gpkg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae91405-93f7-47b7-9d73-092ae86bf4ff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## __Part 02 - Step 04 -__ Connect networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b7d6f4-0ad6-4d37-956b-1b0c5ef32481",
   "metadata": {},
   "source": [
    "Using the connected_nodes df (Created in Part 02 - Step 02), which stablishes the relations between conection_nodes (from network 02, complementary network) and nodes in network 01 (base network), in this step __new edges that connect both networks are drawn.__\n",
    "\n",
    "After that __consequential intersections__ (Which occur when the drawn edge crosses another edge in its way to connect network 01 and 02) __are fixed__ according to its nature (CASE A, CASE B or CASE C)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868b5153-9280-4d24-bd59-b4cb315bf81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_datafor_part02_step04:\n",
    "    # From Part02 - Step02\n",
    "    connected_nodes = pd.read_csv(output_dir + \"part02_step02_connectidentif/connected_nodes_df.csv\")\n",
    "    # From Part02 - Step03\n",
    "    concatenated_nodes = gpd.read_file(output_dir + \"part02_step03_ntwsconcat/concatenated_nodes.gpkg\")\n",
    "    concatenated_edges = gpd.read_file(output_dir + \"part02_step03_ntwsconcat/concatenated_edges.gpkg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53a5e8e-f8e0-49db-936f-0965f97a821e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### __Part 02 - Step 04 -__ Draw new edges and identify consequential intersections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc62424-1dbe-4a01-86aa-25cc8b55339a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show\n",
    "print(connected_nodes.info())\n",
    "connected_nodes.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378ca2dd-ac40-4651-8e6f-8c67751b098e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show\n",
    "print(concatenated_nodes.info())\n",
    "concatenated_nodes.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ac1179-fecb-4514-814c-ecc53bb0a787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show\n",
    "print(concatenated_edges.info())\n",
    "concatenated_edges.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af31823f-63fb-4deb-aec5-d3c3271f1895",
   "metadata": {},
   "outputs": [],
   "source": [
    "casetype_logs = False\n",
    "\n",
    "# Function that compares geometries (Points) considering a tolerance of 1mm \n",
    "def geometries_are_equal_with_tolerance(geom1, geom2, tolerance=0.001): \n",
    "    return geom1.equals(geom2) or geom1.distance(geom2) < tolerance\n",
    "\n",
    "##### Time\n",
    "time_1 = time.time()\n",
    "##### Time\n",
    "\n",
    "# Reset previously_produced osmids (Used in function produce_osmid())\n",
    "previously_produced = 0\n",
    "\n",
    "# Create intersection_nodes_3, a GeoDataFrame that will be used in function network_intersections_update()\n",
    "# to perform a third round of intersections (Used to fix consequential intersections)\n",
    "# ----- OUTPUT DTYPES MANAGEMENT FOR intersection_nodes_3 GEODATAFRAME -----\n",
    "intersection_nodes_3_cols = {\"osmid\": \"int64\", # Node that is the intersection (clipping point)\n",
    "                             \"u\": \"int64\", # Edge that is intersected\n",
    "                             \"v\": \"int64\", # Edge that is intersected\n",
    "                             \"key\": \"int64\", # Edge that is intersected\n",
    "                             \"retain_how\":\"string\", #Stablishes which part of the split edge to keep\n",
    "                             \"edge_origin\":\"string\", #Helps divide network_intersections_update() process\n",
    "                             \"case\":\"string\" #Helps identify if an splitting case must not be repeated in an edge\n",
    "                            }\n",
    "intersection_nodes_3 = gpd.GeoDataFrame(columns=list(intersection_nodes_3_cols.keys()) + [\"geometry\"], crs=projected_crs).astype({**intersection_nodes_3_cols, \"geometry\": \"geometry\"})\n",
    "# Reorder columns\n",
    "intersection_nodes_3 = intersection_nodes_3[[\"osmid\", \"geometry\", # Node to be created to serve as a connection point in ntw_01\n",
    "                                             \"u\", \"v\", \"key\", # Edge from ntw_01 to be split by the created node\n",
    "                                             \"retain_how\", \"edge_origin\",\"case\"]] # Data about the split and division of splitting process\n",
    "# ----- OUTPUT DTYPES MANAGEMENT FOR intersection_nodes_3 GEODATAFRAME -----\n",
    "\n",
    "# Store the osmids and edge_ids created to join both networks (To identify them after the process)\n",
    "all_join_osmids = []\n",
    "ntw_join_edgeids = []\n",
    "\n",
    "# Consequential_intersections and its three cases (A, B and C) explanation.\n",
    "# The concatenated_edges are renamed joined_edges_concat.\n",
    "# The following code iterates over the connected_nodes df (The dataframe with defined relations between networks) and\n",
    "# draws the new needed edges (when needed). The new edges sometimes intersect other edges as a consequence of them being straight lines between\n",
    "# nodes. These new intersections are called consequential_intersections.\n",
    "# The first connected_node explores consequential_intersections with the existing concatenated_edges (renamed as joined_edges_concat), but\n",
    "# since the new-drawn-edge gets added to joined_edges_concat, the following connected_node also explores consequential_intersections with any\n",
    "# recently-drawn edge. This results in cases A, B and C explored at the end of the for loop.\n",
    "\n",
    "print(\"Identifying edge_ids origin (ntw_01 or ntw_02).\")\n",
    "# Create joined_edges_concat with a column for edge_id\n",
    "joined_edges_concat = src.create_unique_edge_id(concatenated_edges)\n",
    "# Extract edge_ids that are known to be either ntw_01 or ntw_02\n",
    "ntw01_edges_ids = set(joined_edges_concat.loc[joined_edges_concat.ntw_origin=='ntw_01'].edge_id.unique())\n",
    "ntw02_edges_ids = set(joined_edges_concat.loc[joined_edges_concat.ntw_origin=='ntw_02'].edge_id.unique())\n",
    "\n",
    "# LOG CODE - Progress logs\n",
    "# Will create progress logs when progress reaches these percentages:\n",
    "progress_logs = [0,5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100] # for log statistics\n",
    "progress_count = 0\n",
    "# LOG CODE - Progress logs\n",
    "\n",
    "##### Time - Divided processes time measurements\n",
    "edge_produced_intersections_time = 0\n",
    "consequential_intersections_time = 0\n",
    "##### Time - Divided processes time measurements\n",
    "\n",
    "# Iterate over each relation stablished\n",
    "print(\"Starting iteration over each relation stablished.\")\n",
    "for idx, connected_node in connected_nodes.iterrows():\n",
    "\n",
    "    ##### Time\n",
    "    time_a = time.time()\n",
    "    ##### Time\n",
    "\n",
    "    # LOG CODE - Progress logs\n",
    "    # Measures current progress, prints if passed a checkpoint of progress_logs list.\n",
    "    current_progress = (progress_count / len(connected_nodes))*100\n",
    "    for checkpoint in progress_logs:\n",
    "        if (current_progress >= checkpoint):\n",
    "            print(f\"Categorizing node relation types. {checkpoint}% done.\")\n",
    "            progress_logs.remove(checkpoint)\n",
    "            break\n",
    "    # LOG CODE - Progress logs\n",
    "    \n",
    "    # Development checks -----------------------------------\n",
    "    #current_osmid = connected_node.connecting_ntw02_osmid\n",
    "    #osmid_checks = [67074577229143110]\n",
    "    #if current_osmid not in osmid_checks:\n",
    "    #    continue\n",
    "    #else:\n",
    "    #    print(current_osmid)\n",
    "    # Development checks -----------------------------------\n",
    "    \n",
    "    # Extract relation's data\n",
    "    # The following words are used to refer to the origin of data:\n",
    "    # 'connection' refers to the data from network 01,\n",
    "    # 'connecting' referes to data from network 02.\n",
    "    # The reasoning used is that network 02 (complementary) is ---connecting--> to network 01 (base)\n",
    "    connection_ntw01_osmid = connected_node.connection_ntw01_osmid\n",
    "    connection_type = connected_node.connection_type\n",
    "    connecting_ntw02_osmid = connected_node.connecting_ntw02_osmid\n",
    "\n",
    "    # If the connection for the current connecting_node was identified to already exist nearby (Part 02 - Step 02), --> skip (continue)\n",
    "    if connection_type == 'existing':\n",
    "        # LOG CODE - Progress logs\n",
    "        progress_count+=1\n",
    "        # LOG CODE - Progress logs\n",
    "        continue # Next nodes_relation\n",
    "\n",
    "    # Identify the node's coordinates on ntw_01 that's going to get connected to the node on ntw_02\n",
    "    connection_node_gdf = concatenated_nodes.loc[concatenated_nodes.osmid==connection_ntw01_osmid].copy()\n",
    "    # Safety check (in case a node failed in re-update of the network)\n",
    "    if len(connection_node_gdf)<1:\n",
    "        print(\"--\"*30)\n",
    "        print(f\"WARNING: Skipping node not found. Connection (ntw01): {connection_ntw01_osmid}. Connecting (ntw02): {connecting_ntw02_osmid}.\")\n",
    "        print(\"--\"*30)\n",
    "        continue\n",
    "    connection_node_geom = connection_node_gdf.geometry.unique()[0]\n",
    "    connection_node_coords = connection_node_geom.coords[0]\n",
    "    if casetype_logs:\n",
    "        print(f\"ntw_01 - connection_node_geom: {connection_node_geom}.\") \n",
    "    \n",
    "    # Identify the node's coordinates on ntw_02 that's going to get connected to the node on ntw_01\n",
    "    connecting_node_gdf = concatenated_nodes.loc[concatenated_nodes.osmid==connecting_ntw02_osmid].copy()\n",
    "    # Safety check (in case a node failed in re-update of the network)\n",
    "    if len(connecting_node_gdf)<1:\n",
    "        print(\"--\"*30)\n",
    "        print(f\"WARNING: Skipping node not found. Connection (ntw01): {connection_ntw01_osmid}. Connecting (ntw02): {connecting_ntw02_osmid}.\")\n",
    "        print(\"--\"*30)\n",
    "        continue\n",
    "    connecting_node_geom = connecting_node_gdf.geometry.unique()[0]\n",
    "    connecting_node_coords = connecting_node_geom.coords[0]\n",
    "    if casetype_logs:\n",
    "        print(f\"ntw_02 - connecting_node_geom: {connecting_node_geom}.\") \n",
    "\n",
    "    # Create LineString between connection_node and connecting_node\n",
    "    line_geom = LineString([[connection_node_coords[0],connection_node_coords[1]],[connecting_node_coords[0],connecting_node_coords[1]]])\n",
    "    \n",
    "    # Store new LineString to concat to joined_edges_concat and to analyse consequential_intersections\n",
    "    # ----- OUTPUT REGISTRATION FOR new_edge IN joined_edges_concat GEODATAFRAME ----- [Concatenates after overlay]\n",
    "    # Create temporary df with the current case's data\n",
    "    df_temporal = pd.DataFrame({'u': [int(connecting_ntw02_osmid)], # Edge that is being created\n",
    "                                'v': [int(connection_ntw01_osmid)], # Edge that is being created\n",
    "                                'key': [int(0)], # Edge that is being created\n",
    "                                'geometry':[line_geom], # Edge that is being created\n",
    "                                'ntw_join':[int(1)], # Helps identify which edges were created to join both networks\n",
    "                                'ntw_origin':['ntw_join'] # Has data of all origins, created edges get assigned \"ntw_join\"\n",
    "                               }\n",
    "                              )\n",
    "    new_edge = gpd.GeoDataFrame(df_temporal, geometry='geometry', crs=projected_crs)\n",
    "    # Force all datatypes to match the datatypes of the gdf to where the data will be merged\n",
    "    dtypes_dict = joined_edges_concat.dtypes.to_dict() #Dict with TARGET dtypes\n",
    "    filtered_dtypes = {col: dtypes_dict[col] for col in new_edge.columns if col in dtypes_dict} # Filters for cols in case of mismatch\n",
    "    new_edge = new_edge.astype(filtered_dtypes) # Assigns those types\n",
    "    if casetype_logs:\n",
    "        print(f\"Created edge between connection and connecting nodes.\") \n",
    "    # ----- OUTPUT REGISTRATION FOR new_edge IN joined_edges_concat GEODATAFRAME ----- [Concatenates after overlay]\n",
    "    \n",
    "    # Create unique edge_id for new_edge (requires input ID cols as int)\n",
    "    new_edge = src.create_unique_edge_id(new_edge)\n",
    "\n",
    "    # Store the edge_ids created to join both networks (To identify them after the process)\n",
    "    join_edge_ids = list(new_edge.edge_id.unique())\n",
    "    ntw_join_edgeids = ntw_join_edgeids + join_edge_ids\n",
    "    \n",
    "    # Find all intersections that the new edge creates on the current network edges\n",
    "    # (Used to search for consequential_intersections)\n",
    "    produced_intersections = joined_edges_concat.overlay(new_edge,how='intersection',keep_geom_type=False)\n",
    "    \n",
    "    # Concatenate new_edge to joined_edges_concat gdf \n",
    "    # (It is after .overlay(), else the LineString intersects with itself)\n",
    "    joined_edges_concat = pd.concat([joined_edges_concat,new_edge])\n",
    "    joined_edges_concat.reset_index(inplace=True,drop=True)\n",
    "\n",
    "    # CONSEQUENTIAL INTERSECTIONS ANALYSIS\n",
    "    # Explode the produced_intersections (If the new edge intersected the same edge twice or more, it produces MultiPoints)\n",
    "    produced_intersections = produced_intersections.explode(index_parts=False)\n",
    "    produced_intersections.reset_index(inplace=True,drop=True)\n",
    "    \n",
    "    # Remove from the produced_intersections the nodes that are currently being joined (Obviously, the line that connects them intersects with them)\n",
    "    joined_nodes_lst = [connection_node_geom,connecting_node_geom] #Starting and ending nodes of the connecting edge itself\n",
    "    if casetype_logs:\n",
    "        print(f\"Looking for produced intersections. Dropping intersections with {joined_nodes_lst}.\")\n",
    "    # Compare produced_intersections with starting and ending nodes, and filter for those that are not them\n",
    "    produced_intersections = produced_intersections.loc[ ~produced_intersections['geometry'].apply(\n",
    "        lambda x: any(geometries_are_equal_with_tolerance(x, node_geom) for node_geom in joined_nodes_lst))].copy()\n",
    "    # Previous way of comparing (Sometimes failed to identify when the line intersected it's own node)\n",
    "    #produced_intersections = produced_intersections.loc[~produced_intersections['geometry'].isin(joined_nodes_lst)].copy()\n",
    "    \n",
    "    if casetype_logs:\n",
    "        print(f\"Found intersections at the following points: {produced_intersections['geometry'].unique()}.\")\n",
    "\n",
    "    ##### Time\n",
    "    time_b = time.time()\n",
    "    ##### Time\n",
    "    \n",
    "    # If there are any intersections remaining, they are consequential_intersections.\n",
    "    # (Meaning, the drawn line is intersecting with other lines in an undesired way)\n",
    "    if len(produced_intersections)>0:\n",
    "\n",
    "        # Rename gdf as \"consequential_intersections\"\n",
    "        consequential_intersections = produced_intersections.copy()\n",
    "        consequential_intersections.reset_index(inplace=True,drop=True)\n",
    "        del produced_intersections\n",
    "\n",
    "        if len(consequential_intersections)>1:\n",
    "            print(f\"NOTE: Edge drawn between connection_ntw01_osmid {connection_ntw01_osmid} and connecting_ntw02_osmid {connecting_ntw02_osmid} produced {len(consequential_intersections)} consequential intersection.\")\n",
    "        if len(consequential_intersections)>10:\n",
    "            print(\"--\"*30)\n",
    "            print(f\"WARNING: Edge drawn between connection_ntw01_osmid {connection_ntw01_osmid} and connecting_ntw02_osmid {connecting_ntw02_osmid} produced {len(consequential_intersections)} consequential intersection.\")\n",
    "            print(\"--\"*30)\n",
    "        \n",
    "        # Rename columns to distinguish network origin\n",
    "        # (Overlay produced _1 for joined_edges_concat data and _2 for new_edge data)\n",
    "        consequential_intersections.rename(columns={'edge_id_1':'intersected_edge_id',\n",
    "                                                    'edge_id_2':'drawn_edge_id'},inplace=True)\n",
    "        \n",
    "        # Load any edges related to the connection [ntw_01] node\n",
    "        connection_related_idx = (joined_edges_concat.u==connection_ntw01_osmid)|(joined_edges_concat.v==connection_ntw01_osmid)\n",
    "        connection_related_edges = joined_edges_concat.loc[connection_related_idx].copy()\n",
    "        connection_related_edges_ids = list(connection_related_edges['edge_id'].unique())\n",
    "        \n",
    "        # Load any edges related to the connecting [ntw_02] node\n",
    "        connecting_related_idx = (joined_edges_concat.u==connecting_ntw02_osmid)|(joined_edges_concat.v==connecting_ntw02_osmid)\n",
    "        connecting_related_edges = joined_edges_concat.loc[connecting_related_idx].copy()\n",
    "        connecting_related_edges_ids = list(connecting_related_edges['edge_id'].unique())\n",
    "        \n",
    "        # Iterate over consequential_intersections:\n",
    "        for idx,intersection in consequential_intersections.iterrows():\n",
    "    \n",
    "            # Extract intersection's data\n",
    "            # Edge that was intersected\n",
    "            intersected_u = int(intersection.u_1)\n",
    "            intersected_v = int(intersection.v_1)\n",
    "            intersected_key = intersection.key_1\n",
    "            intersected_edge_id = intersection.intersected_edge_id\n",
    "            # Find intersected edge's origin\n",
    "            if intersected_edge_id in ntw01_edges_ids:\n",
    "                intersected_edge_origin = 'ntw_01'\n",
    "            elif intersected_edge_id in ntw02_edges_ids:\n",
    "                intersected_edge_origin = 'ntw_02'\n",
    "            elif intersected_edge_id in ntw_join_edgeids:\n",
    "                print(f\"NOTE: Intersected edge id {intersected_edge_id} found in ntw_join_edgeids while intersecting with drawn edge {drawn_edge_id}.\")\n",
    "                intersected_edge_origin = 'ntw_join'\n",
    "            else:\n",
    "                print(f\"WARNING: Could not identify edge {intersected_edge_id}'s origin, intersecting with drawn edge {drawn_edge_id}. Assigning ntw_join.\")\n",
    "                intersected_edge_origin = 'ntw_join'\n",
    "                \n",
    "            # New (drawn) edge that is intersecting\n",
    "            drawn_edge_u = int(intersection.u_2)\n",
    "            drawn_edge_v = int(intersection.v_2)\n",
    "            drawn_edge_key = intersection.key_2\n",
    "            drawn_edge_id = intersection.drawn_edge_id\n",
    "\n",
    "            # The intersection will most likely become a new node\n",
    "            # Extract its geometry and produce a unique osmid\n",
    "            intersectionpoint_geom = intersection.geometry\n",
    "            produced_osmid = produce_osmid(concatenated_nodes, concatenated_nodes, previously_produced)\n",
    "            # Since concatenated_nodes are not being updated, next time try with next possible osmid\n",
    "            previously_produced = produced_osmid+1 \n",
    "            # Store the osmids created to join both networks (To identify them after the process)\n",
    "            all_join_osmids.append(produced_osmid)\n",
    "\n",
    "            # Cases analysis\n",
    "            # n.n --------------- CASE A\n",
    "            if intersected_edge_id in connection_related_edges_ids:\n",
    "                # CASE A: The new edge intersects an edge comming out of the node __TO__ where the connection was being performed.\n",
    "                # --> CASE A will draw the connection from the connecting_node on ntw02 to the intersection point ONLY.\n",
    "                # -->        because thats were there's already a known connection with ntw01.\n",
    "                # -->        The connection_related_edge gets split to add a new node, keeping both sides of the edge.\n",
    "                # -->        The drawn edge gets split at the intersection, keeping one side of the edge.\n",
    "                case = 'CASE A'\n",
    "                if casetype_logs:\n",
    "                    print(f\"CASE A: New edge from connecting node {connecting_ntw02_osmid} intersects an edge comming out of the node TO where the connection was being performed.\")\n",
    "                # Register how both edges will be clipped\n",
    "                # ----- OUTPUT REGISTRATION FOR intersection_nodes_3 GEODATAFRAME -----\n",
    "                # Create temporary df with the current case's data\n",
    "                # (First element of each list is a network 01 or network 02 edge (intersected))\n",
    "                # (Second element of each list is the new (drawn) edge)\n",
    "                df_temporal = pd.DataFrame({'osmid': [int(produced_osmid), int(produced_osmid)],\n",
    "                                            'geometry': [intersectionpoint_geom, intersectionpoint_geom], # Node that is the intersection (clipping point)\n",
    "                                            'u': [int(intersected_u), int(drawn_edge_u)], # Edge that is intersected\n",
    "                                            'v':[int(intersected_v), int(drawn_edge_v)], # Edge that is intersected\n",
    "                                            'key':[int(intersected_key),int(drawn_edge_key)], # Edge that is intersected\n",
    "                                            'retain_how':['both','u'], #Keeps both parts of the intersected edge, and just side comming from ntw02 ('u') of the drawn edge\n",
    "                                            'edge_origin':[intersected_edge_origin,'ntw_join'], #Edge's origin\n",
    "                                            \"case\":[case,case] #Data about the registered case\n",
    "                                           }\n",
    "                                          )\n",
    "                gdf_temporal = gpd.GeoDataFrame(df_temporal, geometry='geometry', crs=projected_crs)\n",
    "                # Force all datatypes to match the datatypes of the gdf to where the data will be merged\n",
    "                dtypes_dict = intersection_nodes_3.dtypes.to_dict() #Dict with TARGET dtypes\n",
    "                filtered_dtypes = {col: dtypes_dict[col] for col in gdf_temporal.columns if col in dtypes_dict} # Filters for cols in case of mismatch\n",
    "                gdf_temporal = gdf_temporal.astype(filtered_dtypes) # Assigns those types\n",
    "                # Concatenate to TARGET without altering original dtypes\n",
    "                intersection_nodes_3 = pd.concat([intersection_nodes_3, gdf_temporal], ignore_index=True)\n",
    "                # ----- OUTPUT REGISTRATION FOR intersection_nodes_3 GEODATAFRAME -----                \n",
    "\n",
    "            # n.n --------------- CASE B\n",
    "            elif intersected_edge_id in connecting_related_edges_ids:\n",
    "                # CASE B: The new edge intersects an edge comming out of the node __FROM__ where the connection was being performed.\n",
    "                # --> CASE B will draw the connection from the intersection point to the connection_node on ntw_01 ONLY.\n",
    "                # -->        because the rest (from intersection point to ntw02) is redundant.\n",
    "                # -->        The connecting_related_edge gets split to add a new node, keeping both sides of the edge.\n",
    "                # -->        The drawn edge gets split at the intersection, keeping one side of the edge.\n",
    "                case = 'CASE B'\n",
    "                if casetype_logs:\n",
    "                    print(f\"CASE B: New edge from connecting node {connecting_ntw02_osmid} intersects an edge comming out of the node FROM where the connection was being performed.\")\n",
    "                # Register how both edges will be clipped\n",
    "                # ----- OUTPUT REGISTRATION FOR intersection_nodes_3 GEODATAFRAME -----\n",
    "                # Create temporary df with the current case's data\n",
    "                # (First element of each list is a network 01 or network 02 edge (intersected))\n",
    "                # (Second element of each list is the new (drawn) edge)\n",
    "                df_temporal = pd.DataFrame({'osmid': [int(produced_osmid), int(produced_osmid)],\n",
    "                                            'geometry': [intersectionpoint_geom, intersectionpoint_geom], # Node that is the intersection (clipping point)\n",
    "                                            'u': [int(intersected_u), int(drawn_edge_u)], # Edge that is intersected\n",
    "                                            'v':[int(intersected_v), int(drawn_edge_v)], # Edge that is intersected\n",
    "                                            'key':[int(intersected_key),int(drawn_edge_key)], # Edge that is intersected\n",
    "                                            'retain_how':['both','v'], #Keeps both parts of the intersected edge, and just side comming from ntw01 ('v') of the drawn edge\n",
    "                                            'edge_origin':[intersected_edge_origin,'ntw_join'], #Edge's origin\n",
    "                                            \"case\":[case,case] #Data about the registered case\n",
    "                                           }\n",
    "                                          )\n",
    "                gdf_temporal = gpd.GeoDataFrame(df_temporal, geometry='geometry', crs=projected_crs)\n",
    "                # Force all datatypes to match the datatypes of the gdf to where the data will be merged\n",
    "                dtypes_dict = intersection_nodes_3.dtypes.to_dict() #Dict with TARGET dtypes\n",
    "                filtered_dtypes = {col: dtypes_dict[col] for col in gdf_temporal.columns if col in dtypes_dict} # Filters for cols in case of mismatch\n",
    "                gdf_temporal = gdf_temporal.astype(filtered_dtypes) # Assigns those types\n",
    "                # Concatenate to TARGET without altering original dtypes\n",
    "                intersection_nodes_3 = pd.concat([intersection_nodes_3, gdf_temporal], ignore_index=True)\n",
    "                # ----- OUTPUT REGISTRATION FOR intersection_nodes_3 GEODATAFRAME ----- \n",
    "            \n",
    "            # n.n --------------- CASE C\n",
    "            else:\n",
    "                # CASE C: The new edge intersects with either network 01 or network 02 at an UNRELATED edge.\n",
    "                #         (Or at a related ntw02 edge, but one that shouldn't be cut)\n",
    "                # --> CASE C will draw only create a new node on the intersection and split the edges, \n",
    "                # -->        keeping both sides on both split edges.\n",
    "                # -->        ntw01 or ntw02 edge gets split to add a new node, keeping both sides of the edge.\n",
    "                # -->        drawn edge gets split at the intersection, keeping both sides of the edge.\n",
    "                case = 'CASE C'\n",
    "                if casetype_logs:\n",
    "                    print(f\"CASE C: New edge from connecting node {connecting_ntw02_osmid} intersects with either network 01 or network 02 at an UNRELATED edge.\")\n",
    "                # Register how both edges will be clipped\n",
    "                # ----- OUTPUT REGISTRATION FOR intersection_nodes_3 GEODATAFRAME -----\n",
    "                # Create temporary df with the current case's data\n",
    "                # (First element of each list is a network 01 or network 02 edge (intersected))\n",
    "                # (Second element of each list is the new (drawn) edge)\n",
    "                df_temporal = pd.DataFrame({'osmid': [int(produced_osmid), int(produced_osmid)],\n",
    "                                            'geometry': [intersectionpoint_geom, intersectionpoint_geom], # Node that is the intersection (clipping point)\n",
    "                                            'u': [int(intersected_u), int(drawn_edge_u)], # Edge that is intersected\n",
    "                                            'v':[int(intersected_v), int(drawn_edge_v)], # Edge that is intersected\n",
    "                                            'key':[int(intersected_key),int(drawn_edge_key)], # Edge that is intersected\n",
    "                                            'retain_how':['both','both'], #Keeps both parts of the intersected edge, and both parts of the drawn edge\n",
    "                                            'edge_origin':[intersected_edge_origin,'ntw_join'], #Edge's origin\n",
    "                                            \"case\":[case,case] #Data about the registered case\n",
    "                                           }\n",
    "                                          )\n",
    "                gdf_temporal = gpd.GeoDataFrame(df_temporal, geometry='geometry', crs=projected_crs)\n",
    "                # Force all datatypes to match the datatypes of the gdf to where the data will be merged\n",
    "                dtypes_dict = intersection_nodes_3.dtypes.to_dict() #Dict with TARGET dtypes\n",
    "                filtered_dtypes = {col: dtypes_dict[col] for col in gdf_temporal.columns if col in dtypes_dict} # Filters for cols in case of mismatch\n",
    "                gdf_temporal = gdf_temporal.astype(filtered_dtypes) # Assigns those types\n",
    "                # Concatenate to TARGET without altering original dtypes\n",
    "                intersection_nodes_3 = pd.concat([intersection_nodes_3, gdf_temporal], ignore_index=True)\n",
    "                # ----- OUTPUT REGISTRATION FOR intersection_nodes_3 GEODATAFRAME ----- \n",
    "    \n",
    "    # LOG CODE - Progress logs\n",
    "    # Finished reviewing current osmid. Continue with next osmid in non_contact_osmids.\n",
    "    progress_count+=1\n",
    "    # LOG CODE - Progress logs\n",
    "    \n",
    "    ##### Time\n",
    "    time_c = time.time()\n",
    "    edge_produced_intersections_time = edge_produced_intersections_time + (time_b - time_a)\n",
    "    consequential_intersections_time = consequential_intersections_time + (time_c - time_b)\n",
    "    ##### Time\n",
    "\n",
    "# Create a edge_id col for the intersected edges registered for each intersection on intersection_nodes_3 gdf\n",
    "intersection_nodes_3 = src.create_unique_edge_id(intersection_nodes_3)\n",
    "\n",
    "##### Time\n",
    "time_2 = time.time()\n",
    "print(f\"TIME: {time_2-time_1} seconds on drawing edges and identifying consequential intersections.\")\n",
    "print(f\"Of that TIME, {edge_produced_intersections_time} seconds on stablishing the edge and finding all produced intersections.\")\n",
    "print(f\"Of that TIME, {consequential_intersections_time} seconds dealing with consequential intersections.\")\n",
    "##### Time\n",
    "\n",
    "# Show\n",
    "print(joined_edges_concat.shape)\n",
    "print(f\"Duplicates: {len(joined_edges_concat.loc[joined_edges_concat.duplicated('edge_id')])}.\")\n",
    "joined_edges_concat.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932bc00e-6b03-40cc-a72a-c5bac635b98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show\n",
    "print(intersection_nodes_3.shape)\n",
    "print(f\"Duplicates: {len(intersection_nodes_3.loc[intersection_nodes_3.duplicated(['osmid','edge_id'])])}.\")\n",
    "intersection_nodes_3.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb346c6-a9ad-4d16-ad98-66136e54e241",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_save = True\n",
    "# Temporary save (Development) [Create folder manually]\n",
    "if dev_save:\n",
    "    output_fold = output_dir + 'part02_step04_ntwsconnect/'\n",
    "    joined_edges_concat.to_file(output_fold + \"tmp_joined_edges_concat.gpkg\")\n",
    "    intersection_nodes_3.to_file(output_fold + \"tmp_intersection_nodes_3.gpkg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa828fbb-bf90-4fc3-bcec-b517d789d928",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### __Part 02 - Step 04 -__ Fix consequential_intersections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96456cb4-c918-4169-b693-95ff539bc2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_datafor_substep_part02_step04:\n",
    "    # From Part02 - Step03\n",
    "    concatenated_nodes = gpd.read_file(output_dir + \"part02_step03_ntwsconcat/concatenated_nodes.gpkg\")\n",
    "    # From previous substep (Draw new edges and identify consequential intersections)\n",
    "    joined_edges_concat = gpd.read_file(output_dir + \"part02_step04_ntwsconnect/tmp_joined_edges_concat.gpkg\")\n",
    "    intersection_nodes_3 = gpd.read_file(output_dir + \"part02_step04_ntwsconnect/tmp_intersection_nodes_3.gpkg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f8b7fa-535d-4aee-972e-2e9114b028cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_nodes.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958b07ae-ac18-4615-b032-c9cff0eeccc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_edges_concat.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f9bcf0-d077-47ed-bb5a-9a161f718492",
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection_nodes_3.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9aaeaf-6927-43fe-901c-205eae30fa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Time\n",
    "time_1 = time.time()\n",
    "##### Time\n",
    "\n",
    "# Apply network_intersections_update() function\n",
    "joined_nodes_fix, joined_edges_fix = network_intersections_update(current_ntw_nodes = concatenated_nodes,\n",
    "                                                                  current_ntw_edges = joined_edges_concat,\n",
    "                                                                  intersection_nodes = intersection_nodes_3,\n",
    "                                                                  projected_crs = projected_crs,\n",
    "                                                                  consider_ntw_origin = True,\n",
    "                                                                  intersection_logs = False,\n",
    "                                                                  clipping_logs=False)\n",
    "##### Time\n",
    "time_2 = time.time()\n",
    "print(f\"TIME: {time_2-time_1} seconds on updating the network.\")\n",
    "##### Time\n",
    "\n",
    "# MULTIPLE INTERSECTION ADAPTATION: drop duplicates\n",
    "# Duplicated nodes and edges may occur when three or more lines cross at the same point.\n",
    "# Regular .drop_duplicates() cannot be used since edges could be mirrored.\n",
    "# Example:\n",
    "#        Line 1: u=1, v=2, key=0, geom=((1,1),(2,1))\n",
    "#        Line 2: u=2, v=1, key=0, geom=((2,1),(1,1))\n",
    "# Drop nodes and edges duplicates\n",
    "joined_nodes_fix,joined_edges_fix = drop_intersection_network_duplicates(joined_nodes_fix,joined_edges_fix)\n",
    "\n",
    "##### Time\n",
    "time_3 = time.time()\n",
    "print(f\"TIME: {time_3-time_2} seconds on dropping intersection duplicates.\")\n",
    "##### Time\n",
    "\n",
    "# Create unique edge_id to simplify edge identification\n",
    "joined_edges_fix = src.create_unique_edge_id(joined_edges_fix)\n",
    "# Identify edges created during current intersection\n",
    "no_longer_in_use = \"\"\" [Substituted by adding 'consider_ntw_origin' to function network_intersections_update()]\n",
    "join_idx = joined_edges_fix.intersecting==1\n",
    "edge_ids_lst = list(joined_edges_fix.loc[join_idx].edge_id.unique())\n",
    "all_join_edgeids = all_join_edgeids + edge_ids_lst\n",
    "\"\"\"\n",
    "\n",
    "# Final format (Drop cols 'intersecting' and reset indexes)\n",
    "joined_nodes_fix.drop(columns=['intersecting'],inplace=True)\n",
    "joined_edges_fix.drop(columns=['intersecting'],inplace=True)\n",
    "joined_nodes_fix.reset_index(inplace=True,drop=True)\n",
    "joined_edges_fix.reset_index(inplace=True,drop=True)\n",
    "\n",
    "# Show\n",
    "print(joined_nodes_fix.shape)\n",
    "joined_nodes_fix.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776656f5-768d-4e09-b566-e9bacfeed1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show\n",
    "print(joined_edges_fix.shape)\n",
    "joined_edges_fix.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b440b3e2-21a9-48d9-ae3f-c3f0ad2bcdef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### __Part 02 - Step 04 -__ Retrieve origin data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a4c46b-6020-4d01-94f9-212885ceca39",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_longer_in_use = \"\"\" [Substituted by adding 'consider_ntw_origin' to function network_intersections_update()]\n",
    "# Nodes from ntw_01\n",
    "ntw01_nodes_osmid = list(concatenated_nodes.loc[concatenated_nodes.ntw_origin=='ntw_01'].osmid.unique())\n",
    "join_idx = joined_nodes_fix.osmid.isin(ntw01_nodes_osmid)\n",
    "joined_nodes_fix.loc[join_idx,'ntw_origin'] = 'ntw_01'\n",
    "\n",
    "# Nodes from ntw_02\n",
    "ntw02_nodes_osmid = list(concatenated_nodes.loc[concatenated_nodes.ntw_origin=='ntw_02'].osmid.unique())\n",
    "join_idx = joined_nodes_fix.osmid.isin(ntw02_nodes_osmid)\n",
    "joined_nodes_fix.loc[join_idx,'ntw_origin'] = 'ntw_02'\n",
    "\n",
    "# Nodes used to join both networks (all_join_osmids)\n",
    "join_idx = joined_nodes_fix.osmid.isin(all_join_osmids)\n",
    "joined_nodes_fix.loc[join_idx,'ntw_origin'] = 'ntw_join'\n",
    "\"\"\"\n",
    "\n",
    "# Origin assignment test\n",
    "len_ntw01_nodes = len(joined_nodes_fix.loc[joined_nodes_fix.ntw_origin=='ntw_01'])\n",
    "len_ntw02_nodes = len(joined_nodes_fix.loc[joined_nodes_fix.ntw_origin=='ntw_02'])\n",
    "len_ntwjoin_nodes = len(joined_nodes_fix.loc[joined_nodes_fix.ntw_origin=='ntw_join'])\n",
    "print(f\"Identified origin of {len_ntw01_nodes + len_ntw02_nodes + len_ntwjoin_nodes} nodes.\")\n",
    "\n",
    "# Show\n",
    "print(joined_nodes_fix.shape)\n",
    "joined_nodes_fix.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a18256-7610-4d7c-bfb0-9b69196bd6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show detail\n",
    "print(joined_nodes_fix.crs)\n",
    "print(joined_nodes_fix.dtypes)\n",
    "joined_nodes_fix.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e784268f-8a1d-4445-bde1-aa2ec3862c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_longer_in_use = \"\"\" [Substituted by adding 'consider_ntw_origin' to function network_intersections_update()]\n",
    "# Edges from ntw_01\n",
    "ntw01_edges_ids = list(joined_edges_concat.loc[joined_edges_concat.ntw_origin=='ntw_01'].edge_id.unique())\n",
    "join_idx = joined_edges_fix.edge_id.isin(ntw01_edges_ids)\n",
    "joined_edges_fix.loc[join_idx,'ntw_origin'] = 'ntw_01'\n",
    "\n",
    "# Edges from ntw_02\n",
    "ntw02_edges_ids = list(joined_edges_concat.loc[joined_edges_concat.ntw_origin=='ntw_02'].edge_id.unique())\n",
    "join_idx = joined_edges_fix.edge_id.isin(ntw02_edges_ids)\n",
    "joined_edges_fix.loc[join_idx,'ntw_origin'] = 'ntw_02'\n",
    "\n",
    "# Edges used to join both networks (all_join_edgeids)\n",
    "join_idx = joined_edges_fix.edge_id.isin(all_join_edgeids)\n",
    "joined_edges_fix.loc[join_idx,'ntw_origin'] = 'ntw_join'\n",
    "\"\"\"\n",
    "\n",
    "# Origin assignment test\n",
    "len_ntw01_edges = len(joined_edges_fix.loc[joined_edges_fix.ntw_origin=='ntw_01'])\n",
    "len_ntw02_edges = len(joined_edges_fix.loc[joined_edges_fix.ntw_origin=='ntw_02'])\n",
    "len_ntwjoin_edges = len(joined_edges_fix.loc[joined_edges_fix.ntw_origin=='ntw_join'])\n",
    "print(f\"Identified origin of {len_ntw01_edges + len_ntw02_edges + len_ntwjoin_edges} edges.\")\n",
    "\n",
    "# Show\n",
    "print(joined_edges_fix.shape)\n",
    "joined_edges_fix.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dc2656-e52c-4c0b-b0cc-7edd38fb2a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show detail\n",
    "print(joined_edges_fix.crs)\n",
    "print(joined_edges_fix.dtypes)\n",
    "joined_edges_fix.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882f79d1-1e4e-4eed-8e66-2ad1d8a58e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "if localsave_02_04:\n",
    "    # Current step output directory\n",
    "    output_fold = output_dir + 'part02_step04_ntwsconnect/'\n",
    "    \n",
    "    # Create folder if it doesn't exist\n",
    "    if os.path.exists(output_fold) == False:\n",
    "        os.mkdir(output_fold)\n",
    "    \n",
    "    # Save result\n",
    "    joined_nodes_fix.to_file(output_fold + \"joined_nodes_fix.gpkg\")\n",
    "    joined_edges_fix.to_file(output_fold + \"joined_edges_fix.gpkg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5e70c3-9b6b-4efd-82a7-25bd2b63ecb9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## __Part 02 - Step 05 -__ Network cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9105346d-d82c-42c0-97ed-d47712d93a43",
   "metadata": {},
   "source": [
    "This step cleans the joined network (nodes and edges) by analysing the number of edges that use each node (osmid).\n",
    "\n",
    "1. Osmids that recieve __0 edges__ should be __deleted__. They are unconnected.\n",
    "2. Osmids that recieve __1 edge__ will be __deleted__ (Along with that edge) __IF__ their ntw origin is __not ntw_01.__ (ntw_01 is the 'reliable' base_network, deleting osmids and edges this way would remove dead end streets).\n",
    "3. Osmids that recieve __2 edges__ will be __joined__ (Shouldn't be an intersection, but a continous edge).\n",
    "4. Osmids that recieve __3 or more edges__ are regular intersections. Nothing is done with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e44aae-919b-42e1-862e-995064522b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_datafor_part02_step05:\n",
    "    # From Part02 - Step04\n",
    "    joined_nodes_fix = gpd.read_file(output_dir + \"part02_step04_ntwsconnect/joined_nodes_fix.gpkg\")\n",
    "    joined_edges_fix = gpd.read_file(output_dir + \"part02_step04_ntwsconnect/joined_edges_fix.gpkg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3289a1f9-3bc3-4bad-836d-aef56fc3bdc0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### __Part 02 - Step 05 -__ Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469e9705-6503-4b2c-bdb6-3b1fd12157cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_nodes_cleaning = joined_nodes_fix.copy()\n",
    "joined_edges_cleaning = joined_edges_fix.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd329d6d-73e6-49f1-a3da-95e0d6b06b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No duplicates in osmid check\n",
    "print(joined_nodes_cleaning.shape)\n",
    "test = joined_nodes_cleaning.copy()\n",
    "test.drop_duplicates(subset='osmid',inplace=True)\n",
    "print(test.shape)\n",
    "# Geometry revision (Should be all points)\n",
    "print(f\"GEOMETRIES: ---> {joined_nodes_cleaning.geom_type.value_counts()}.\")\n",
    "joined_nodes_cleaning.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5334335f-da7d-469c-bbb1-a19e893834f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No duplicates in 'u', 'v', 'key' check\n",
    "print(joined_edges_cleaning.shape)\n",
    "test = joined_edges_cleaning.copy()\n",
    "test.drop_duplicates(subset=['u','v','key'],inplace=True)\n",
    "print(test.shape)\n",
    "# Geometry revision (Should be all LineStrings)\n",
    "print(f\"GEOMETRIES: ---> {joined_edges_cleaning.geom_type.value_counts()}.\")\n",
    "joined_edges_cleaning.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c62796-c7be-47e3-9999-dd0824e65cbf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### __Part 02 - Step 05 -__ Identify the number of edges that use each node ('streets_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bd14e2-6134-4cc6-93a7-ded001546735",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_list = list(joined_edges_cleaning.u)\n",
    "v_list = list(joined_edges_cleaning.v)\n",
    "\n",
    "old_way = \"\"\"\n",
    "# NOTE: It takes about 546s (9mins) in a city the size of Medelln, Colombia. Could optimize process.\n",
    "# NOTE: For Guadalajara it took +1400s (23 min)\n",
    "# Find the number of edges that reach that osmid\n",
    "for osmid in list(joined_nodes_cleaning.osmid.unique()):\n",
    "    # Total times = Times where that osmid is an edges 'u' + Times where that osmid is an edges 'v'\n",
    "    streets_count = u_list.count(osmid) + v_list.count(osmid)\n",
    "    # Data registration\n",
    "    idx = joined_nodes_cleaning.osmid == osmid\n",
    "    joined_nodes_cleaning.loc[idx,'streets_count'] = streets_count\n",
    "\"\"\"\n",
    "# OPTIMIZED WAY:\n",
    "# Count osmid occurrences and store as a pd.Series (fillna as 0s if a given osmid is misssing from 'u' or 'v')\n",
    "u_counts = pd.Series(u_list).value_counts().fillna(0)\n",
    "v_counts = pd.Series(v_list).value_counts().fillna(0)\n",
    "\n",
    "# Add counts from both series \n",
    "streets_count_series = (u_counts.add(v_counts, fill_value=0)).astype(int)\n",
    "\n",
    "# Assign values to joined_nodes_cleaning using map\n",
    "joined_nodes_cleaning['streets_count'] = joined_nodes_cleaning['osmid'].map(streets_count_series).fillna(0).astype(int)\n",
    "\n",
    "# Show\n",
    "print(joined_nodes_cleaning.shape)\n",
    "joined_nodes_cleaning.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279f0ea0-3674-42a5-b3aa-69e743adc635",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### __Part 02 - Step 05 -__ Osmids that recieve __0 edges__ should be __deleted__. They are unconnected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a11f2cd-3236-4f25-a0d4-e41c22ca9b06",
   "metadata": {},
   "source": [
    "__Show__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b965bc24-ae3d-475d-8693-3c5c8eda07c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_idx = joined_nodes_cleaning.streets_count==0\n",
    "zero_edge_osmids = joined_nodes_cleaning.loc[find_idx]\n",
    "# Show\n",
    "print(len(zero_edge_osmids))\n",
    "zero_edge_osmids.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a734c3ce-f8b3-4ab8-9e39-2a1878384368",
   "metadata": {},
   "source": [
    "__Remove__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dcc856-f3a0-433a-a111-9b060c83e2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_nodes_cleaning_0 = joined_nodes_cleaning.copy()\n",
    "keep_idx = joined_nodes_cleaning_0.streets_count>0\n",
    "joined_nodes_cleaning_0 = joined_nodes_cleaning_0.loc[keep_idx].copy()\n",
    "print(f\"Deleted {len(joined_nodes_cleaning)-len(joined_nodes_cleaning_0)} nodes with 0 edges attatched.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94f7301-d8f6-4023-ab5a-ff5237c80034",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### __Part 02 - Step 05 -__ Osmids that recieve __1 edge__ will be __deleted (Along with that edge)__ IF their ntw origin is __not ntw_01.__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad0d108-adf7-4a5c-8de9-30b8944c48da",
   "metadata": {},
   "source": [
    "__Show__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f099400-9fdf-4539-b770-a60734fe9436",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_idx = (joined_nodes_cleaning.streets_count==1)&(joined_nodes_cleaning.ntw_origin!='ntw_01')\n",
    "one_edge_osmids = joined_nodes_cleaning.loc[find_idx]\n",
    "# Show\n",
    "print(len(one_edge_osmids))\n",
    "one_edge_osmids.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b149ee9b-90bd-475d-add6-cececaee6e44",
   "metadata": {},
   "source": [
    "__Remove nodes__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940c2643-0bd3-426b-b45e-6670a5014eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_nodes_cleaning_1 = joined_nodes_cleaning_0.copy()\n",
    "drop_idx = (joined_nodes_cleaning.streets_count==1)&(joined_nodes_cleaning.ntw_origin!='ntw_01')\n",
    "joined_nodes_cleaning_1 = joined_nodes_cleaning_1.loc[~drop_idx].copy()\n",
    "print(f\"Deleted {len(joined_nodes_cleaning_0)-len(joined_nodes_cleaning_1)} nodes with 1 edge attatched.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db7d684-ad5d-4d94-969f-cc2c0012e654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geometry revision (Should be all points)\n",
    "print(f\"GEOMETRIES: ---> {joined_nodes_cleaning_1.geom_type.value_counts()}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3df2501-3351-41a8-94d0-a635eb389d50",
   "metadata": {},
   "source": [
    "__Remove edges__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471354aa-abfa-49c1-a6e8-350e462ff60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate the osmids that were dropped\n",
    "drop_idx = (joined_nodes_cleaning.streets_count==1)&(joined_nodes_cleaning.ntw_origin!='ntw_01')\n",
    "dropped_osmids = list(joined_nodes_cleaning_0.loc[drop_idx].osmid.unique())\n",
    "# Delete the edges that are related to those osmids\n",
    "joined_edges_cleaning_1 = joined_edges_cleaning.copy()\n",
    "drop_edge_idx = (joined_edges_cleaning_1.u.isin(dropped_osmids))|(joined_edges_cleaning_1.v.isin(dropped_osmids))\n",
    "joined_edges_cleaning_1 = joined_edges_cleaning_1.loc[~drop_edge_idx]\n",
    "print(f\"Deleted {len(joined_edges_cleaning)-len(joined_edges_cleaning_1)} edges (not from ntw_01).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71144a7-a925-4ab3-8b53-a30efdb6f292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geometry revision (Should be all LineStrings)\n",
    "print(f\"GEOMETRIES: ---> {joined_edges_cleaning_1.geom_type.value_counts()}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42326b3-d276-4952-93f4-a801e8231fd9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### __Part 02 - Step 05 -__ Osmids that recieve __2 edges__ will be __joined__ (Shouldn't be an intersection, but a continous edge)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ebb7f7-ff3c-446b-8b15-73b6fb2b371b",
   "metadata": {},
   "source": [
    "__Show__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70397030-3d69-4f10-a603-0ba9d69a463c",
   "metadata": {},
   "outputs": [],
   "source": [
    "replaced_by_functions = \"\"\"\n",
    "find_idx = joined_nodes_cleaning.streets_count==2\n",
    "two_edge_osmids = joined_nodes_cleaning.loc[find_idx].copy()\n",
    "# Show\n",
    "print(len(two_edge_osmids))\n",
    "two_edge_osmids.head(2)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759b7f2e-eb01-449c-955e-f2ebea5b19ae",
   "metadata": {},
   "source": [
    "__Remove -__ Dissolve edges from two_edge_osmids into one edge (MultiLineString) and try to convert it to LineString."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7323db83-527a-4519-bb94-02b520bfb270",
   "metadata": {},
   "outputs": [],
   "source": [
    "replaced_by_functions = \"\"\"\n",
    "joined_nodes_cleaning_2 = joined_nodes_cleaning_1.copy()\n",
    "joined_edges_cleaning_2 = joined_edges_cleaning_1.copy()\n",
    "\n",
    "multilinestring_fail_lst = []\n",
    "\n",
    "for osmid in list(two_edge_osmids.osmid.unique()):\n",
    "\n",
    "    try:\n",
    "\n",
    "        #print(\"--\"*10)\n",
    "        #print(f\"OSMID OF INTEREST: {osmid}.\")\n",
    "        \n",
    "        # Find edges that use that osmid\n",
    "        found_in_u = joined_edges_cleaning_2.loc[joined_edges_cleaning_2.u == osmid].copy()\n",
    "        found_in_v = joined_edges_cleaning_2.loc[joined_edges_cleaning_2.v == osmid].copy()\n",
    "        found_edges = pd.concat([found_in_u,found_in_v])\n",
    "        #print(found_edges)\n",
    "        \n",
    "        # Find the other osmids those edges connect with\n",
    "        u_v_list = list(found_edges.u.unique()) + list(found_edges.v.unique())\n",
    "        # Remove itself\n",
    "        u_v_list = [i for i in u_v_list if i != osmid]\n",
    "        # If both edges connect to the same osmid (It is a loop road split in two)\n",
    "        # Double that osmid\n",
    "        if len(u_v_list) == 1:\n",
    "            u_v_list.append(u_v_list[0])\n",
    "        \n",
    "        # Dissolve lines (Creates MultiLineString, will convert to LineString)\n",
    "        flattened_edge = found_edges.dissolve()\n",
    "    \n",
    "        # Add data to new edge\n",
    "        flattened_edge['u'] = u_v_list[0]\n",
    "        flattened_edge['v'] = u_v_list[1]\n",
    "        flattened_edge['key'] = 0\n",
    "        flattened_edge['ntw_origin'] = 'ntw_cleaning'\n",
    "        flattened_edge = src.create_unique_edge_id(flattened_edge)\n",
    "    \n",
    "        # Convert MultiLineStrings to LineStrings\n",
    "        flattened_edge = flattened_edge.apply(src.multilinestring_to_linestring,axis=1)\n",
    "        # If conversion fails, flattened edge_id gets added to global list multilinestring_fail_lst.\n",
    "        flattened_edge_id = flattened_edge.edge_id.unique()[0]\n",
    "        if flattened_edge_id not in multilinestring_fail_lst:\n",
    "            # Delete useless node and previous edges, concat new flattened edge.\n",
    "            joined_nodes_cleaning_2 = joined_nodes_cleaning_2.loc[joined_nodes_cleaning_2.osmid != osmid].copy()\n",
    "            joined_edges_cleaning_2 = joined_edges_cleaning_2.loc[(joined_edges_cleaning_2.u != osmid)&(joined_edges_cleaning_2.v != osmid)].copy()\n",
    "            joined_edges_cleaning_2 = pd.concat([joined_edges_cleaning_2,flattened_edge])\n",
    "        else:\n",
    "            print(f\"Not dissolving edges reaching node {osmid}.\")\n",
    "\n",
    "    except:\n",
    "        print(osmid)\n",
    "        print(u_v_list)\n",
    "\n",
    "# Final format\n",
    "joined_edges_cleaning_2.reset_index(inplace=True,drop=True)\n",
    "joined_nodes_cleaning_2.reset_index(inplace=True,drop=True)\n",
    "\n",
    "# Show\n",
    "print(joined_edges_cleaning_2.shape)\n",
    "joined_edges_cleaning_2.tail(5)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44625710-7c86-4fd5-91fa-855c37487946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial network\n",
    "print(\"--\"*30)\n",
    "print(f\"Entering nodes: {joined_nodes_cleaning_1.shape[0]}.\")\n",
    "print(f\"Entering edges: {joined_edges_cleaning_1.shape[0]}.\")\n",
    "print(\"--\"*30)\n",
    "\n",
    "##### Time\n",
    "time_1 = time.time()\n",
    "##### Time\n",
    "\n",
    "# Re-create network\n",
    "# ---> Substitutes created osmids with coordinate osmids\n",
    "# ---> Removes two_edge_osmids using function remove_redundant_nodes()\n",
    "# ---> Fixes all keys using resolve_duplicates_indexes()\n",
    "print('Transforming network...')\n",
    "project_nodes, project_edges = src.network_entities(joined_nodes_cleaning_1,\n",
    "                                                    joined_edges_cleaning_1,\n",
    "                                                    projected_crs,\n",
    "                                                    expand_coords=(True,100))\n",
    "\n",
    "##### Time\n",
    "time_2 = time.time()\n",
    "print(f\"TIME: {time_2-time_1} seconds on transforming the network.\")\n",
    "##### Time\n",
    "\n",
    "# Show result\n",
    "print(f\"Transformed nodes: {project_nodes.shape[0]}.\")\n",
    "print(f\"Transformed edges: {project_edges.shape[0]}.\")\n",
    "print(\"--\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e8d2a0-d2de-4efd-9156-c8e5e697e6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geometry revision (Should be all points)\n",
    "print(f\"GEOMETRIES: ---> {joined_nodes_cleaning_2.geom_type.value_counts()}.\")\n",
    "# Geometry revision (Should be all LineStrings)\n",
    "print(f\"GEOMETRIES: ---> {joined_edges_cleaning_2.geom_type.value_counts()}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6ce0c1-b758-4904-b73e-fcfcded0e351",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### __Part 02 - Step 05 -__ Save result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592140d8-1d9a-42f6-8aa6-1088acc37de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nodes final format\n",
    "if 'osmid' not in project_nodes.columns:\n",
    "    project_nodes.reset_index(inplace=True)\n",
    "project_nodes['osmid'] = project_nodes.osmid.astype(int)\n",
    "\n",
    "# Edges final format\n",
    "if 'u' not in project_edges.columns:\n",
    "    project_edges.reset_index(inplace=True)\n",
    "project_edges = src.create_unique_edge_id(project_edges)\n",
    "project_edges['u'] = project_edges.u.astype(int)\n",
    "project_edges['v'] = project_edges.v.astype(int)\n",
    "\n",
    "# Set projected crs\n",
    "project_nodes = project_nodes.to_crs(projected_crs)\n",
    "project_edges = project_edges.to_crs(projected_crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbc5e6a-d08e-4afc-a2e2-4aa084bdddd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show detail\n",
    "print(project_nodes.crs)\n",
    "print(project_nodes.dtypes)\n",
    "project_nodes.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d23f33-7a48-4635-9363-d7b0c5f37b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show detail\n",
    "print(project_edges.crs)\n",
    "print(project_edges.dtypes)\n",
    "project_edges.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778ae08c-ceb4-48af-8ee7-d38148bc495d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if localsave_02_05:\n",
    "    # Current step output directory\n",
    "    output_fold = output_dir + 'part02_step05_ntwsclean/'\n",
    "    \n",
    "    # Create folder if it doesn't exist\n",
    "    if os.path.exists(output_fold) == False:\n",
    "        os.mkdir(output_fold)\n",
    "    \n",
    "    # Save result\n",
    "    project_nodes.to_file(output_fold + f\"{city}_nodes_proj_net_final.gpkg\")\n",
    "    project_edges.to_file(output_fold + f\"{city}_edges_proj_net_final.gpkg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176dd445-9ba2-4aa7-93fc-fbbab362dd86",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## __Part 02 - Step 06 -__ Network consolidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f031a0-2357-47bd-921a-c5e3bfa81f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_datafor_part02_step06:\n",
    "    # From Part02 - Step05\n",
    "    project_nodes = gpd.read_file(output_dir + f\"part02_step05_ntwsclean/{city}_nodes_proj_net_final.gpkg\")\n",
    "    project_edges = gpd.read_file(output_dir + f\"part02_step05_ntwsclean/{city}_edges_proj_net_final.gpkg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bec8c8-0bc0-4a96-8f0f-d20c8fcbeb1f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### __Part 02 - Step 06 -__ Network preparation for consolidation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9a547d-c89f-4a91-86a8-16692ed7dc59",
   "metadata": {},
   "source": [
    "__Remove all edges whose 'u' or 'v' cannot be found in 'osmid'__ (Temporary fix, must fix remove_redundant_nodes() function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b3ea39-8802-4943-aec4-4b0d07a6bd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All 'osmid' in project_nodes\n",
    "node_ids = set(project_nodes['osmid'])\n",
    "\n",
    "# All 'u's or 'v's that cannot be found in 'osmid'\n",
    "missing_u = list(project_edges[~project_edges['u'].isin(node_ids)]['u'].unique())\n",
    "missing_v = list(project_edges[~project_edges['v'].isin(node_ids)]['v'].unique())\n",
    "\n",
    "# Drop (if they exist)\n",
    "drop_these_edges = missing_u+missing_v\n",
    "if len(drop_these_edges)>0:\n",
    "    \n",
    "    # Show\n",
    "    print(f\"Found {len(drop_these_edges)} existing 'u's or 'v's without osmid.\")\n",
    "    print(\"Nodos faltantes en u:\", missing_u)\n",
    "    print(\"Nodos faltantes en v:\", missing_v)\n",
    "\n",
    "    # Remove\n",
    "    missing_osmid_idx = (project_edges.u.isin(drop_these_edges))|(project_edges.v.isin(drop_these_edges))\n",
    "    original_len = len(project_edges)\n",
    "    project_edges = project_edges.loc[~missing_osmid_idx].copy()\n",
    "    updated_len = len(project_edges)\n",
    "\n",
    "    # Show\n",
    "    print(f\"Dropped {original_len-updated_len} edges.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9409947-51f1-4639-be52-4823aa96d586",
   "metadata": {},
   "source": [
    "__Prepare nodes__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538761b8-84ed-47ba-8b21-466ff6e36db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- ----- ----- Time start\n",
    "time_1 = time.time()\n",
    "# ----- ----- ----- Process\n",
    "\n",
    "# Ensure crs\n",
    "if project_nodes.crs != projected_crs:\n",
    "    project_nodes = project_nodes.to_crs(projected_crs)\n",
    "    print(f\"Changed crs to {projected_crs}.\")\n",
    "# Set index if necessary\n",
    "if 'osmid' in project_nodes.columns:\n",
    "    original_len = len(project_nodes)\n",
    "    project_nodes = project_nodes.drop_duplicates(subset=['osmid'])\n",
    "    new_len = len(project_nodes)\n",
    "    project_nodes.set_index('osmid',inplace=True)\n",
    "    print(f\"Dropped {new_len-original_len} nodes to set osmid as nodes index.\")\n",
    "# Show\n",
    "project_nodes.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8634162-137d-45df-9b47-2724511736e1",
   "metadata": {},
   "source": [
    "__Prepare edges__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725e400e-af90-43c4-ac9c-0302a49b3727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- ----- ----- Time start\n",
    "time_1 = time.time()\n",
    "# ----- ----- ----- Process\n",
    "\n",
    "# Ensure crs\n",
    "if project_edges.crs != projected_crs:\n",
    "    project_edges = project_edges.to_crs(projected_crs)\n",
    "    print(f\"Changed crs to {projected_crs}.\")\n",
    "# Set index if necessary\n",
    "if 'u' in project_edges.columns:\n",
    "    original_len = len(project_edges)\n",
    "    project_edges = project_edges.drop_duplicates(subset=['u','v','key'])\n",
    "    new_len = len(project_edges)\n",
    "    project_edges.set_index(['u','v','key'],inplace=True)\n",
    "    print(f\"Dropped {new_len-original_len} edges to set 'u','v' and 'key' as nodes index.\")\n",
    "# Filter for data of interest\n",
    "project_edges = project_edges[['length','geometry']]\n",
    "# ----- ----- ----- Time end\n",
    "time_2 = time.time()\n",
    "print(f\"TIME: {time_2-time_1} seconds.\")\n",
    "\n",
    "# CRS revision\n",
    "print(f\"NODES CRS: -----> {project_edges.crs}.\")\n",
    "# Index revision\n",
    "print(f\"INDEX COL: -----> {project_edges.index.names}.\")\n",
    "# Duplicated index revision\n",
    "print(f\"DUP INDEXES: ---> {project_edges.index.duplicated().sum()}.\")\n",
    "# Geometry revision (Should be all Lines)\n",
    "print(f\"GEOMETRIES: ---> {project_edges.geom_type.value_counts()}.\")\n",
    "# Columns revision\n",
    "print(project_edges.info())\n",
    "project_edges.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd04baa5-bf6d-4b8d-8dc4-c0522652ec45",
   "metadata": {},
   "source": [
    "__Make sure geometries in edges are LineStrings__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80b125c-de06-4570-9ae2-b3b87b021b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "line_types = list(project_edges.geom_type.value_counts().keys())\n",
    "if 'MultiLineString' in line_types:\n",
    "    print(\"There are MultiLineStrings in project_edges. Converting single lines to LineStrings.\")\n",
    "    # Convert MultiLineStrings to LineString if they have one line only\n",
    "    project_edges[\"geometry\"] = project_edges[\"geometry\"].apply(lambda geom: geom if geom.geom_type == \"LineString\" #Keeps LineStrings\n",
    "                                                                else LineString(geom.geoms[0]) if len(geom.geoms) == 1 #Turns to LineString if there's one line only\n",
    "                                                                else geom) # Keeps MultiLineString if there's more than one line\n",
    "\n",
    "line_types = list(project_edges.geom_type.value_counts().keys())\n",
    "if 'MultiLineString' in line_types:\n",
    "    print(\"There are still MultiLineStrings in project_edges. Forcing to sigle lines.\")\n",
    "    # Filter for remaining MultiLineStrings\n",
    "    multilinestring_edges = project_edges[project_edges.geometry.geom_type == \"MultiLineString\"].copy()\n",
    "    # Apply shapely's ops.linemerge\n",
    "    multilinestring_edges[\"geometry\"] = multilinestring_edges[\"geometry\"].apply(ops.linemerge)\n",
    "    # Re-join with project_edges\n",
    "    project_edges = pd.concat([project_edges[project_edges.geometry.geom_type == \"LineString\"], # Edges that are already LineStrings\n",
    "                               multilinestring_edges  # Forced MuLtiLineStrings to LineStrings\n",
    "                              ])\n",
    "\n",
    "line_types = list(project_edges.geom_type.value_counts().keys())\n",
    "if 'MultiLineString' in line_types:\n",
    "    print(\"There are still MultiLineStrings in project_edges. Eliminating them (Procedure to dismantle and recreating lines is a work in progress).\")\n",
    "    # Filter for remaining MultiLineStrings\n",
    "    multilinestring_edges = project_edges[project_edges.geometry.geom_type == \"MultiLineString\"].copy()\n",
    "    print(f\"DELETING {len(multilinestring_edges)} EDGES.\")\n",
    "    # Save them for solution development\n",
    "    multilinestring_edges.to_file(output_dir + \"part02_step05_ntwsclean/multilines_debug.gpkg\", driver=\"GPKG\")\n",
    "\n",
    "    # Keep LineStrings only\n",
    "    project_edges = project_edges[project_edges.geometry.geom_type == \"LineString\"].copy()\n",
    "    # Make sure there are no problems by exploding the edges\n",
    "    original_len = len(project_edges)\n",
    "    project_edges = project_edges.explode()\n",
    "    exploded_len = len(project_edges)\n",
    "    print(f\"Exploded edges and created {exploded_len-original_len} edges.\")\n",
    "    \n",
    "print(f\"GEOMETRIES: ---> {project_edges.geom_type.value_counts()}.\")\n",
    "project_edges.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835b7c58-b2b3-4127-9bda-02628d2430e4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### __Part 02 - Step 06 -__ Network consolidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53411f1a-6c99-4680-9b5c-89c36e3b1b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Creating graph...')\n",
    "G = ox.graph_from_gdfs(project_nodes, project_edges)\n",
    "print(f'Consolidating graph using tolerance of 5 meters...')\n",
    "# consolidate graph\n",
    "G2 = ox.consolidate_intersections(G, rebuild_graph=True, tolerance=5, dead_ends=True)\n",
    "del G #Save space\n",
    "# Extract nodes and edges from consolidated graph\n",
    "cons_nodes, cons_edges = ox.graph_to_gdfs(G2)\n",
    "del G2 #Save space\n",
    "# Format nodes\n",
    "print('Formating nodes...')\n",
    "cons_nodes = cons_nodes.reset_index()\n",
    "cons_nodes = cons_nodes.drop(columns=['osmid'])\n",
    "cons_nodes = cons_nodes.rename(columns={'osmid_original':'osmid'})\n",
    "cons_nodes = cons_nodes.set_index('osmid')\n",
    "\n",
    "# Format edges\n",
    "print('Formating edges...')\n",
    "cons_edges = cons_edges.reset_index()\n",
    "cons_edges = cons_edges.drop(columns=['u','v'])\n",
    "cons_edges = cons_edges.rename(columns={'u_original':'u',\n",
    "                                        'v_original':'v'})\n",
    "cons_edges = cons_edges.set_index(['u','v','key'])\n",
    "\n",
    "# Drop column 'index' if present\n",
    "if 'index' in cons_nodes.columns:\n",
    "    cons_nodes = cons_nodes.drop(columns=['index'])\n",
    "if 'index' in cons_edges.columns:\n",
    "    cons_edges = cons_edges.drop(columns=['index'])\n",
    "\n",
    "# Show\n",
    "print(cons_nodes.crs)\n",
    "print(cons_nodes.info())\n",
    "cons_nodes.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c680005-8d54-4e20-9166-618d67ede32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show\n",
    "print(cons_edges.crs)\n",
    "print(cons_edges.info())\n",
    "cons_edges.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36b93e73-6c2c-43ac-99c4-68e464af6500",
   "metadata": {},
   "outputs": [],
   "source": [
    "if localsave_02_06:\n",
    "    # Current step output directory\n",
    "    output_fold = output_dir + 'part02_step06_ntwsconsolidate/'\n",
    "    \n",
    "    # Create folder if it doesn't exist\n",
    "    if os.path.exists(output_fold) == False:\n",
    "        os.mkdir(output_fold)\n",
    "    \n",
    "    # Save result\n",
    "    cons_nodes.to_file(output_fold + f\"{city}_nodes_proj_net_consolidated5m.gpkg\")\n",
    "    cons_edges.to_file(output_fold + f\"{city}_edges_proj_net_consolidated5m.gpkg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55bdfbb-563a-4e57-b062-73f4cc1cf3ed",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## __Part 02 - Step 07 -__ Network rebuilding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc19e34c-27e3-4ee1-8b84-8b72177523e4",
   "metadata": {},
   "source": [
    "This step __rebuilds the network__ using function __src.network_entities()__\n",
    "\n",
    "Rebuilding the network is desirable because consolidating the network creates osmid in the form of [osmid_1, osmid_2, (...) osmid_n]\n",
    "For example, if node __67636596229115605__ and node __67636388229116380__ are <consolidation_distance> meters or less from each other, they get consolidated into one node. The edges that used either node will now use node with __osmid '[67636596229115605, 67636388229116380]'__. However, the 'u' and 'v' values of those edges remain 67636596229115605 or 67636388229116380.\n",
    "\n",
    "__Rebuilding the network creates coordinate-based IDs (osmid, u, v).__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35a473f2-5f36-49af-b42b-aab9fc6adc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_datafor_part02_step07:\n",
    "    # From Part02 - Step06\n",
    "    cons_nodes = gpd.read_file(output_dir + f\"part02_step06_ntwsconsolidate/{city}_nodes_proj_net_consolidated5m.gpkg\")\n",
    "    cons_edges = gpd.read_file(output_dir + f\"part02_step06_ntwsconsolidate/{city}_edges_proj_net_consolidated5m.gpkg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15291143-c923-464a-aefa-3e7233497320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Entering nodes: 125089.\n",
      "Entering edges: 197918.\n",
      "------------------------------------------------------------\n",
      "Transforming network...\n",
      "Creating unique ids (osmid) for nodes based on coordinates...\n",
      "Creating unique ids (u,v) for edges based on coordinates...\n",
      "Removing redundant nodes...\n",
      "Resolving indexes u, v, key...\n",
      "TIME: 143.15663504600525 seconds on transforming the network.\n",
      "WARNING: Missing 163 u nodes [66498932228963725 66831561228647682 67157194228549323 66874200228354076\n",
      " 66709082228283112 67470233228273987 68154221228141147 67865309228947949\n",
      " 68056402228332925 67362914229430959 67745632229259088 66898039229483979\n",
      " 66321298227119192 67831191227363882 66996837229561779 66619133229240948\n",
      " 67294483227904685 68569472228343639 66490964229364988 68330295227642892\n",
      " 67135737229573737 65810066228241327 67092965227811862 67191674227980171\n",
      " 67326306228966945 67120512228573070 66818718228628528 66960953228366683\n",
      " 66962235228383511 66904222228448470 66715579229127577 66586185228728959\n",
      " 66278773228332400 67902016228260760 67822139228093988 66070552228067234\n",
      " 66068430228285663 66069826228291431 66377903229513782 66476391227847210\n",
      " 67394728228012935 68055634228214407 67290927227808564 67624883227741399\n",
      " 66448728227738458 68453584227817096 68475476228581008 67333174228830440\n",
      " 67588494227867391 68351148227641757 67369917229392779 67113829226837417\n",
      " 65919355228145930 67023668229388107 68034472228307414 67106979226775844\n",
      " 65887575227292497 66105371228151411 67155109229274993 67854392228918461\n",
      " 67122718227816633 67009025229607308 66435392228661882 68213403228223998\n",
      " 66747180228001691 66349649227427594 67922055228596421 68182375228211339\n",
      " 66278684228052189 67780948228454788 67389936229581441 67292591229731862\n",
      " 66094776229685545 67919047227860712 67237374227987184 67782909227426832\n",
      " 67527692229355625 68094109228436146 68026348228382646 67146510228987651\n",
      " 67946879228309454 65905090227587903 65903775228216296 66062324227147693\n",
      " 66102433227142902 66108640226965517 66162658227137470 66174538228576149\n",
      " 66175598228883291 66249468229465184 66268883229538852 66311412227088813\n",
      " 66315360228930205 66336742227462862 66344261228884228 66370405228407439\n",
      " 66388801228392088 66404343229448038 66478558226972317 66526127229361996\n",
      " 66572894227901181 66614185229653552 66659373229058000 66650972226620745\n",
      " 66679103228036966 66682408229630365 66698281228060837 66704508226833920\n",
      " 66722530228818375 66757403227020731 66764376228392198 66809429229533644\n",
      " 66796092229666300 66832472227589383 66834079227594687 66855293227661002\n",
      " 66860009227049441 66903148227612354 66910873227608756 66912570227608552\n",
      " 66941172227870039 67040556227290118 67047960228099148 67100274229714484\n",
      " 67144628228183793 67135695227412086 67152767227133536 67179943227105742\n",
      " 67248646228708302 67273931228521331 67532981229448732 67866691228233496\n",
      " 67919813228185903 67924845228184510 67942660228019537 67938376228017920\n",
      " 67947617228177444 67978573227458785 67999257228531294 68018236228788668\n",
      " 68176069228153024 68196587228145056 68337313228252905 68345707227610994\n",
      " 68451641227606984 66197434228570611 68353157228577958 68029022227918996\n",
      " 68208932228169994 67444823229318588 68095038227849492 67612863227817865\n",
      " 67185539228904933 68047396227941670 67990160228381078 67886211228317354\n",
      " 66863299227898151 66183452228028047 65883628228387334 67218245228004792\n",
      " 66273682228483812 67381388227992097 67512304228110204]\n",
      "------------------------------------------------------------\n",
      "WARNING: Missing 241 v nodes: [66970458229384643 66861629228633243 66710292228336214 66945229228628403\n",
      " 67177023228640572 66963579228400507 67253107228237271 66702258228379763\n",
      " 67300248228538243 66219332228916915 67126898229204967 67030130227942027\n",
      " 66457891228590036 67049129228333876 67664255227986789 66039469228191722\n",
      " 67118875229012152 66360782228455574 66702254229226215 67889822228328247\n",
      " 67891938228781667 67978057228800722 68040338228787860 68354441228536685\n",
      " 67201628227889701 68026977228349657 68038918228337330 67986959228342564\n",
      " 65970040228039128 67989746227962004 66719438229624204 67231549229560754\n",
      " 67307395229446037 67410071229666973 67210371229646799 67161579229763301\n",
      " 67249913229779644 68278213228823817 66318227228205396 66521525227857223\n",
      " 68370451228211905 68237792228237870 68077029228182306 67401242227766969\n",
      " 67188848229262167 67490836227863780 66499484229100619 67509974229363740\n",
      " 66787171227008035 66798865227020784 66846461227006428 66609541226704461\n",
      " 66735215227685060 65873556227163385 66189834227232702 67125243227709661\n",
      " 67707274227705441 68078655228374065 67262663227746612 66193003228899399\n",
      " 67232575229450382 68488435227684533 68291457227839017 67865117227228929\n",
      " 67946241227270349 67399644227601273 67406071227471531 67421031227406451\n",
      " 67630276227919595 66423581229467492 66417790229741002 66256056228896073\n",
      " 67808163228079410 66036849228228645 66450848229502171 67803686227171774\n",
      " 66516039229573675 66203824227164500 66913908227096976 68160534227741466\n",
      " 67957882227932295 67795304227883645 67802488227889739 67000465229701376\n",
      " 67289155227214851 66047876227208435 66345786229110957 68163758227703944\n",
      " 67555650228279098 66363075229716550 68049853228235765 66822900227862949\n",
      " 68223738228158767 68110033228003707 67315931229397601 67264006229792404\n",
      " 66969449227025988 66877739226919871 67005534227070348 66765197228171861\n",
      " 65956569227249862 66376874229585134 66188544227832197 65972711228350357\n",
      " 66485936229365192 68420519227711582 68118981228908728 68374436228783675\n",
      " 67145402228034826 67986156228067999 67163985227493767 66394954229745314\n",
      " 67427339229621612 68173896228130303 66189515228271746 67023668229388107\n",
      " 68433603227906468 67134359227795846 67509086229265925 67116410227796098\n",
      " 66435392228661882 67432793229239379 66399445228648066 66826201229607561\n",
      " 67118831227799219 65763359227565055 65895103227323150 65953579227203060\n",
      " 65957905228360298 66043460227257247 66081178227195535 66122391229397530\n",
      " 66156563229123391 66199315228931786 66205995229590030 66213422229596073\n",
      " 66214231229605621 66265767228004324 66284611228454740 66277187228863504\n",
      " 66310469228515316 66321705229807902 66357622228461840 66453068227976840\n",
      " 66486859228765938 66497018227888854 66545289226740154 66584042229250642\n",
      " 66581122229437724 66590389229254519 66619687229240226 66616066229220682\n",
      " 66616155229220585 66618595226603821 66623758229444883 66624347229444767\n",
      " 66667337226545214 66676403228058924 66700953229217934 66706531229584494\n",
      " 66722447228672762 66771678227126931 66772467227126804 66829320228216182\n",
      " 66860547228468554 66864303227108970 66865056227645621 66915421228251439\n",
      " 66972860228452605 66987620227304204 66997231228097150 66997425228097140\n",
      " 66997906229126887 67041454228060726 67047067229617971 67098991228702048\n",
      " 67105549227826979 67108560227830380 67113220229580902 67146630228987897\n",
      " 67146669229213330 67176032227963389 67192549227342976 67193080227079267\n",
      " 67191132227945746 67198371229465837 67215751228707001 67251498227982560\n",
      " 67289672229614478 67294644227904734 67316349229579927 67341328227566334\n",
      " 67344725228855201 67365961227570728 67419357228550347 67469829227928237\n",
      " 67480419229205300 67512201228068926 67578550229228608 67591951227845100\n",
      " 67628011229053963 67651944229179562 67649675227809142 67659394229201747\n",
      " 67706051228882175 67801138229280620 67745020227289899 67796746229152086\n",
      " 67890906228208918 67905509228229330 67908354228241353 67917710228170857\n",
      " 67928015228358856 67928020228358828 67948036228182245 67936800228282912\n",
      " 67960331227356606 67986954227329531 67987424228070281 67995716228013977\n",
      " 68027153227742905 68025947228382749 68035254227431372 68056196228333052\n",
      " 68163354228207906 68175500228243205 68182574228197144 68185035228173638\n",
      " 68187119228184722 68245367228290698 68435280227749099 68599178227760187\n",
      " 68746671228567091 67990160228381078 66273682228483812 67801497228251348\n",
      " 66467752227885156 67345490227305427 67167978227742009 66291643227187732\n",
      " 67600441228166633]\n",
      "------------------------------------------------------------\n",
      "Transformed nodes: 124377.\n",
      "Transformed edges: 197206.\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Initial network\n",
    "print(\"--\"*30)\n",
    "print(f\"Entering nodes: {cons_nodes.shape[0]}.\")\n",
    "print(f\"Entering edges: {cons_edges.shape[0]}.\")\n",
    "print(\"--\"*30)\n",
    "\n",
    "##### Time\n",
    "time_1 = time.time()\n",
    "##### Time\n",
    "\n",
    "# --- Re-create network\n",
    "# ---> Substitutes created osmids with coordinate osmids\n",
    "# ---> Removes two_edge_osmids using function remove_redundant_nodes()\n",
    "# ---> Fixes all keys using resolve_duplicates_indexes()\n",
    "print('Transforming network...')\n",
    "rebuilt_nodes, rebuilt_edges = src.network_entities(cons_nodes,\n",
    "                                                    cons_edges,\n",
    "                                                    projected_crs,\n",
    "                                                    expand_coords=(True,100))\n",
    "\n",
    "##### Time\n",
    "time_2 = time.time()\n",
    "print(f\"TIME: {time_2-time_1} seconds on transforming the network.\")\n",
    "##### Time\n",
    "\n",
    "# --- Restore nodes and edges final format\n",
    "# Nodes final format\n",
    "if 'osmid' not in rebuilt_nodes.columns:\n",
    "    rebuilt_nodes.reset_index(inplace=True)\n",
    "rebuilt_nodes['osmid'] = rebuilt_nodes.osmid.astype(int)\n",
    "# Edges final format\n",
    "if 'u' not in rebuilt_edges.columns:\n",
    "    rebuilt_edges.reset_index(inplace=True)\n",
    "rebuilt_edges = src.create_unique_edge_id(rebuilt_edges)\n",
    "rebuilt_edges['u'] = rebuilt_edges.u.astype(int)\n",
    "rebuilt_edges['v'] = rebuilt_edges.v.astype(int)\n",
    "\n",
    "# Set projected crs\n",
    "rebuilt_nodes = rebuilt_nodes.to_crs(projected_crs)\n",
    "rebuilt_edges = rebuilt_edges.to_crs(projected_crs)\n",
    "\n",
    "# --- Check for missing u/v values\n",
    "# All osmids\n",
    "node_ids = set(rebuilt_nodes['osmid'])\n",
    "# Filter for 'u' and 'v' values that are not in node's 'osmid' column\n",
    "missing_u = rebuilt_edges[~rebuilt_edges['u'].isin(node_ids)]['u'].unique()\n",
    "missing_v = rebuilt_edges[~rebuilt_edges['v'].isin(node_ids)]['v'].unique()\n",
    "# Mostrar los nodos faltantes en u y v\n",
    "if len(missing_u)>0:\n",
    "    print(f\"WARNING: Missing {len(missing_u)} u nodes\", missing_u)\n",
    "    print(\"--\"*30)\n",
    "if len(missing_v)>0:\n",
    "    print(f\"WARNING: Missing {len(missing_v)} v nodes:\", missing_v)\n",
    "    print(\"--\"*30)\n",
    "\n",
    "# Show result\n",
    "print(f\"Transformed nodes: {rebuilt_nodes.shape[0]}.\")\n",
    "print(f\"Transformed edges: {rebuilt_edges.shape[0]}.\")\n",
    "print(\"--\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "005e57ec-f179-4761-9f52-1d7d235f10fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if localsave_02_07:\n",
    "    # Current step output directory\n",
    "    output_fold = output_dir + 'part02_step07_ntwsrebuild/'\n",
    "    \n",
    "    # Create folder if it doesn't exist\n",
    "    if os.path.exists(output_fold) == False:\n",
    "        os.mkdir(output_fold)\n",
    "    \n",
    "    # Save result\n",
    "    rebuilt_nodes.to_file(output_fold + f\"{city}_nodes_proj_net_rebuilt.gpkg\")\n",
    "    rebuilt_edges.to_file(output_fold + f\"{city}_edges_proj_net_rebuilt.gpkg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f6d03d-ccb6-4db8-a286-76018d1aae18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GDS-10.0",
   "language": "python",
   "name": "gds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
